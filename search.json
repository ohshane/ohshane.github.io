[
  {
    "objectID": "posts/HuffmanCoding.html",
    "href": "posts/HuffmanCoding.html",
    "title": "Huffman Coding",
    "section": "",
    "text": "Thanks to Pizzey Technology for the wonderful video."
  },
  {
    "objectID": "posts/HuffmanCoding.html#concepts",
    "href": "posts/HuffmanCoding.html#concepts",
    "title": "Huffman Coding",
    "section": "Concepts",
    "text": "Concepts\nHuffman coding is a type of variable-length prefix coding that assigns shorter codes to more frequent symbols and longer codes to less frequent symbols. Formal definitions aren’t very useful when dealing with other concepts, in this case, entropy.\nWell, keep this in mind: lossless.\nHuffman coding is one of the lossless compression methods. In terms of compression, you cannot compress data smaller than the limit defined by entropy.\nHuffman is a bottom-up approach to compression which is optimal, while Shannon entropy defines the theoretical limit, a lower bound.\nExamples first."
  },
  {
    "objectID": "posts/HuffmanCoding.html#ascii",
    "href": "posts/HuffmanCoding.html#ascii",
    "title": "Huffman Coding",
    "section": "ASCII",
    "text": "ASCII\nASCII is a character encoding developed in the ’60s by ANSI. It uses 7 bits for character representation. However, in modern computing, characters are stored in bytes (8 bits) for compatibility reasons.\nLet’s take a look at the 8-bit encoded sentence.\n\n\nAN APPLE A DAY KEEPS THE DOCTOR AWAY\n\n01000001 01001110 00100000 01000001 01010000 01010000 01001100 01000101 \n00100000 01000001 00100000 01000100 01000001 01011001 00100000 01001011 \n01000101 01000101 01010000 01010011 00100000 01010100 01001000 01000101 \n00100000 01000100 01001111 01000011 01010100 01001111 01010010 00100000 \n01000001 01010111 01000001 01011001 \n\n288 bits / 36.0 bytes\ncompression rate: 0.0%\n\n\nYou can easily determine the total number of bits in this sentence by simply multiplying the number of characters (including spaces) by the fixed encoding size."
  },
  {
    "objectID": "posts/HuffmanCoding.html#compress-it",
    "href": "posts/HuffmanCoding.html#compress-it",
    "title": "Huffman Coding",
    "section": "Compress it!",
    "text": "Compress it!\nThe more frequently a character appears, the shorter its encoded length becomes.\nThe phrase AN APPLE A DAY KEEPS THE DOCTOR AWAY uses A to rhyme which makes the example more fun.\n\nHuffman tree\nWe build the tree from the bottom using the less frequent characters. Eventually, the less frequent ones are placed at deeper levels. The deeper a character is, the longer the traversal route from the root, which results in a longer encoding length.\nCheck the code below.\n\nfrom collections import Counter\n\nclass Node:\n    def __init__(self, char='', freq=0, left=None, right=None):\n        self.char  = char\n        self.freq  = freq\n        self.left  = left\n        self.right = right\n    \n    @property\n    def is_leaf(self):\n        return self.left is None and self.right is None\n    \n    def __str__(self):\n        if self.is_leaf:\n            return f\"'{self.char}': {self.freq}\"\n        return str(self.freq)\n    \n    def __lt__(self, other):\n        return self.freq &lt; other\n\n    def __gt__(self, other):\n        return self.freq &gt; other\n        \nclass HuffmanTree:\n    def __init__(self, freq_table):\n        self.freq_table = dict(freq_table)\n        self.encoded_table = {}\n        self.root = None\n\n        self.build()\n        self.encode()\n\n    @property\n    def l_bar(self):\n        total = 0\n        for char, freq in self.freq_table.items():\n            total += len(self.encoded_table[char]) * freq\n        return total / sum(self.freq_table.values())\n    \n    def build(self):\n        nodes = [Node(char, freq) for char, freq in self.freq_table.items()]\n        while len(nodes) &gt; 1:\n3            node1 = nodes.pop(nodes.index(min(nodes)))\n            node2 = nodes.pop(nodes.index(min(nodes)))\n            node  = Node(freq=node1.freq+node2.freq,\n                         left=node2,\n                         right=node1)\n            nodes.append(node)\n            self.root = node\n    \n    def encode(self):\n        def dfs(node, path=''):\n1            if node.is_leaf:\n                self.encoded_table[node.char] = path\n                return\n2            dfs(node.left,  path+'0')\n            dfs(node.right, path+'1')\n\n        dfs(self.root)\n\n\n1\n\nCharacters are only at the leaf nodes. This property ensures that each encoded value is not a substring of another and keeps the unique decodability. This is also called as the prefix-free coding.\n\n2\n\nUse 0 for the left and 1 for the right. The path from the root to the leaf node represents the encoded result.\n\n3\n\nAs the function min returns the index of the first occurring minimum value (node.freq) from the list nodes, the built tree is not unique but preserves optimality.\n\n\n\n\n\n\n\n\n\ngraph RL\n140415568351952[36]\n140415568351952 -- 0 --&gt; 140415568362448\n140415568351952 -- 1 --&gt; 140415568355792\n140415568362448[21]\n140415568362448 -- 0 --&gt; 140415568361424\n140415568362448 -- 1 --&gt; 140415568360720\n140415568355792[15]\n140415568355792 -- 0 --&gt; 140415568362384\n140415568355792 -- 1 --&gt; 140415568362128\n140415568361424[13]\n140415568361424 -- 0 --&gt; 140415568356624\n140415568361424 -- 1 --&gt; 140415568347600\n140415568360720[8]\n140415568360720 -- 0 --&gt; 140415568348816\n140415568360720 -- 1 --&gt; 140415568357776\n140415568362384[8]\n140415568362384 -- 0 --&gt; 140415568359440\n140415568362384 -- 1 --&gt; 140415568361616\n140415568362128[7]\n140415568362128 -- 0 --&gt; 140415568359568\n140415568362128 -- 1 --&gt; 140415568356688\n140415568356624[' ': 7]\n140415568347600['A': 6]\n140415568348816[4]\n140415568348816 -- 0 --&gt; 140415568348368\n140415568348816 -- 1 --&gt; 140415568357136\n140415568357776[4]\n140415568357776 -- 0 --&gt; 140415568353680\n140415568357776 -- 1 --&gt; 140415568358352\n140415568359440[4]\n140415568359440 -- 0 --&gt; 140415568362960\n140415568359440 -- 1 --&gt; 140415568347792\n140415568361616[4]\n140415568361616 -- 0 --&gt; 140415568351888\n140415568361616 -- 1 --&gt; 140415568358672\n140415568359568['E': 4]\n140415568356688['P': 3]\n140415568348368[2]\n140415568348368 -- 0 --&gt; 140415568352016\n140415568348368 -- 1 --&gt; 140415568362576\n140415568357136[2]\n140415568357136 -- 0 --&gt; 140415568353040\n140415568357136 -- 1 --&gt; 140415568354192\n140415568353680[2]\n140415568353680 -- 0 --&gt; 140415568357520\n140415568353680 -- 1 --&gt; 140415568361552\n140415568358352[2]\n140415568358352 -- 0 --&gt; 140415568349264\n140415568358352 -- 1 --&gt; 140415568361488\n140415568362960['O': 2]\n140415568347792['T': 2]\n140415568351888['Y': 2]\n140415568358672['D': 2]\n140415568352016['W': 1]\n140415568362576['R': 1]\n140415568353040['C': 1]\n140415568354192['H': 1]\n140415568357520['S': 1]\n140415568361552['K': 1]\n140415568349264['L': 1]\n140415568361488['N': 1]\n\n\n\n\n\n\n\n\nHuffman code\nFinally, the Huffman coded result is below.\n\n\nAN APPLE A DAY KEEPS THE DOCTOR AWAY\n\n001 01111 000 001 111 111 01110 110 000 001 000 1011 001 1010 000 01101 \n110 110 111 01100 000 1001 01011 110 000 1011 1000 01010 1001 1000 01001 \n000 001 01000 001 1010 \n\n132 bits / 16.5 bytes\ncompression rate: 54.2%\n\n\nYou might be familiar with the pangram THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG. It contains all the letters from A to Z, which makes it harder to compress.\n\n\nTHE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n\n00011 00010 0101 001 11111 00001 11110 11101 11100 001 11011 00000 0100 \n11010 11001 001 11000 0100 10111 001 10110 00001 10101 10100 10011 001 0100 \n10010 0101 00000 001 00011 00010 0101 001 10001 10000 01111 01110 001 01101 \n0100 01100 \n\n192 bits / 24.0 bytes\ncompression rate: 44.2%\n\n\nJust for fun, let’s apply Huffman coding to images.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mondriaan\n\n\n\n\n\n\n\n\n\n\n\n(b) Monet\n\n\n\n\n\n\n\nFigure 1: Image compression\n\n\n\nBoth Figure 1 (a) and Figure 1 (b) are resized to 100x100 greyscale images. Each pixel is represented by the intensity of light from 0 to 255 which corresponds to one byte of information. This results in 10000 bytes for each image.\nAfter applying Huffman coding to each image, each is compressed to 7903.6 bytes and 8354.9 bytes, respectively.\nSo, why do we think Figure 1 (a) is easier to draw? Why is it easier to remember pop music notes than those of bebop jazz?\nIt becomes clear when explained using data compression. Since it has a higher compression rate, it can be described verbally or communicated with a shorter and more compact explanation."
  },
  {
    "objectID": "posts/HuffmanCoding.html#average-codeword-length-per-character-barlambda",
    "href": "posts/HuffmanCoding.html#average-codeword-length-per-character-barlambda",
    "title": "Huffman Coding",
    "section": "Average codeword length per character (\\(\\bar{\\lambda}\\))",
    "text": "Average codeword length per character (\\(\\bar{\\lambda}\\))\nLet’s use some notations from now on.\n\\[\n\\bar \\lambda = \\mathbb E [\\lambda] = \\sum_{i=1}^N p(x_i) \\lambda_i\n\\]\n\n\\(\\lambda_i\\): codeword length for the character \\(x_i\\)\n\\(p(x_i)\\): probability of the character \\(x_i\\) occurring\n\nFor the phrase AN APPLE A DAY KEEPS THE DOCTOR AWAY, \\(\\bar \\lambda\\) is 3.667 with the unit in bits."
  },
  {
    "objectID": "posts/HuffmanCoding.html#sec-kraft-mcmillan-inequality",
    "href": "posts/HuffmanCoding.html#sec-kraft-mcmillan-inequality",
    "title": "Huffman Coding",
    "section": "Kraft-McMillan inequality",
    "text": "Kraft-McMillan inequality\nIn coding theory, the Kraft–McMillan inequality gives a necessary and sufficient condition for the existence of a prefix code1.\nBefore diving into this, let’s discuss the upper bound of \\(\\lambda\\). When all the probabilities \\(p(x_i)\\) are powers of two (\\(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\cdots\\)), \\(\\lambda\\) will be integers (\\(1, 2, 3, \\cdots\\)), which can be derived from \\(\\log_2 \\frac{1}{p(x_i)}\\). However, most of the time, the expression will not result in an integer. In such cases, we can account for this by using the ceiling function, giving extra freedom. This is still useful when calculating \\(\\bar \\lambda\\).\n\n\n\n\n\n\nNote\n\n\n\nBasically, ceiling \\(\\lambda_i\\) is same as flooring \\(p(x_i)\\) to the nearest \\(2^{-\\lambda_i}\\).\n\n\nFrom now on, we will define \\(\\lambda_i\\) like below.\n\\[\n\\lambda_i \\triangleq \\lceil \\log_2 \\dfrac{1}{p(x_i)} \\rceil\n\\]\nThis will give us some insights.\n\\[\n\\begin{align*}\n\\lambda_i &\\geq \\log_2 \\dfrac{1}{p(x_i)} = -\\log_2 p(x_i) \\\\[10pt]\n-\\lambda_i &\\leq \\log_2 p(x_i) \\\\[10pt]\n2^{-\\lambda_i} &\\leq p(x_i) \\\\[10pt]\n\\sum_{i=1}^N 2^{-\\lambda_i} &\\leq \\sum_{i=1}^N p(x_i)\n\\end{align*}\n\\]\nAdding all the probability is equal to \\(1\\) which leads to the final form of Kraft-McMillan inequality.\n\\[\n\\sum_{i=1}^N 2^{-\\lambda_i} \\leq 1\n\\]\nWe can represent this in a figure like below.\n\n\n\n\n\n\nFigure 2: Kraft-McMillan inequality\n\n\n\nIn Figure 2, all \\(\\lambda_i\\) are integers. We are assigning the total width of \\(1\\) by the power of 2 for each codewords."
  },
  {
    "objectID": "posts/HuffmanCoding.html#footnotes",
    "href": "posts/HuffmanCoding.html#footnotes",
    "title": "Huffman Coding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKraft-McMillan inequality - Wikipedia↩︎"
  },
  {
    "objectID": "posts/BinaryHeap.html",
    "href": "posts/BinaryHeap.html",
    "title": "Binary Heap",
    "section": "",
    "text": "Check out a video by Abdul Bari."
  },
  {
    "objectID": "posts/BinaryHeap.html#represent-a-binary-tree-in-an-array",
    "href": "posts/BinaryHeap.html#represent-a-binary-tree-in-an-array",
    "title": "Binary Heap",
    "section": "Represent a binary tree in an array",
    "text": "Represent a binary tree in an array\n\n\n\n\n\nflowchart TB\n  A(1: A) --- B(2: B)\n  A --- C(3: C)\n  B --- D(4: D)\n  B --- E(5: E)\n  C --- F(6: F)\n  C --- G(7: G)\n\n\n\n\n\n\n\n  \n  A\n  B\n  C\n  D\n  E\n  F\n  G\n\n  idx\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo represent a binary tree in an array, you can follow these rules:\n\nIf the root of the tree is at index 1 &lt;- easier to remember\nFor a node at index \\(i\\):\n\nThe left child is at index \\(2i\\).\nThe right child is at index \\(2i+1\\).\nThe parent is at index \\(\\lfloor \\frac{i}{2} \\rfloor\\).\n\nIf the root of the tree is at index 0 &lt;- when implementing\nFor a node at index \\(i\\):\n\nThe left child is at index \\(2i+1\\).\nThe right child is at index \\(2i+2\\).\nThe parent is at index \\(\\lfloor \\frac{i - 1}{2} \\rfloor\\).\n\n\n\n\nThe definition of an (almost) complete binary tree becomes clearer when we represent tree structure as an array. We should not allow any null values between the elements.\nNow you may be thinking about the difference between binary trees and heaps. Actually, while all heaps are binary trees, not all binary trees are heaps. You must fulfill the heap property. So what is it?"
  },
  {
    "objectID": "posts/BinaryHeap.html#heap-property",
    "href": "posts/BinaryHeap.html#heap-property",
    "title": "Binary Heap",
    "section": "Heap property",
    "text": "Heap property\n\n  \n    Tree\n    \n      Binary tree\n      \n        Almost complete binary tree\n        \n          Heap\n          \n            Min-heap / Max-heap\n          \n        \n      \n    \n  \n\n\n\nThe heap property dictates the relationship between a parent node and its children in a binary tree. It can be defined in two ways, leading to two different types of heaps:\n\nMin-heap property \\[A[\\lfloor \\frac{i}{2} \\rfloor] \\leq A[i]\\]\nMax-heap property \\[A[\\lfloor \\frac{i}{2} \\rfloor] \\geq A[i]\\]\n\nI find defining the property with the relationship of a current node \\(i\\) and the parent node \\(\\lfloor \\dfrac{i}{2} \\rfloor\\) is more simple (since it can handle the root condition).\nIf you have an array \\(A\\) with no null values between elements, you are already satisfying the almost complete binary tree property and are halfway ready to be a heap structure. How cool is that!\nThe remaining half of the process is called build-heap. We build-heap by heapifying \\(n\\) times.\nFrom now on, we will take a close look at the max-heap since the min-heap and max-heap are basically the same."
  },
  {
    "objectID": "posts/BinaryHeap.html#heapify",
    "href": "posts/BinaryHeap.html#heapify",
    "title": "Binary Heap",
    "section": "Heapify",
    "text": "Heapify\nCheck out the video by Techdose helps!\n\n\n\n\n\nflowchart TB\n  A(  ) -.- 10\n  A(  ) -.- B(  )\n  10 --- 8\n  10 --- 12\n  8 -.- C(  )\n  8 -.- D(  )\n  12 -.- E(  )\n  12 -.- F(  )\n\n\n\n\n\n\nLet’s examine the tree above (with node 10 as root). Assert that the subtrees under node 10 are max-heaps. By recursively sifting down (similar to bubble sorting) on node 10, the entire tree will eventually become a max-heap.\nThe time complexity of heapifying is the same as the height of the heapifying index. Thankfully, the tree is balanced from the very start, which makes it \\(O(\\log n)\\). Obvious, right?"
  },
  {
    "objectID": "posts/BinaryHeap.html#build-heap",
    "href": "posts/BinaryHeap.html#build-heap",
    "title": "Binary Heap",
    "section": "Build heap",
    "text": "Build heap\nThanks again for the video!\nLet’s transform an arbitrary array \\(A\\) into a heap using the build-heap process.\n\\[\\begin{align*}\n  A &\\quad\n  \\begin{bmatrix}\n    2 & 12 & 5 & 15 & 16 & 2 & 6 & 9 & 1 & 4\n  \\end{bmatrix} \\\\\n\n  \\text{Build-Max-Heap}(A) &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\n\\end{align*}\\]\nAt first glance, it might seem intuitive that repeatedly heapifying the array \\(n\\) times, starting from the right (or bottom), will transform \\(A\\) into a heap.\nWould it be surprising to you if I told you that the time complexity of the build-heap algorithm is actually \\(O(n)\\) instead of \\(O(n \\log n)\\)?\n\n\nmaxheapify.go\n\n// MaxHeap constructs a max-heap from an unordered array\n// `i` starts from 0 in this code\n// We are basically bubble sorting from node `i` to the leaf node\n// while iterating `i` from n to 0.\nfunc BuildMaxHeap(arr []int, n int) {\n    // Start from the last non-leaf node and heapify each node\n1    for i := n/2 - 1; i &gt;= 0; i-- {\n        MaxHeapify(arr, n, i)\n    }\n}\n\n// MaxHeapify ensures the subtree rooted at index i is a max-heap\n//      i\n//    /   \\\n// left   right\nfunc MaxHeapify(arr []int, n, i int) {\n    largest := i       // Initialize largest as root\n    left := 2*i + 1    // left child index\n    right := 2*i + 2   // right child index\n\n    // If left child is larger than root\n    if left &lt; n && arr[left] &gt; arr[largest] {\n        largest = left\n    }\n\n    // If right child is larger than the largest so far\n    if right &lt; n && arr[right] &gt; arr[largest] {\n        largest = right\n    }\n\n    // If largest is not root\n    if largest != i {\n        arr[i], arr[largest] = arr[largest], arr[i]  // Swap\n\n        // Recursively heapify the affected subtree\n        MaxHeapify(arr, n, largest)\n    }\n}\n\n\n1\n\nThe for loop can be iterated from n to 0. However, since half of the elements (which are leaf nodes) are already part of a heap, we can start from the node that is not a leaf.\n\n\n\n\n\n\n\n\nLevels and heights of binary trees\n\n\n\nLevel 0                  1                  Height 3\n                        / \\                         \nLevel 1          2               3          Height 2\n                / \\             / \\                 \nLevel 2      4       5       6       7      Height 1\n            / \\     / \\     / \\     / \\             \nLevel 3    8   9  10   11 12   13 14   15   Height 0\nTry to imagine a complete binary tree with a large number of levels. Pick any level you desire in between and set it as \\(l\\).\n\nThe indices of the first elements at each level are \\(2^l\\) and the level of a certain index is \\(\\lfloor \\log_{2} i \\rfloor\\).\nThere are \\(2^{l}-1\\) nodes in the whole tree just before level \\(l\\).\nThere are \\(2^{l}\\) nodes at level \\(l\\).\nThere are \\(2^{l+1}\\) nodes at the next level, \\(l+1\\), which is twice as many.\nAt height \\(h\\), there are a maximum of \\(\\lceil \\frac{N}{2^{h+1}} \\rceil\\) nodes.\n\n\n\nThe nodes at height \\(h\\) needs to be heapified by sifting down \\(h\\) times and there are \\(\\lceil \\frac{N}{2^{h+1}} \\rceil\\) nodes max at each height \\(h\\) which makes,\n\\[\n\\begin{align*}\n\\sum_{h=0}^{\\lfloor \\log_2 N \\rfloor} \\lceil \\dfrac{N}{2^{h+1}} \\rceil O(h)\n&&lt; O \\left( \\sum_{h=0}^{\\infty} \\dfrac{N}{2^{h+1}} C h \\right) \\\\\n&= O \\left( \\dfrac{CN}{2} \\sum_{h=0}^{\\infty} \\dfrac{h}{2^{h}} \\right) \\\\\n&= O \\left( \\dfrac{CN}{2} 2 \\right) \\\\\n&= O(N)\n\\end{align*}\n\\]\nThe talor series is useful when explaining this. Check out the [post] if you are interested.\n\n\n\n\n\n\nWhat is the range of leaf nodes?\n\n\n\nThe parent of the last element can be considered the last node that is not a leaf node.\n\\[\nA[\\lfloor \\dfrac{n}{2} \\rfloor + 1:n]\n\\]\n\n\nCongratulations! You now have a beautiful max-heap ready. Let’s utilize this as a priority queue."
  },
  {
    "objectID": "posts/BinaryHeap.html#inserting-and-popping-elements-in-the-queue",
    "href": "posts/BinaryHeap.html#inserting-and-popping-elements-in-the-queue",
    "title": "Binary Heap",
    "section": "Inserting and popping elements in the queue",
    "text": "Inserting and popping elements in the queue\nInsert from the right, pop from the left.\nInserting from the right helps us to maintain the almost complete binary tree property, but the max-heap property is broken.\n\\[\\begin{align*}\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  A_\\text{broken-max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4 & 100\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 100 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 100 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    100 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\\end{align*}\\]\nThe solution is to check the integrity from the inserted leaf node to the root. Simply compare with the parent node and propagate upward to the top. In the array representation, it seems like hopping to the left for \\(\\log n\\) times.\nPopping is done by removing the root and replacing it with the last leaf node. To preserve the max-heap property, we propagate downward from the root to the bottom.\n\\[\\begin{align*}\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    100 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  A_\\text{broken-max-heap} &\\quad\n  \\begin{bmatrix}\n    12 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 12 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n\\end{align*}\\]\nNotice that the root node always holds the maximum value of the entire tree. This characteristic enables sorting; you simply keep popping the root until the heap is empty."
  },
  {
    "objectID": "posts/BinaryHeap.html#heap-sort",
    "href": "posts/BinaryHeap.html#heap-sort",
    "title": "Binary Heap",
    "section": "Heap sort",
    "text": "Heap sort\nWhy do we need to create a dedicated paragraph for sorting when it is so straightforward? There is a fun little idea behind heap sorting in an array that makes it more elegant.\n\n\nheapsort.go\n\nfunc HeapSort(arr []int, n int) {\n1    BuildMaxHeap(arr)\n2    for m := n - 1; m &gt;= 0; m-- {\n3        arr[0], arr[m] = arr[m], arr[0]\n        MaxHeapify(arr, m, 0)\n    }\n}\n\n\n1\n\nFirst, create a max-heap.\n\n2\n\nInstead of popping and stacking the elements into a new empty array, we utilize the original array. After popping, there is a spare index because the size of the heap is reduced.\n\n3\n\nBy marking the end of the max-heap with m, we can swap the root value with the leaf at the very end. The popped value will be stacked from the end of the array, and eventually, the array will become a sorted array in ascending order."
  },
  {
    "objectID": "posts/Tmux.html",
    "href": "posts/Tmux.html",
    "title": "Tmux",
    "section": "",
    "text": "Start off with installing tmux with homebrew on Mac."
  },
  {
    "objectID": "posts/Tmux.html#intro",
    "href": "posts/Tmux.html#intro",
    "title": "Tmux",
    "section": "Intro",
    "text": "Intro\nThere are three main concepts in tmux: session, window, and pane. Start by entering tmux in the terminal.\nThe screen you see right after entering the command is a pane in a window."
  },
  {
    "objectID": "posts/Tmux.html#pane",
    "href": "posts/Tmux.html#pane",
    "title": "Tmux",
    "section": "Pane",
    "text": "Pane\nSplit the pane using Ctrl-bCtrl-b %% and Ctrl-bCtrl-b \"\".\nThe Ctrl-bCtrl-b works as a prefix to send a command — later below.\nNavigate through the panes using Ctrl-bCtrl-b ↑↑, Ctrl-bCtrl-b →→, Ctrl-bCtrl-b ↓↓, Ctrl-bCtrl-b ←←."
  },
  {
    "objectID": "posts/Tmux.html#window",
    "href": "posts/Tmux.html#window",
    "title": "Tmux",
    "section": "Window",
    "text": "Window\nOpen a new window with Ctrl-bCtrl-b cc.\nSee the windows you opened on the bottom green bar? This gives us some information about the windows in the session. The current window you are seeing is marked with an *.\nNavigate through the windows using Ctrl-bCtrl-b nn — n is for next. This will cycle through all the windows in the current session. Reverse navigate with Ctrl-bCtrl-b pp — p is for previous. You can also navigate directly to a window using the index. The bindings will be like Ctrl-bCtrl-b 11."
  },
  {
    "objectID": "posts/Tmux.html#session",
    "href": "posts/Tmux.html#session",
    "title": "Tmux",
    "section": "Session",
    "text": "Session\nFrom the very first, right after the tmux command, you are attached to a session with an auto generated index. Detach the session with the Ctrl-bCtrl-b dd command — and d is for detach. It’s almost the same as starting the bash session and detaching with the exit command. Tmux can also be detached using the exit command, but this can be tedious because each split pane needs to be closed individually with the exit command."
  },
  {
    "objectID": "posts/Tmux.html#configurations",
    "href": "posts/Tmux.html#configurations",
    "title": "Tmux",
    "section": "Configurations",
    "text": "Configurations\nFeeling comfortable with the keybindings? I hope not. The default keybindings can put significant stress on your left pinky. This brings us to the .tmux.conf file for some configuration.\n\n\n~/.tmux.conf\n\nunbind-key C-b\nset -g prefix C-a\n1bind-key C-a send-prefix\n\n2set -g mouse on\n3set -g base-index 1\nset -g renumber-windows on\nset -g default-terminal \"tmux-256color\"\n\n4bind r source-file ~/.tmux.conf \\; display-message \".tmux.conf reloaded!\"\n\n5bind '\\' split-window -h -c \"#{pane_current_path}\"\n6bind - split-window -v -c \"#{pane_current_path}\"\n\n7bind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n8bind x kill-pane\n9bind X kill-window\n\n\n1\n\nReplace the prefix key from Ctrl-bCtrl-b to Ctrl-aCtrl-a. When you are using a keyboard like HHKB, this will come in pretty handy. Let’s talk more about HHKB in some other posts.\n\n2\n\nPretty straight forward. Helps you navigate split panes with a mouse.\n\n3\n\nThe default is 0. For me, 0 key is a bit far for everyday use.\n\n4\n\nReload the .tmux.conf file and display a message when done — similar to something like source .bashrc.\n\n5\n\nSplit the window horizontally using the current pane’s path.\n\n6\n\nSplit the window vertically using the current pane’s path.\n\n7\n\nMove between panes using Vim-style keybindings.\n\n8\n\nKill the current pane.\n\n9\n\nKill the current window.\n\n\nPersonal preference on the following one.\n\n\n.zshrc\n\ntmux() {\n    if [ \"$#\" -eq 0 ]; then\n1        command tmux new-session -A -s default\n    else\n        command tmux \"$@\"\n    fi\n}\n\n\n1\n\nAttach a session named default when tmux is typed."
  },
  {
    "objectID": "posts/Kubeconfig.html",
    "href": "posts/Kubeconfig.html",
    "title": ".kube/config",
    "section": "",
    "text": "I have set up Minikube on my MacBook for an easy development environment, and at home, I have a Raspberry Pi cluster set up for my homelab. These configurations allow me to access and manage my clusters from anywhere. Specifically, I have properly configured the .kube/config file to enable remote access to the cluster at home.\nBelow is an example of the configured .kube/config file:\n\n\n.kube/config\n\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0...LS0tCg==\n    server: https://kubernetes.docker.internal:6443\n  name: docker-desktop\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://121.135.111.111:6443\n  name: raspberrypi\n- cluster:\n    certificate-authority: /Users/shane/.minikube/ca.crt\n    extensions:\n    - extension:\n        last-update: Sun, 20 Aug 2023 17:04:00 KST\n        version: v1.30.1\n        provider: minikube.sigs.k8s.io\n      name: cluster_info\n    server: https://127.0.0.1:60544\n  name: minikube\n\nThe above configuration file defines three clusters:\n\ndocker-desktop\nraspberrypi (the Raspberry Pi cluster at home, with the server address https://121.135.111.111:6443)\nminikube\n\nWith this configuration file, you can easily access various clusters using the kubectl command. For example, to access the Raspberry Pi cluster, you can use the following command:\nkubectl config use-context raspberrypi\nTo bypass the process of verifying the SSL certificate as a public certificate, use the following command:\n- cluster:\n    insecure-skip-tls-verify: true"
  },
  {
    "objectID": "posts/InternallyDividingPoint.html",
    "href": "posts/InternallyDividingPoint.html",
    "title": "Internally Dividing Point",
    "section": "",
    "text": "Figure 1: Internally dividing point\n\n\n\n\\[\n\\begin{align*}\nz &= (y-x) \\cdot \\dfrac{a}{a+b} + x \\\\[10pt]\n&= \\dfrac{ay - ax + ax + bx}{a+b} \\\\[10pt]\n&= \\dfrac{bx + ay}{a+b}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/GitPrompt.html",
    "href": "posts/GitPrompt.html",
    "title": "Git Prompt",
    "section": "",
    "text": "Download the git-prompt.sh with the curl command.\ncurl https://raw.githubusercontent.com/git/git/master/contrib/completion/git-prompt.sh -o ~/.git-prompt.sh\nThen add the following lines to the rc files depending on your environment.\n\n\n.zshrc\n\nsource ~/.git-prompt.sh\nsetopt PROMPT_SUBST\nPS1='%n@%m %c%F{green}$(__git_ps1 \" (%s)\")%f \\$ '\n\n\n\n.bashrc\n\nsource ~/.git-prompt.sh\nexport GIT_PS1_SHOWDIRTYSTATE=1\nexport PS1='\\[\\e[0;32m\\]\\u@\\h\\[\\e[0m\\] \\[\\e[0;34m\\]\\W\\[\\e[0m\\]\\[\\e[0;33m\\]$(__git_ps1 \" (%s)\")\\[\\e[0m\\] \\$ '"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n.kube/config\n\n\nwith KubeContext\n\n\n2 min\n\n\n\nK8s\n\n\n\n\n\n\n\nShane Oh\n\n\nAug 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Heap\n\n\nFor better priority queuing\n\n\n12 min\n\n\n\nAlgorithms\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEntropy\n\n\nThe lower bound of \\(\\bar \\lambda\\)\n\n\n4 min\n\n\n\nInformation Theory\n\n\nData Compression\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGAN\n\n\nWhich Comes First? Generator or Discriminator?\n\n\n7 min\n\n\n\nML\n\n\nGenerative Models\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Seuqnece\n\n\nAdding up terms in a sequence with a pattern\n\n\n1 min\n\n\n\nMath\n\n\n\n\n\n\n\nShane Oh\n\n\nAug 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Prompt\n\n\nCheck the branch you are working on\n\n\n1 min\n\n\n\nGit\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGo Concurrency\n\n\nChannel, waitgroup, mutex\n\n\n2 min\n\n\n\nGo\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHuffman Coding\n\n\nLossless data compression\n\n\n9 min\n\n\n\nInformation Theory\n\n\nData Compression\n\n\nAlgorithms\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInternally Dividing Point\n\n\n\n\n\n1 min\n\n\n\nMath\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality\n\n\n\n\n\n1 min\n\n\n\nMath\n\n\nProbability Theory\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Git Commands\n\n\nEver thought of Git graphs as linked lists?\n\n\n7 min\n\n\n\nGit\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTmux\n\n\nTerminal multiplexer\n\n\n5 min\n\n\n\nLinux\n\n\n\n\n\n\n\nShane Oh\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Made with ♥ by Shane Oh"
  },
  {
    "objectID": "posts/Entropy.html",
    "href": "posts/Entropy.html",
    "title": "Entropy",
    "section": "",
    "text": "I highly recommend taking a look at the Huffman Coding post first. Huffman coding provides an optimal compression solution for a given data distribution, whereas Shannon-Fano coding does not.\nIt may be easier for us to first learn Huffman coding (a bottom-up approach to building the tree) in the algorithms class and then move on to Shannon-Fano coding (a top-down approach).\nTake a look at the video."
  },
  {
    "objectID": "posts/Entropy.html#recap-of-huffman-coding",
    "href": "posts/Entropy.html#recap-of-huffman-coding",
    "title": "Entropy",
    "section": "Recap of Huffman coding",
    "text": "Recap of Huffman coding\nIn the last post, we derived the average codeword length of Huffman coding.\n\\[\n\\bar \\lambda = \\mathbb E [ \\lambda ]\n= \\sum_{i=1}^N p(x_i) \\lambda_i\n= \\sum_{i=1}^N p(x_i) \\lceil \\log_2 \\dfrac{1}{p(x_i)} \\rceil\n\\]\nOne thing we had a hard time deriving was \\(\\lambda\\). We needed to check the frequency (or probability) of each character and aggregate them into a binary tree structure. After that, we traced the path from the root node to each leaf, encoding every character into a binary codeword. Finally, we mapped all of the codewords to the function len to obtain our most desired value, the length of each codeword.\nThe value \\(\\lambda\\) can be interpreted as the number of bits (information) required to losslessly represent an arbitrary group (such as a character or color) within the given data. As you can feel from the word group, we were working on a discrete random variable.\nWe can guarantee the optimality of \\(\\lambda\\) only with the given data distribution. If different data is provided, the frequency (probability) changes, causing the entire Huffman tree to differ from before.\n\n\n\n\n\n\nWhat if we provide a data only with the most frequent character from the existing Huffman tree?\n\n\n\nThis will reduce \\(\\bar \\lambda\\) exceptionally but not with an optimal length. If all characters are the same, the optimal length for the given data will be 0 because the leaf node will also be the root."
  },
  {
    "objectID": "posts/Entropy.html#entropy-as-a-lower-bound-of-bar-lambda",
    "href": "posts/Entropy.html#entropy-as-a-lower-bound-of-bar-lambda",
    "title": "Entropy",
    "section": "Entropy as a lower bound of \\(\\bar \\lambda\\)",
    "text": "Entropy as a lower bound of \\(\\bar \\lambda\\)\nWhat if I say we don’t need all of the cumbersome processes mentioned above? Take a look at the formula below.\n\\[\nH(X) = \\mathbb E[I(X)] = \\sum_{i=1}^N p(x_i) \\log_2 \\dfrac{1}{p(x_i)}\n\\]\nIs entropy just another form of average codeword length?\nThe answer is no. This works as the theoretical lower bound for any data distribution (both discrete and continuous) when compressing. We can not get below this bound if we are performing a lossless compression.\nWe can see that the \\(\\lambda_i\\) has turned into a \\(\\log\\) form with a probability. Since \\(x_i\\) is the only parameter, we can define \\(H\\) as a scalar funciton for any random vaiable \\(X\\). As the probability of \\(x_i\\) increases we can compress the information into a smaller, more compact \\(I(x_i)\\).\nWe can set the theoretical lower bound for \\(\\bar \\lambda\\) as below.\n\\[\nH(X) \\leq \\bar \\lambda\n\\]\nLet’s prove this!"
  },
  {
    "objectID": "posts/Entropy.html#proofing-the-lower-bound-of-bar-lambda",
    "href": "posts/Entropy.html#proofing-the-lower-bound-of-bar-lambda",
    "title": "Entropy",
    "section": "Proofing the lower bound of \\(\\bar \\lambda\\)",
    "text": "Proofing the lower bound of \\(\\bar \\lambda\\)\nWe need to know some basics of Jensen’s inequality and Kraft-McMillan inequality to prove this. Check out the Jensen’s inequality, Kraft-McMillan inequality posts to see more.\n\\[\n\\begin{align*}\nH(X) - \\bar{\\lambda}\n&= \\sum_{i=1}^N \\left( p(x_i) \\dfrac{1}{\\log_2 p(x_i)} - p(x_i) \\lambda_i \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\left( \\dfrac{1}{\\log_2 p(x_i)} - \\lambda_i \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\left( \\dfrac{1}{\\log_2 p(x_i)} \\log_2 2^{-\\lambda_i} \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\log_2 \\left( \\dfrac{2^{-\\lambda_i}}{p(x_i)} \\right)\n   \\leq \\log_2 \\sum_{i=1}^N p(x_i) \\dfrac{2^{-\\lambda_i}}{p(x_i)} &\\cdots \\text{Jensen's inequality} \\\\[10pt]\n&= \\log_2 \\sum_{i=1}^N 2^{-\\lambda_i} \\leq \\log_2 1 &\\cdots \\text{Kraft-McMillan inequality} \\\\[10pt]\n&= 0\n\\end{align*}\n\\]\nWell, that’s it! For fun, let’s take a look at the graph below.\n\n\n\n\n\n\nFigure 1: Lower bound of lambda bar\n\n\n\nThink \\(\\varphi(x)\\) as \\(-\\log_2 p(x)\\). You can clearly see that entropy (the blue cross) is working as a lower bound.\nWe have not talked about the upper bound of \\(\\bar \\lambda\\).\n\\[\nH(X) \\leq \\bar \\lambda \\lt H(X)+1\n\\]\nThe red and blue crosses are the weighted averages of each function’s outputs. Try ripping the crosses apart from each other. Can you make it greater than 1? Probably not. Infinitely approaching all \\(p(x_i)\\) to values near powers of 2 from the left-hand side is the most probable way, but you won’t be able to get there.\nIf you do get there, it means you are allowing all \\(x_i\\) to have an extra bit, which is not optimal in the first place."
  },
  {
    "objectID": "posts/GoConcurrency.html",
    "href": "posts/GoConcurrency.html",
    "title": "Go Concurrency",
    "section": "",
    "text": "Checkout the video by Ben Davis. Great explanation!\nI recently started learning Go (Golang) and I find it easy to pick up. There isn’t much magic involved, which gives me a solid, reliable feeling when working with it.\nLet’s get right into the concurrency world of Go."
  },
  {
    "objectID": "posts/GoConcurrency.html#wait-group",
    "href": "posts/GoConcurrency.html#wait-group",
    "title": "Go Concurrency",
    "section": "Wait Group",
    "text": "Wait Group\nSo, what is a WaitGroup?\nA WaitGroup waits for a collection of goroutines to finish. The main goroutine calls Add to set the number of goroutines to wait for. Each of the goroutines runs and calls Done when finished. We make sure to call Done with a defer keyword. At the same time, you can use Wait to block until all goroutines have finished.\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nfunc main() {\n    names := []string{\n        \"Alice\",\n        \"Bob\",\n        \"Chuck\",\n        \"Dan\",\n        \"Ed\",\n        \"Fred\",\n        \"Greg\",\n    }\n\n    var wg sync.WaitGroup\n\n    for _, name := range names {\n        wg.Add(1)\n        go func(name string) {\n            defer wg.Done()\n            sayHello(name)\n        }(name)\n    }\n    wg.Wait()\n}\n\nfunc sayHello(name string) {\n    fmt.Printf(\"Hello %v\\n\", name)\n}\nHello Greg\nHello Ed\nHello Fred\nHello Alice\nHello Chuck\nHello Dan\nHello Bob\n\n\n\n\n\n\nNote\n\n\n\nThink of wg as a counter. The counter increments with the values passed into Add and decreases by one with the Done method."
  },
  {
    "objectID": "posts/GoConcurrency.html#channels",
    "href": "posts/GoConcurrency.html#channels",
    "title": "Go Concurrency",
    "section": "Channels",
    "text": "Channels\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n1    ch := make(chan int)\n\n    go func() {\n        ch &lt;- 1\n        ch &lt;- 2\n        ch &lt;- 3\n2        close(ch)\n    }()\n\n    fmt.Println(&lt;-ch)\n    fmt.Println(&lt;-ch)\n    fmt.Println(&lt;-ch)\n3    fmt.Println(&lt;-ch)\n}\n\n1\n\nA new channel is spawned with a make function.\n\n2\n\nWe need to close the channel with close function, otherwise the fourth &lt;-ch will cause a dead lock since all goroutines are asleep.\n\n3\n\nReceiving from a closed channel does not cause a panic or runtime error. In this case, 0 is printed.\n\n\n1\n2\n3\n0\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    ch := make(chan int)\n    value := make(chan int)\n\n    go func() {\n        for i := 1; i &lt; 4; i++ {\n            ch &lt;- i\n        }\n        close(ch)\n    }()\n\n    go sum(ch, value)\n\n1    fmt.Println(&lt;-value)\n}\n\nfunc sum(ch, value chan int) {\n    total := 0\n\n2    for {\n        select {\n3        case num, ok := &lt;-ch:\n            fmt.Println(num, ok)\n            if !ok {\n                value &lt;- total\n                return\n            }\n            total += num\n        }\n    }\n}\n\n1\n\nThis line waits for all the goroutines to be finished.\n\n2\n\nSyntactic sugar with for and select. You can select from multiple channels like this.\n\n3\n\nThe second return value is a boolean which indicates the channel is open or not.\n\n\n1 true\n2 true\n3 true\n0 false\n6"
  },
  {
    "objectID": "posts/JensensInequality.html",
    "href": "posts/JensensInequality.html",
    "title": "Jensen’s Inequality",
    "section": "",
    "text": "Checkout this post to see the background behind this.\nJensen’s inequality generalizes the statement that the secant line of a convex function lies above the graph of the function1.\n\\(f:\\mathbb R \\to \\mathbb R\\) is convex when,\n\\[\n\\forall t \\in [0,1], f(tx_1 + (1-t)x_2) \\leq tf(x_1)+(1-t)f(x_2)\n\\]"
  },
  {
    "objectID": "posts/JensensInequality.html#jensens-inequality-with-probability-theory",
    "href": "posts/JensensInequality.html#jensens-inequality-with-probability-theory",
    "title": "Jensen’s Inequality",
    "section": "Jensen’s inequality with probability theory",
    "text": "Jensen’s inequality with probability theory\n\n\n\n\n\n\nFigure 2: Jensen’s inequality with probability theory\n\n\n\nWhen \\(\\varphi: \\mathbb R \\to \\mathbb R\\) is a convex function,\n\\[\n\\begin{align*}\n\\varphi(\\mathbb E [X]) &\\leq \\mathbb E [\\varphi(X)] \\\\[10pt]\n\\varphi \\left( \\sum p(x_i) \\ x_i \\right) &\\leq \\sum p(x_i) \\varphi(x_i) \\\\\n\\end{align*}\n\\]\n\nFinite form\nWhen \\(\\varphi: \\mathbb R \\to \\mathbb R\\) is a convex function,\n\\[\n\\varphi \\left( \\dfrac{\\sum a_i x_i}{\\sum a_i} \\right) \\leq \\dfrac{\\sum a_i \\varphi(x_i)}{\\sum a_i}\n\\]\nJensen’s inequality also can be applied under weighted average conditions. Weighted average of \\(\\varphi(x_i)\\) can reside in the dashed quardrangle (including the dashed line).\n\n\nExamples\n\\[\n\\mathrm{Var}[X] = \\mathbb E[X^2] - \\mathbb E[X]^2\n\\]\nWhen \\(\\varphi: x \\mapsto x^2\\), \\(\\varphi\\) is convex, which makes \\(\\mathbb E[X]^2 \\leq \\mathbb E[X^2]\\). This is correct because the variance of random variables cannot be negative."
  },
  {
    "objectID": "posts/JensensInequality.html#footnotes",
    "href": "posts/JensensInequality.html#footnotes",
    "title": "Jensen’s Inequality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJensen’s inequality - Wikipedia↩︎"
  },
  {
    "objectID": "posts/GeometricSequence.html",
    "href": "posts/GeometricSequence.html",
    "title": "Geometric Seuqnece",
    "section": "",
    "text": "When adding up terms in a sequence with a specific pattern, such as a geometric sequence, using a formula or recognizing a pattern can make the process much easier.\nLet’s see how it works.\n\\[\na, ar, ar^2, ar^3, \\cdots, ar^n\n\\]\nThe sequence above is an arbitrary geometric sequence, where each character denotes the following:\n\n\\(a\\): the first term\n\\(r\\): the common ratio\n\nThe sum from the first term to the n-th term can be calculated using the following formula.\n\\[\n\\begin{align*}\nS_n &= a + ar + ar^2 + \\cdots + ar^{n-1} \\\\\nrS_n &= ar + ar^2 + ar^3 \\cdots + ar^n \\\\\nrS_n - S_n &= ar^n - a \\\\\nS_n &= \\dfrac{a(r^n - 1)}{r-1}\n\\end{align*}\n\\]\nThis formula can be easily seen when counting nodes in a complete binary tree or calculating the moving average of a gradient using the momentum."
  },
  {
    "objectID": "posts/GAN.html",
    "href": "posts/GAN.html",
    "title": "GAN",
    "section": "",
    "text": "There is an interactive playground available at GAN Lab. Feel free to explore it."
  },
  {
    "objectID": "posts/GAN.html#zero-sum-game-of-a-generator-and-a-discriminator",
    "href": "posts/GAN.html#zero-sum-game-of-a-generator-and-a-discriminator",
    "title": "GAN",
    "section": "Zero-sum game of a Generator and a Discriminator",
    "text": "Zero-sum game of a Generator and a Discriminator\nThe zero-sum property (if one gains, another loses) means that any result of a zero-sum situation is Pareto optimal which is also called a conflict game1.\nThe generator and discriminator engage in a zero-sum game, where the generator tries to produce data that fools the discriminator, while the discriminator aims to correctly identify real versus generated (fake) data. This interaction can be expressed with a payoff matrix.\n\n\n\n\n\n\n\n\\(D_\\text{good}\\)\n\n\n\\(D_\\text{poor}\\)\n\n\n\n\n\\(G_\\text{good}\\)\n\n\n\\(( 0, 0)\\)\n\n\n\\(( 1,-1)\\)\n\n\n\n\n\\(G_\\text{poor}\\)\n\n\n\\((-1, 1)\\)\n\n\n\\(( 0, 0)\\)\n\n\n\n\n\n\nThe payoff matrix shows all the combinations of what players can move. The gains for \\(G\\) and \\(D\\) in each state are denoted by tuples.\nThe best choice for \\(D\\), regardless of what \\(G\\) chooses, is \\(D_\\text{good}\\). When \\(G\\) chooses \\(G_\\text{good}\\), \\(D\\) can move from \\(-1\\) to \\(0\\), gaining \\(+1\\), and when \\(G\\) chooses \\(G_\\text{poor}\\), \\(D\\) can move from \\(0\\) to \\(1\\), also gaining \\(+1\\), making \\(D_\\text{good}\\) a dominant strategy. Same for \\(G\\), making \\(G_\\text{good}\\) a dominant strategy.\nThe solution for this game is choosing \\(D_\\text{good}\\) and \\(G_\\text{good}\\) which is considered as a Nash Equilibrium in GANs, maximizing both players’ gains.\nHowever, the question of whether a Nash Equilibrium exists in the GAN framework remains open. Read Farnia and Ozdaglar (2020) to find out more.\nSince the GAN framework can be modeled as a zero-sum game, we can also derive the same Nash Equilibrium using the second element of the tuples (\\(D\\)’s gain), which provides a more compact representation. This becomes the value function."
  },
  {
    "objectID": "posts/GAN.html#the-value-function",
    "href": "posts/GAN.html#the-value-function",
    "title": "GAN",
    "section": "The Value Function",
    "text": "The Value Function\n\\[\n\\min_G \\max_D V(D,G)\n= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D(G(\\mathbf z)))]\n\\]\nThe generator takes a latent variable \\(\\mathbf z\\) as input and outputs generated data \\(\\mathbf x\\). The discriminator takes data \\(\\mathbf x\\) as input and outputs a probability \\(y\\), representing whether the data is real (\\(1\\)) or fake (\\(0\\)).\n\\[\n\\begin{align*}\nG &:\\mathbf z \\to \\mathbf x \\\\\nD &:\\mathbf x \\to y\n\\end{align*}\n\\]\nThe value function (\\(V\\)) consists of two log-likelihood losses, each from a Bernoulli distribution: one representing the genuine data distribution and the other, the fake data distribution.\nLet \\(\\mathbf{x} \\sim p_\\text{data}(\\mathbf{x})\\) represent a sample drawn from the genuine data distribution. They are all labeled as \\(1\\). Same thing for the fake data \\(\\mathbf{z} \\sim p_\\mathbf{z}(\\mathbf{z})\\). They are all labeled as \\(0\\).\n\n\n\n\n\n\nNote\n\n\n\nDuring training, we will keep the number of samples for \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) the same.\n\n\n\nObjective of \\(D\\)\nThe Binary Cross-Entropy (BCE) is defined as follows.\n\\[\n\\mathcal L_\\text{BCE}(\\hat y, y)\n= - \\lbrace y \\log{\\hat y} + (1-y) \\log{(1-\\hat y)} \\rbrace\n\\]\nWe can try to optimize the model by decreasing the \\(\\mathcal L\\). Conversely, increasing \\(-\\mathcal L\\) resembles the same objective. When dealing with \\(D\\)’s loss, we use the latter approach.\nThe loss of \\(D\\) is calculated with negative BCE loss on both real and fake data distributions and are added together.\n\\[\n\\max_D V(D,G) =\n\\sum_{\\mathbf x}\n-\\mathcal L_\\text{BCE}(D(\\mathbf x), 1) +\n\\sum_{\\mathbf z}\n-\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 0)\n\\]\n\\(D\\) will try to maximize the value function \\(V(D,G)\\) thus, \\(\\theta_\\text{d}\\) is updated with gradient ascent.\n\n\nObjective of \\(G\\)\nIn \\(G\\)’s perspective, \\(G\\)’s objective is to fool \\(D\\) by creating more realistic data.\n\\[\n\\min_G V(G) =\n\\sum_{\\mathbf z}\n-\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 0) \\\\\n\\]\n\\(G\\) will try to minimize the value function \\(V(G)\\) thus, \\(\\theta_\\text{d}\\) is updated with gradient descent.\n\n\n\n\n\n\nUnifying the loss with respect to \\(V\\)\n\n\n\nNote that we are not using \\(\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 1)\\) as the value function for \\(G\\). In the context of zero-sum loss, we will try to unify the losses in terms of \\(V\\)."
  },
  {
    "objectID": "posts/GAN.html#theoretical-results",
    "href": "posts/GAN.html#theoretical-results",
    "title": "GAN",
    "section": "Theoretical Results",
    "text": "Theoretical Results\n\n\n\n\n\n\nFigure 1: Training GANs\n\n\n\nThe figure above is from Goodfellow et al. (2014). The genuine data distribution \\(p_\\text{data}\\) is represented by a black dotted line. and green denotes the generated distribution \\(p_\\text{g}\\), and lastly the color blue denotes the discriminated distribution,\nAs mentioned above, the output of \\(D\\) is the probability that the data is genuine, so the height of the blue bar represents the corresponding value between \\([0, 1]\\). The value of the blue distribution is one-half on \\(x\\), where \\(x\\) represents the intersection of the black and blue lines.\n\n\n\n\n\n\nNote\n\n\n\nThe results of this section are done in a non-parametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions; \\(p_\\text{data}\\) and \\(p_\\text{g}\\).\n\n\nWe reach the global optimum of \\(V\\) when we approach optimal \\(D\\) and \\(G\\).\n\nOptimal \\(D\\)\nOptimal \\(D\\) can be obtained by maximizing \\(V\\). For any given \\(G\\), \\(D\\) will try it’s best to discriminate genuine data from the fake.\n\\[\\begin{align*}\nV(D,G)\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D(G(\\mathbf z)))] \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x) d \\mathbf x\n+ \\int_\\mathbf{z} p_\\mathbf{z}(\\mathbf z) \\log (1 - D(G(\\mathbf z))) d \\mathbf z \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x) d \\mathbf x\n+ \\int_\\mathbf{x} p_\\text{g}(\\mathbf x) \\log (1 - D(\\mathbf x)) d \\mathbf x \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x)\n                + p_\\text{g}(\\mathbf x) \\log (1 - D(\\mathbf x)) d \\mathbf x\n\\end{align*}\\]\nBy setting \\(p_\\text{data}(\\mathbf x)\\) as \\(a\\) and \\(p_\\text{g}(\\mathbf x)\\) as \\(b\\), the inner part of the integral can be expressed as the following.\n\\[\\begin{align*}\nf(y) &= a \\log y + b \\log (1-y) \\\\\n\\dfrac{d}{dy} f(y) &= \\dfrac{a}{y} - \\dfrac{b}{1-y} = 0 \\\\\n\\therefore y &= \\frac{a}{a+b}\n\\end{align*}\\]\nThe optimal \\(D\\) can be defined as follows.\n\\[\nD^*(\\mathbf x) = \\dfrac{p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\]\n\n\nOptimal \\(G\\)\nWith same \\(V\\), optimal \\(G\\) can be obtained by minimizing \\(V\\). We will find optimal \\(G\\) with respective to the optimal discriminator \\(D^*\\).\n\\[\\begin{align*}\nV(D^*,G)\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D^*(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D^*(G(\\mathbf z)))] \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{p_\\text{g}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right] \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{2 \\ p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{2 \\ p_\\text{g}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n- \\log 4 \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{p_\\text{data}(\\mathbf x)}{\\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2}}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{p_\\text{g}(\\mathbf x)}{\\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2}}\n\\right]\n- \\log 4\n\\\\\n\n&= D_\\text{KL} \\left( p_\\text{data}(\\mathbf x) \\bigg\\| \\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2} \\right)\n+ D_\\text{KL} \\left( p_\\text{g}(\\mathbf x) \\bigg\\| \\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2} \\right) - \\log 4 \\\\\n\n&= 2 \\ \\mathrm{JSD}(p_\\text{data}(\\mathbf x) \\| p_\\text{g}(\\mathbf x)) - \\log 4 \\\\\n\n&\\geq -\\log 4 \\\\[20pt]\n\n\\min_{G} V(D^*, G) &= -\\log 4 \\iff p_\\text{data} = p_\\text{g} \\\\\n\nV(D^*,G^*) &= -\\log4\n\\end{align*}\\]\n\n\n\n\n\n\nDoes the order of \\(\\min\\), \\(\\max\\) matter?\n\n\n\nYes, it does.\nMathematically, \\(\\min_G \\max_D V(D, G)\\) is the same as \\(\\min_G (\\max_D V(D, G))\\).\nWe try to solve from the inner parentheses.\nIn GAN’s perspective, as long as \\(D\\) tries its best to discriminate the two distributions, the best strategy for \\(G\\) is to mimic \\(p_\\text{data}\\) as closely as possible."
  },
  {
    "objectID": "posts/GAN.html#footnotes",
    "href": "posts/GAN.html#footnotes",
    "title": "GAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia - Zero-sum game↩︎"
  },
  {
    "objectID": "posts/Git.html",
    "href": "posts/Git.html",
    "title": "Local Git Commands",
    "section": "",
    "text": "Thanks Cottle for creating this educational tool. Ever thought of Git graphs as linked lists? Here is a great explanation.\nFirst things first,"
  },
  {
    "objectID": "posts/Git.html#git-commit",
    "href": "posts/Git.html#git-commit",
    "title": "Local Git Commands",
    "section": "git commit",
    "text": "git commit\nThis creates a new commit C1, which references where it was based off of — in this case, a C0 which is a initial commit becomes the parent.\ngit commit\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  main:::pointer -.-&gt; C0:::commit\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-branch",
    "href": "posts/Git.html#git-branch",
    "title": "Local Git Commands",
    "section": "git branch",
    "text": "git branch\nBranches is Git are simply pointers to a specific commit – nothing more. This is why many Git enthusiasts chant the mantra\n\nbranch early, and branch often.\n\nWhen we start mixing branches (pointers) and commits, we will see how these two features combine. For now, just remember that a branch essentially says\n\nI want to include the work of this commit and all parent commits.\n\ngit checkout main\ngit branch dev\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  dev:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\ngit checkout main\ngit checkout -b dev\ngit checkout main\ngit switch -c dev\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  dev:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-merge",
    "href": "posts/Git.html#git-merge",
    "title": "Local Git Commands",
    "section": "git merge",
    "text": "git merge\nmerge command eventually creates a special commit which has two unique parents. A commit with two parents essentially means\n\nI want to include all the work from both parents, and the set of all their parents.\n\ngit checkout main\n1git merge dev\ngit checkout dev\n2git merge main\n\n1\n\nThe command merges dev branch into the current main branch. This leaves the dev branch (pointer) behind.\n\n2\n\nWe don’t need to derive the work from both the main and dev parents again because the dev graph is a subgraph of the main graph. Instead, we can simply move the dev pointer to match the position of the main pointer. This is also called fast forward.\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C4\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-rebase",
    "href": "posts/Git.html#git-rebase",
    "title": "Local Git Commands",
    "section": "git rebase",
    "text": "git rebase\nrebase copies the commits and stack them on somewhere else.\n\n\n\n\n\n\nNote\n\n\n\nThe position of the HEAD pointer is different when merging and rebasing.\ngit checkout main # &lt;- HEAD\ngit merge dev\ngit checkout dev # &lt;- HEAD\ngit rebase main\nWhen rebasing, we are willing to rebase with copied commits onto main.\n\n\ngit checkout dev\ngit rebase main\ngit checkout main\ngit merge dev\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C4'\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-checkout",
    "href": "posts/Git.html#git-checkout",
    "title": "Local Git Commands",
    "section": "git checkout",
    "text": "git checkout\nDidn’t we use the git checkout command without any discomfort so far? What checkout does is point HEAD to the desired object such as a branch or a commit.\ngit checkout 1b7979e16daafabf7c052411b083ea9e2e8a13d5\n\nRelative reference (^ and ~)\ngit checkout C1\ngit checkout C2^\ngit checkout main^^\ngit checkout main~2\ngit checkout HEAD^; git checkout HEAD^\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C2:::commit\n  main:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C2:::commit\n  main:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; C1\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\nBranch forcing (git branch -f)\nThis is called branch forcing.\ngit branch -f feature dev^\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  C5:::commit --&gt; C4:::commit\n  C6:::commit --&gt; C5:::commit\n  C7:::commit --&gt; C4:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C6\n  feature:::pointer -.-&gt; C7\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  C5:::commit --&gt; C4:::commit\n  C6:::commit --&gt; C5:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C6\n  feature:::pointer -.-&gt; C5\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-reset",
    "href": "posts/Git.html#git-reset",
    "title": "Local Git Commands",
    "section": "git reset",
    "text": "git reset\nThe reset command is often used to undo changes that have been staged or committed. This sets the HEAD to the desired commit object. The most common command will be as follows.\nUndoing the add command.\ngit reset\ngit reset HEAD\nThis command moves HEAD to the parent of the current commit (HEAD^), effectively undoing the most recent commit.\ngit reset HEAD^\nUndo a commit and make a topic branch out of it.\ngit branch topic/foo\ngit reset --hard HEAD~3\ngit checkout topic/foo"
  },
  {
    "objectID": "posts/Git.html#git-revert",
    "href": "posts/Git.html#git-revert",
    "title": "Local Git Commands",
    "section": "git revert",
    "text": "git revert\nrevert is a command that creates a new commit that undoes the changes made by a previous commit. This means that instead of deleting or altering past commits, git revert adds a new commit on top of the branch.\nWhen HEAD is on the merge commit,\ngit revert HEAD -m 1\ngit revert HEAD -m 2 \nThe number (index) of a parent can be checked with the command git cat-file -p HEAD."
  }
]