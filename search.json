[
  {
    "objectID": "posts/TraefikHelm.html",
    "href": "posts/TraefikHelm.html",
    "title": "Traefik dashboard with Helm",
    "section": "",
    "text": "Check the official Github.\nhelm repo add traefik https://traefik.github.io/charts\nkubectl create ns traefik\nhelm install traefik traefik/traefik \\\n  --set ingressRoute.dashboard.enabled=true \\\n  --namespace traefik"
  },
  {
    "objectID": "posts/TraefikHelm.html#install-traefik-using-helm",
    "href": "posts/TraefikHelm.html#install-traefik-using-helm",
    "title": "Traefik dashboard with Helm",
    "section": "",
    "text": "Check the official Github.\nhelm repo add traefik https://traefik.github.io/charts\nkubectl create ns traefik\nhelm install traefik traefik/traefik \\\n  --set ingressRoute.dashboard.enabled=true \\\n  --namespace traefik"
  },
  {
    "objectID": "posts/TraefikHelm.html#accessing-the-dashboard",
    "href": "posts/TraefikHelm.html#accessing-the-dashboard",
    "title": "Traefik dashboard with Helm",
    "section": "Accessing the dashboard",
    "text": "Accessing the dashboard\n$ k get po -n traefik\nNAME                       READY   STATUS    RESTARTS   AGE\ntraefik-6d574648c7-gwb8t   1/1     Running   0          18m\n\n$ k port-forward -n traefik traefik-6d574648c7-gwb8t 8080:8080\nForwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\nAccess through http://localhost:8080/dashboard/."
  },
  {
    "objectID": "posts/RookCeph.html",
    "href": "posts/RookCeph.html",
    "title": "Rook Ceph",
    "section": "",
    "text": "Rook is a cloud-native storage orchestrator for Kubernetes, and Ceph is a highly scalable distributed storage solution. Together, they provide a powerful storage management system for Kubernetes clusters.\nCeph integrates with Kubernetes using the Container Storage Interface (CSI). CSI provides a standardized mechanism for Kubernetes to manage storage systems, allowing dynamic provisioning and lifecycle management of storage volumes.\n\n\nKubernetes supports two types of Persistent Volume (PV) provisioning: dynamic and static.\n\nStatic Provisioning: In this method, cluster administrators manually create Persistent Volumes (PVs) before they can be claimed by applications. This requires predefining storage resources, which can be inefficient and lead to resource underutilization.\nDynamic Provisioning: This method enables Kubernetes to automatically provision storage resources when a Persistent Volume Claim (PVC) is requested by an application. It eliminates the need for pre-created PVs, making storage management more flexible and scalable. A StorageClass defines the parameters for dynamic provisioning of PVs.\n\nRook Ceph is one of the dynamic storage solution which is widely used in cloud native architectures, and it is just 2 lines of kubectl create away! Check out more intriguing CNCF projects from the Cloud Native Landscape."
  },
  {
    "objectID": "posts/RookCeph.html#introduction",
    "href": "posts/RookCeph.html#introduction",
    "title": "Rook Ceph",
    "section": "",
    "text": "Rook is a cloud-native storage orchestrator for Kubernetes, and Ceph is a highly scalable distributed storage solution. Together, they provide a powerful storage management system for Kubernetes clusters.\nCeph integrates with Kubernetes using the Container Storage Interface (CSI). CSI provides a standardized mechanism for Kubernetes to manage storage systems, allowing dynamic provisioning and lifecycle management of storage volumes.\n\n\nKubernetes supports two types of Persistent Volume (PV) provisioning: dynamic and static.\n\nStatic Provisioning: In this method, cluster administrators manually create Persistent Volumes (PVs) before they can be claimed by applications. This requires predefining storage resources, which can be inefficient and lead to resource underutilization.\nDynamic Provisioning: This method enables Kubernetes to automatically provision storage resources when a Persistent Volume Claim (PVC) is requested by an application. It eliminates the need for pre-created PVs, making storage management more flexible and scalable. A StorageClass defines the parameters for dynamic provisioning of PVs.\n\nRook Ceph is one of the dynamic storage solution which is widely used in cloud native architectures, and it is just 2 lines of kubectl create away! Check out more intriguing CNCF projects from the Cloud Native Landscape."
  },
  {
    "objectID": "posts/RookCeph.html#installing",
    "href": "posts/RookCeph.html#installing",
    "title": "Rook Ceph",
    "section": "Installing",
    "text": "Installing\nRefer to the official quickstart guide for starting. The slack channel was a big help for me when trouble shooting. Thanks Madhu!\nThe hardware setup I used includes:\n\nA RaspberryPi cluster (1 \\(\\times\\) Control-plane + 3 \\(\\times\\) Worker nodes)\n3 \\(\\times\\) USB thumb drives (128GB of storage each)\n\n\nAttatching Pysical Storage\nFollow the prerequisites when setting up the cluster for the first time.\n$ lsblk -f\n\nNAME        FSTYPE   LABEL       UUID        FSAVAIL FSUSE% MOUNTPOINT\nloop0       squashfs                               0   100% /snap/core20/1614\nloop1       squashfs                               0   100% /snap/core20/2437\nloop2       squashfs                               0   100% /snap/lxd/22761\nloop3       squashfs                               0   100% /snap/lxd/29631\nloop4       squashfs                               0   100% /snap/snapd/23546\nloop5       squashfs                               0   100% /snap/snapd/23259\n1sda\nmmcblk0\n├─mmcblk0p1 vfat     system-boot 5D5B-XXXX      131M    48% /boot/firmware\n└─mmcblk0p2 ext4     writable    a7c22XXXX    104.5G     7% /\n\n1\n\nThe drives were recognized as /dev/sda on each worker node. They should NOT have a file system or be mounted. If they do, use wipefs --all /dev/sda or zap the device.\n\n\n\n\n\n\n\n\nUsing a Thumb Drive\n\n\n\nThere is a known issue when configuring Ceph with thumb drives. Check this section before provisioning.\n\n\n\n\n\n\n\n\nNot a Fresh Install?\n\n\n\nDelete all /var/lib/rook files on each node. Details are here.\n\n\n\n\nCreating Resources\n$ git clone https://github.com/rook/rook.git\ncd rook/deploy/examples\nkubectl create -f crds.yaml -f common.yaml -f operator.yaml\nkubectl create -f cluster.yaml\n$ kubectl get pods -n rook-ceph\nNAME                                                 READY   STATUS      RESTARTS   AGE\ncsi-cephfsplugin-59rx4                               3/3     Running     0          12h\ncsi-cephfsplugin-dhkpd                               3/3     Running     0          12h\ncsi-cephfsplugin-provisioner-cccbd7c6d-m7844         6/6     Running     0          12h\ncsi-cephfsplugin-provisioner-cccbd7c6d-wjvt6         6/6     Running     0          12h\ncsi-cephfsplugin-vjdvb                               3/3     Running     0          12h\ncsi-rbdplugin-fp67b                                  3/3     Running     0          12h\ncsi-rbdplugin-gfnxh                                  3/3     Running     0          12h\ncsi-rbdplugin-provisioner-897c4d994-77kxn            6/6     Running     0          12h\ncsi-rbdplugin-provisioner-897c4d994-l5k6n            6/6     Running     0          12h\ncsi-rbdplugin-t7kwq                                  3/3     Running     0          12h\nrook-ceph-crashcollector-knode-02-7d68dbc866-qsttt   1/1     Running     0          12h\nrook-ceph-crashcollector-knode-03-78b9d64f4f-g4bjx   1/1     Running     0          12h\nrook-ceph-crashcollector-knode-04-78bbd5d99b-7cj5t   1/1     Running     0          12h\nrook-ceph-exporter-knode-02-7d5f47b679-l9wvr         1/1     Running     0          12h\nrook-ceph-exporter-knode-03-7bcf6b9b88-cfvlk         1/1     Running     0          12h\nrook-ceph-exporter-knode-04-7d869467d9-q6pb9         1/1     Running     0          12h\nrook-ceph-mgr-a-5d69dfbd98-5cvdd                     3/3     Running     0          12h\nrook-ceph-mgr-b-6888bccd5c-nbf7b                     3/3     Running     0          12h\nrook-ceph-mon-a-5745c67854-xhccz                     2/2     Running     0          12h\nrook-ceph-mon-b-84c46c4589-rjptk                     2/2     Running     0          12h\nrook-ceph-mon-c-785c749569-p99wl                     2/2     Running     0          12h\n1rook-ceph-operator-79f8754564-9bt5x                  1/1     Running     0          12h\n3rook-ceph-osd-0-856fb9cd8f-c55h6                     2/2     Running     0          12h\nrook-ceph-osd-1-856979b788-4zpbn                     2/2     Running     0          11h\nrook-ceph-osd-2-6df96bd8b7-gghrd                     2/2     Running     0          11h\n2rook-ceph-osd-prepare-knode-02-cpspg                 0/1     Completed   0          11h\nrook-ceph-osd-prepare-knode-03-s7d97                 0/1     Completed   0          11h\nrook-ceph-osd-prepare-knode-04-5922p                 0/1     Completed   0          11h\n4rook-ceph-tools-7dd7bbcd4b-vmfvj                     1/1     Running     0          12h\n\n1\n\nThe rook-ceph-operator is installed using the operator.yaml file. When create or apply is used for new configurations, the operator detects the changes and updates the cluster to the desired state. If the cluster does not automatically apply the desired state, delete the operator pod using kubectl delete to force a restart, which will trigger a re-evaluation of the current state.\n\n\n2\n\nAs the name suggests, rook-ceph-osd-prepare prepares the available disks on each node. It automatically detects the disks based on the selector defined in cluster.yaml.\n\n\n3\n\nThe rook-ceph-osd is the actual daemon running on each node. It takes over five minutes for all instances to become fully operational.\n\n\n4\n\nThe Toolbox can be installed and used for Rook debugging. Use exec to access the rook-ceph-tools pod for inspection.\n\n\n\n\ntoolbox\n\n$ kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash\nbash-5.1$ ceph status\n  cluster:\n    id:     11aa2177-be62-417d-83f5-46e8bb97ecd1\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 12h)\n    mgr: a(active, since 12h), standbys: b\n    osd: 3 osds: 3 up (since 11h), 3 in (since 12h)\n\n  data:\n    pools:   2 pools, 33 pgs\n    objects: 7 objects, 449 KiB\n    usage:   95 MiB used, 351 GiB / 352 GiB avail\n    pgs:     33 active+clean\n\nbash-5.1$ ceph df\n--- RAW STORAGE ---\nCLASS     SIZE    AVAIL    USED  RAW USED  %RAW USED\nhdd    352 GiB  351 GiB  95 MiB    95 MiB       0.03\nTOTAL  352 GiB  351 GiB  95 MiB    95 MiB       0.03\n\n--- POOLS ---\nPOOL         ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL\n.mgr          1    1  449 KiB        2  1.3 MiB      0    111 GiB\nreplicapool   2   32     19 B        5   12 KiB      0    111 GiB"
  },
  {
    "objectID": "posts/RookCeph.html#uninstalling",
    "href": "posts/RookCeph.html#uninstalling",
    "title": "Rook Ceph",
    "section": "Uninstalling",
    "text": "Uninstalling\nWhen tearing down the cluster, PLEASE FOLLOW the instructions.\nAfter removing the resources from the K8s cluster, there can still be some remaining data that could interfere with the installation such as the /var/lib/rook folder defined in the cluster.yaml.\n\n\ncluster.yaml\n\napiVersion: ceph.rook.io/v1\nkind: CephCluster\nmetadata:\n  name: rook-ceph\n  namespace: rook-ceph\nspec:\n  cephVersion:\n    image: quay.io/ceph/ceph:v19.2.0\n    allowUnsupported: false\n1  dataDirHostPath: /var/lib/rook\n  ...\n\n\n1\n\nThe path on the host where configuration files will be persisted. Must be specified. If there are multiple clusters, the directory must be unique for each cluster. If you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster."
  },
  {
    "objectID": "posts/RookCeph.html#troubleshooting",
    "href": "posts/RookCeph.html#troubleshooting",
    "title": "Rook Ceph",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nWhen using a thumb drive\nBy default, Ceph does NOT accept USB thumb drives. This is due to Ceph detecting the ID_BUS value as usb and excluding such devices.\nTo work around this, create a rule in /etc/udev/rules.d/ to mimic a SCSI drive:\n\n\n/etc/udev/rules.d/99-usb-to-scsi.rules\n\nACTION==\"add|change|online\", ENV{ID_TYPE}==\"disk\", ENV{ID_BUS}==\"usb\", ENV{ID_SCSI}=\"1\"\nACTION==\"add|change|online\", ENV{ID_TYPE}==\"disk\", ENV{ID_BUS}==\"usb\", ENV{ID_BUS}=\"scsi\"\n\n$ udevadm control --reload-rules && udevadm trigger\n$ udevadm info --query=property /dev/sda | grep -i id_bus\n\n1ID_BUS=scsi\n\n1\n\nIf the old value persists, try physically detaching and reattaching the drive."
  },
  {
    "objectID": "posts/PfizerClinicalTrial.html",
    "href": "posts/PfizerClinicalTrial.html",
    "title": "Shane",
    "section": "",
    "text": "The efficacy of an arbitrary vaccine is defined by various parameters. A blog post published from the World Health Organization (commonly known as WHO) describes this in much greater detail.\n\n\n\nVaccine efficacy\n\n\nVaccine efficacy (VE) requires two sample groups for calculation. Although unequal group sizes are acceptable, best practice is to make the sample sizes as equal as possible. The control and test groups will be prescribed placebo and vaccine, respectively.\nIn the context of the protocol, the key consideration is the extent of deviation between the actual and the stipulated group sizes. The discrepancy in group sizes will later be evaluated using chi-sqaure test.\n\\[\n\\mathrm{VE} = 100 \\dfrac{\\pi_P - \\pi_V}{\\pi_P}\n\\]\n\n\\(\\pi_P\\): Proportion of subjects infected after receiving the placebo.\n\\(\\pi_V\\): Proportion of subjects infected after vaccinated.\n\nBy setting the value of \\(\\pi_P = 0.8\\) and \\(\\pi_V = 0.72\\), the VE can be calculated as \\(10\\%\\). Notice that VE is theoretically between \\(-\\infty\\) and \\(100\\).\nFor modeling, it is commonly preferred to use the numerical range between \\((0,1)\\).\n\\[\n\\theta = \\dfrac{\\pi_V}{\\pi_P + \\pi_V} = \\dfrac{1-\\frac{\\mathrm{VE}}{100}}{2-\\frac{\\mathrm{VE}}{100}}\n\\]\nThe beta distribution is likely befitting for the modeling of \\(\\theta\\) which will later be discussed in detail."
  },
  {
    "objectID": "posts/PfizerClinicalTrial.html#definition-of-vaccine-efficacy",
    "href": "posts/PfizerClinicalTrial.html#definition-of-vaccine-efficacy",
    "title": "Shane",
    "section": "",
    "text": "The efficacy of an arbitrary vaccine is defined by various parameters. A blog post published from the World Health Organization (commonly known as WHO) describes this in much greater detail.\n\n\n\nVaccine efficacy\n\n\nVaccine efficacy (VE) requires two sample groups for calculation. Although unequal group sizes are acceptable, best practice is to make the sample sizes as equal as possible. The control and test groups will be prescribed placebo and vaccine, respectively.\nIn the context of the protocol, the key consideration is the extent of deviation between the actual and the stipulated group sizes. The discrepancy in group sizes will later be evaluated using chi-sqaure test.\n\\[\n\\mathrm{VE} = 100 \\dfrac{\\pi_P - \\pi_V}{\\pi_P}\n\\]\n\n\\(\\pi_P\\): Proportion of subjects infected after receiving the placebo.\n\\(\\pi_V\\): Proportion of subjects infected after vaccinated.\n\nBy setting the value of \\(\\pi_P = 0.8\\) and \\(\\pi_V = 0.72\\), the VE can be calculated as \\(10\\%\\). Notice that VE is theoretically between \\(-\\infty\\) and \\(100\\).\nFor modeling, it is commonly preferred to use the numerical range between \\((0,1)\\).\n\\[\n\\theta = \\dfrac{\\pi_V}{\\pi_P + \\pi_V} = \\dfrac{1-\\frac{\\mathrm{VE}}{100}}{2-\\frac{\\mathrm{VE}}{100}}\n\\]\nThe beta distribution is likely befitting for the modeling of \\(\\theta\\) which will later be discussed in detail."
  },
  {
    "objectID": "posts/PfizerClinicalTrial.html#design-of-the-clinical-trial",
    "href": "posts/PfizerClinicalTrial.html#design-of-the-clinical-trial",
    "title": "Shane",
    "section": "Design of the Clinical Trial",
    "text": "Design of the Clinical Trial\nAfter the outbreak of the COVID-19, FDA of the United States has released a guidance document123 for the design of clinical trials for COVID-19 vaccines. The COVID-19 Emergency Use Authorization (EUA) has required additional safety considerations and efficacy data for the vaccine to be approved with interim analysis.\nThe following requirements are extracted from the guidance document:\n\nA point estimate for a placebo controlled efficacy trial of at least 50%, with a lower bound of the appropriately alpha-adjusted confidence interval around the primary efficacy endpoint point estimate of &gt;30%\n\nThe value \\(30%\\) was widely accepted as the null hypothesis for this clinical trial."
  },
  {
    "objectID": "posts/PfizerClinicalTrial.html#footnotes",
    "href": "posts/PfizerClinicalTrial.html#footnotes",
    "title": "Shane",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFDA In Brief: FDA Issues Guidance on Emergency Use Authorization for COVID-19 Vaccines↩︎\nDevelopment and Licensure of Vaccines to Prevent COVID-19↩︎\nEmergency Use Authorization for Vaccines to Prevent COVID-19↩︎"
  },
  {
    "objectID": "posts/NginxIngress.html",
    "href": "posts/NginxIngress.html",
    "title": "Nginx Ingress",
    "section": "",
    "text": "This document is based on a bare-metal Raspberry Pi Kubernetes (K8s) cluster, as discussed in the previous post. To facilitate external access, a load balancer like MetalLB is required. The setup of MetalLB was covered in the previous post.\nFor this guide, we will use the Ingress-Nginx Controller."
  },
  {
    "objectID": "posts/NginxIngress.html#introduction",
    "href": "posts/NginxIngress.html#introduction",
    "title": "Nginx Ingress",
    "section": "Introduction",
    "text": "Introduction\nSo, what is an ingress? There are two kinds of network communication: Ingress and Egress.\n\nIngress — Controls external access to services within the cluster, typically via HTTP/HTTPS. It acts as a gateway for handling incoming traffic and routing it to the appropriate backend services.\nEgress — Manages outbound traffic from the cluster to external services. It defines policies to regulate how pods can communicate with external networks.\n\nIngress is particularly useful for exposing services via a single endpoint while managing SSL termination, path-based routing, and load balancing. In contrast, egress rules help enforce security by restricting outbound connections. Manipulating ingress resource is similar to manipulating Nginx server with nginx.conf.\nThe basic architecture is as follows.\n\n\n\n\n\n\nFigure 1: Nginx ingress architecture\n\n\n\nThere is a detailed post about the Ingress-Nginx Controller from the AWS blog. Simply replace the load balancer in the Figure 1 with MetalLB service which is already exposed using the Ingress-Nginx Controller Service. This comes right out of the box."
  },
  {
    "objectID": "posts/NginxIngress.html#installation",
    "href": "posts/NginxIngress.html#installation",
    "title": "Nginx Ingress",
    "section": "Installation",
    "text": "Installation\nInstalling the NGINX Ingress Controller is straightforward. Refer to the official guide and apply the following configuration.\nThis setup includes two primary components:\n\nDeployment — responsible for running the ingress controller pods\nService — exposing the ingress controller to the network\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.12.0\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - appProtocol: http\n    name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - appProtocol: https\n    name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n1  type: NodePort\n\n1\n\nThe default network type is NodePort. Change this to LoadBalancer when using MetalLB."
  },
  {
    "objectID": "posts/NginxIngress.html#exposing-the-ingress-controller",
    "href": "posts/NginxIngress.html#exposing-the-ingress-controller",
    "title": "Nginx Ingress",
    "section": "Exposing the Ingress Controller",
    "text": "Exposing the Ingress Controller\nTo expose the ingress controller, define a service of type NodePort or LoadBalancer.\n$ kubectl edit svc ingress-nginx-controller -n ingress-nginx\nTo route traffic to services inside your cluster, create an Ingress resource. This routes https://shaneoh.org traffic to the https://www.shaneoh.org Kubernetes service on port 80.\nFor more details check the blog post on URI based versus Host based routing in Ingress.\n\n\nredirect-root-to-www.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: redirect-root-to-www\n  namespace: default\n  annotations:\n1    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/permanent-redirect: \"http://www.shaneoh.org\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: \"shaneoh.org\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: placeholder-service\n            port:\n              number: 80\n\n\n1\n\nThe routing path handled by the ingress service is omitted and then passed to the services.\n\n\nAfter creating the resource, check if it is running correctly by:\n$ kubectl get ingress\n\nNAME                   CLASS   HOSTS         ADDRESS        PORTS   AGE\nredirect-root-to-www   nginx   shaneoh.org   192.168.0.11   80      5d9h\nStay tuned for future posts covering advanced ingress configurations, including Let’s Encrypt TLS using cert-manager."
  },
  {
    "objectID": "posts/MeasureingDistributions.html",
    "href": "posts/MeasureingDistributions.html",
    "title": "Measuring Distributions",
    "section": "",
    "text": "Entropy plays an important role in measuring probability distributions (or RVs). Besides coding theories such as Huffman and Shannon-Fano, etc., it helps quantify the amount of information using only probability.\n\\[\nH(X) =\nH(p) =\nH(p,p) =\n\\sum_{\\mathbf x \\in \\mathcal X} p_X(\\mathbf x) \\log \\dfrac{1}{p_X(\\mathbf x)}\n\\]\nThus, we can define entropy as a scalar function of an arbitrary probability mass function (PMF). We often write \\(H(X)\\) as \\(H(p)\\), same for the differential entropy: \\(h(X)\\) as \\(h(f)\\)1.\nOne thing to become accustomed to is using \\(\\mathbb E\\) and \\(\\sum_x p(x)\\) (or \\(\\int_\\mathcal{X} f(x)\\), depending on the RVs) interchangeably.\nIn the previous posts of Huffman coding and entropy, we have derived that entropy is the the lower bound of average codeword length (\\(\\bar \\lambda\\)), simultaniously meaning the optimality of codeword length.\nFor the dedicated data distribution, the best encoding strategy to achieve the shortest codeword length is using the codewords with repective to the frequency of its own data distribution. Shorter codewords length for more frequent random variable.\nThe optimality in Huffman coding is expressed as follows. Huffman coding achieves optimality in lossless coding using a greedy algorithm.\n\\[\n\\sum_{i=1}^N p_i \\lambda_i \\leq\n\\sum_{i=1}^N p_i \\lambda_i^{'}\n\\]\nSimilarly, the optimality for entropy can be expressed with Gibbs’ inequality.\n\\[\n\\sum_{i=1}^{N} p_i \\log \\dfrac{1}{p_i} \\leq \\sum_{i=1}^{N} p_i \\log \\dfrac{1}{q_i}\n\\]\nWhat this tells you is that for the given distribution \\(P\\), \\(-\\log P\\) is global optimal."
  },
  {
    "objectID": "posts/MeasureingDistributions.html#proving-gibbs-inequality",
    "href": "posts/MeasureingDistributions.html#proving-gibbs-inequality",
    "title": "Measuring Distributions",
    "section": "Proving Gibbs’ inequality",
    "text": "Proving Gibbs’ inequality\n\\[\\begin{align*}\n&\\sum_{i=1}^{N} p_i \\log \\dfrac{1}{q_i} - \\sum_{i=1}^{N} p_i \\log \\dfrac{1}{p_i} \\\\\n=& \\sum_{i=1}^{N} p_i \\log \\dfrac{p_i}{q_i} \\\\\n=& \\sum_{i=1}^{N} p_i \\left( -\\log \\dfrac{q_i}{p_i} \\right) \\\\\n\\geq& -\\log \\sum_{i=1}^{N} p_i \\dfrac{q_i}{p_i} \\quad \\cdots \\text{Jensen's inequality} \\\\\n=& -\\log \\sum_{i=1}^{N} q_i \\\\\n=& \\, 0\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/MeasureingDistributions.html#kullback-leibler-divergence",
    "href": "posts/MeasureingDistributions.html#kullback-leibler-divergence",
    "title": "Measuring Distributions",
    "section": "Kullback-Leibler divergence",
    "text": "Kullback-Leibler divergence\nWe can reform the upper inequality as below. \\(H(p,q)\\) is called the cross entropy.\n\\[\nD_\\text{KL}(p \\| q) \\triangleq H(p,q) - H(p,p)\n\\]\n\\[\nH(p,q) - H(p,p) = \\sum_{i=1}^{N} p_i \\log \\dfrac{p_i}{q_i} \\geq 0\n\\]\n\\[\nD_\\text{KL}(p \\| q) = 0 \\iff p = q\n\\]\nEntropy of distribution is less than or equal to its cross entropy with any other distribution. The difference between the two quantities is the Kullback-Leibler divergence (relative entropy).\nMurphy (2013) states that the cross entropy is the average number of bits needed to encode data coming from a source with distribution \\(p\\) when we use model \\(q\\) to define our codebook. The Kullback Leibler (KL) divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution \\(q\\) to encode the data instead of the true distribution \\(p\\).\n\nJensen-Shannon divergence\nAs you can see from the definition, it is generally not the case that \\(D_\\text{KL}(p \\| q) = D_\\text{KL}(q \\| p)\\). In some cases, it is useful to define a non-negative scalar between the two distributions that can act as a symmetric distance metric.\n\\[\n\\mathrm{JSD}(p \\| q) =\n\\dfrac{1}{2} D_\\text{KL}\\left(p \\, \\bigg\\| \\, \\dfrac{p+q}{2} \\right) +\n\\dfrac{1}{2} D_\\text{KL}\\left(q \\, \\bigg\\| \\, \\dfrac{p+q}{2} \\right)\n\\]\nThis will satisfy \\(\\mathrm{JSD}(p \\| q) = \\mathrm{JSD}(q \\| p)\\).\nSince \\(D_\\text{KL}\\) is non-negative, \\(\\mathrm{JSD}\\) also inherits this property. This will become useful when deriving the optimality of GANs."
  },
  {
    "objectID": "posts/MeasureingDistributions.html#mutual-information",
    "href": "posts/MeasureingDistributions.html#mutual-information",
    "title": "Measuring Distributions",
    "section": "Mutual information",
    "text": "Mutual information\nConsider two random variables, \\(X\\) and \\(Y\\). Suppose we want to know how much knowing one variable tells us about the other. The more they are correlated, the more different \\(p(X,Y)\\) and \\(p(X)p(Y)\\) will get. Defining the \\(D_\\text{KL}\\) between \\(p(X,Y)\\) and \\(p(X)p(Y)\\) can represent how different \\(p(X,Y)\\) is from independence.\n\\[\nI(X;Y) \\triangleq D_\\text{KL}(p(X,Y) \\| p(X)p(Y))\n= \\sum_{x} \\sum_{y} p(x,y) \\log \\dfrac{p(x,y)}{p(x)p(y)}\n\\]\n\\[\nI(X;Y) = 0 \\iff p(X,Y) = p(X)p(Y) \\iff X \\perp Y\n\\]\n\n\n\n\n\n\n\n\n\n\n\n(a) p(X,Y)\n\n\n\n\n\n\n\n\n\n\n\n(b) p(X)p(Y)\n\n\n\n\n\n\n\nFigure 1: Mutual Information"
  },
  {
    "objectID": "posts/MeasureingDistributions.html#footnotes",
    "href": "posts/MeasureingDistributions.html#footnotes",
    "title": "Measuring Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nECE 587 / STA 563: Lecture 7 - Diﬀerential Entropy↩︎"
  },
  {
    "objectID": "posts/Keycloak.html",
    "href": "posts/Keycloak.html",
    "title": "Introduction to Keycloak",
    "section": "",
    "text": "Keycloak is an open-source Identity and Access Management (IAM) solution aimed at modern applications and services."
  },
  {
    "objectID": "posts/Keycloak.html#features",
    "href": "posts/Keycloak.html#features",
    "title": "Introduction to Keycloak",
    "section": "Features",
    "text": "Features\n\nSingle Sign-On (SSO): Users authenticate with Keycloak rather than individual applications.\nIdentity Brokering and Social Login: Enable login with social networks like Google, Facebook, etc.\nUser Federation: Connect to existing user directories like LDAP or Active Directory.\nAdmin Console: Manage all aspects of Keycloak through a web-based GUI.\nStandard Protocols: Supports OAuth 2.0, OpenID Connect, and SAML."
  },
  {
    "objectID": "posts/Keycloak.html#getting-started",
    "href": "posts/Keycloak.html#getting-started",
    "title": "Introduction to Keycloak",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started with Keycloak on K8s, follow the official guide:\n\nKubectl create\nUse the following command to deploy Keycloak on your Kubernetes cluster:\nkubectl create -f https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/refs/heads/main/kubernetes/keycloak.yaml\nThis creates a deployment and a service in the default namespace. Before exposing the admin console via ingress,\n\n\nChange Default Admin User\nKeycloak requires replacing the pre-configured admin account with a new one. This can be done by accessing the web-based admin console. Access the admin console within your local network (via NodePort, etc.).\nLog in with the username admin and password admin as described in the guide.\n\n\n\nKeycloak login\n\n\nNavigate to Manage &gt; Users &gt; User list and click the Add user button. Add a new admin user with a name you desire. This will create a new account without a password.\n\n\n\n\n\n\nCaution\n\n\n\nBefore deleting the old admin account, create the new one first and set a password. Otherwise, you will no longer be able to configure the account from the web.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are willing to create a new admin account with the same name, Navigate to Configure &gt; Realm settings &gt; Login and toggle the Edit username button.\n\n\nAfter creating a new user, set a password for the new admin account by: Navigating to Credentials tab and clicking the Reset Password button.\nNow you can delete the old admin account.\n\n\nRequire SSL (Optional)\nKeycloak requires SSL connection for external requests by default. Turn it off by navigating to Configure &gt; Realm settings &gt; General and toggle the Require SSL button.\n\n\nFrontend URL\nLastly, just before exposing the admin console with ingress change the Frontend URL value in Configure &gt; Realm settings &gt; General.\n\n\n\nFrontend URL\n\n\n\n\nExpose Admin Console via Ingress\nAfter configuring the upper settings, you are now ready to make the admin console accessable to the world!\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-keycloak\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - auth.shaneoh.org\n    secretName: letsencrypt-tls\n  rules:\n  - host: auth.shaneoh.org\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: keycloak\n            port:\n              number: 8080"
  },
  {
    "objectID": "posts/K8sSANs.html",
    "href": "posts/K8sSANs.html",
    "title": "certSANs",
    "section": "",
    "text": "After installing kubeadm and setting up your cluster, you may need to configure the Subject Alternative Names (SANs) for your certificates. This is particularly important if you are using a custom domain or hostname for your Kubernetes API server."
  },
  {
    "objectID": "posts/K8sSANs.html#prerequisites",
    "href": "posts/K8sSANs.html#prerequisites",
    "title": "certSANs",
    "section": "Prerequisites",
    "text": "Prerequisites\nFrom the control plane node, copy the kubeadm configuration file to your local machine:\n$ scp root@knode-01:/etc/kubernetes/admin.conf ~/.kube/admin.conf\nMerge the admin.conf file to .kube/config Replace the existing IP address with the hostname or domain you want to use:\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: &lt;base64-encoded-ca-cert&gt;\n    server: https://&lt;your-hostname-or-domain&gt;:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\n\ncurrent-context: kubernetes-admin@kubernetes"
  },
  {
    "objectID": "posts/K8sSANs.html#update-kubeadm-configuration",
    "href": "posts/K8sSANs.html#update-kubeadm-configuration",
    "title": "certSANs",
    "section": "Update kubeadm configuration",
    "text": "Update kubeadm configuration\nThe alternative SANs should have been set during the initial kubeadm init command. If you need to update them, you can do so by modifying the kubeadm configuration file.\nThis process involves creating a new key pair and certificate for the API server with the updated SANs. This can be done by deleting the existing certificates and regenerating them with the new SANs. To reduce the risk of this process, you can create a backup file of the existing certificates before proceeding.\n\nBackup existing certificates\n$ cd /etc/kubernetes/pki\n$ mv apiserver.crt apiserver.crt.bak\n$ mv apiserver.key apiserver.key.bak\n\n\nExport the kubeadm configuration\n$ kubectl -n kube-system get configmaps kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' &gt; kubeadm-config.yaml\nEdit the kubeadm-config.yaml file to include the new SANs under the apiServer section. For example:\napiServer:\n  certSANs:\n  - &lt;your-hostname-or-domain&gt;\n\n\nRegenerate the certificates\nkubeadm init phase certs apiserver --config kubeadm-config.yaml"
  },
  {
    "objectID": "posts/InternallyDividingPoint.html",
    "href": "posts/InternallyDividingPoint.html",
    "title": "Internally Dividing Point",
    "section": "",
    "text": "Figure 1: Internally dividing point\n\n\n\nInternally dividing point is defined as follows:\n\\[\n\\begin{align*}\nz &= (y-x) \\cdot \\dfrac{a}{a+b} + x \\\\[10pt]\n&= \\dfrac{ay - ax + ax + bx}{a+b} \\\\[10pt]\n&= \\dfrac{bx + ay}{a+b}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/GoConcurrency.html",
    "href": "posts/GoConcurrency.html",
    "title": "Go Concurrency",
    "section": "",
    "text": "Checkout the video by Ben Davis. Great explanation!\nI recently started learning Go (Golang) and I find it easy to pick up. There isn’t much magic involved, which gives me a solid, reliable feeling when working with it. Go’s approach to concurrency is one of its standout features, built around the philosophy: “Don’t communicate by sharing memory; share memory by communicating.”\nLet’s dive into the concurrency world of Go and explore its key primitives."
  },
  {
    "objectID": "posts/GoConcurrency.html#goroutines-lightweight-threads",
    "href": "posts/GoConcurrency.html#goroutines-lightweight-threads",
    "title": "Go Concurrency",
    "section": "Goroutines: Lightweight Threads",
    "text": "Goroutines: Lightweight Threads\nGoroutines are Go’s lightweight threads managed by the Go runtime. They’re incredibly cheap to create - you can spawn thousands of them without significant overhead.\npackage main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nfunc main() {\n    // Launch a goroutine\n    go sayHello(\"World\")\n    \n    // Main goroutine continues\n    fmt.Println(\"Main function\")\n    \n    // Give the goroutine time to complete\n    time.Sleep(1 * time.Second)\n}\n\nfunc sayHello(name string) {\n    fmt.Printf(\"Hello %s\\n\", name)\n}\n\n\n\n\n\n\nImportant\n\n\n\nThe main function doesn’t wait for goroutines to complete by default. If the main function exits, all goroutines are terminated regardless of their state."
  },
  {
    "objectID": "posts/GoConcurrency.html#wait-groups-coordinating-goroutines",
    "href": "posts/GoConcurrency.html#wait-groups-coordinating-goroutines",
    "title": "Go Concurrency",
    "section": "Wait Groups: Coordinating Goroutines",
    "text": "Wait Groups: Coordinating Goroutines\nA WaitGroup waits for a collection of goroutines to finish. The main goroutine calls Add to set the number of goroutines to wait for. Each goroutine runs and calls Done when finished. We use defer to ensure Done is called even if the function panics.\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nfunc main() {\n    names := []string{\n        \"Alice\", \"Bob\", \"Chuck\", \"Dan\", \"Ed\", \"Fred\", \"Greg\",\n    }\n\n    var wg sync.WaitGroup\n\n    for _, name := range names {\n        wg.Add(1) // Increment the counter\n        go func(name string) {\n            defer wg.Done() // Decrement the counter when done\n            sayHello(name)\n        }(name) // Pass name as parameter to avoid closure issues\n    }\n    \n    wg.Wait() // Block until counter reaches zero\n    fmt.Println(\"All greetings completed!\")\n}\n\nfunc sayHello(name string) {\n    time.Sleep(100 * time.Millisecond) // Simulate work\n    fmt.Printf(\"Hello %v\\n\", name)\n}\n\n\n\n\n\n\nNote\n\n\n\nThink of wg as a counter. The counter increments with values passed to Add and decreases by one with each Done call. Wait blocks until the counter reaches zero."
  },
  {
    "objectID": "posts/GoConcurrency.html#channels-communication-between-goroutines",
    "href": "posts/GoConcurrency.html#channels-communication-between-goroutines",
    "title": "Go Concurrency",
    "section": "Channels: Communication Between Goroutines",
    "text": "Channels: Communication Between Goroutines\nChannels are Go’s way of allowing goroutines to communicate safely. They’re typed conduits that can send and receive values of a specific type.\n\nBasic Channel Operations\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    ch := make(chan int) // Create an unbuffered channel\n\n    go func() {\n        ch &lt;- 1    // Send values\n        ch &lt;- 2\n        ch &lt;- 3\n        close(ch) // Close the channel when done\n    }()\n\n    // Receive values\n    fmt.Println(&lt;-ch) // 1\n    fmt.Println(&lt;-ch) // 2\n    fmt.Println(&lt;-ch) // 3\n    fmt.Println(&lt;-ch) // 0 (zero value from closed channel)\n}\n\n\nRange Over Channels\nA more elegant way to receive from channels is using range:\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    ch := make(chan int)\n\n    go func() {\n        for i := 1; i &lt;= 5; i++ {\n            ch &lt;- i\n        }\n        close(ch) // Important: close the channel\n    }()\n\n    // Range automatically breaks when channel is closed\n    for num := range ch {\n        fmt.Printf(\"Received: %d\\n\", num)\n    }\n}\n\n\nSelect Statement: Non-blocking Operations\nThe select statement lets you wait on multiple channel operations:\npackage main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nfunc main() {\n    ch1 := make(chan string)\n    ch2 := make(chan string)\n\n    go func() {\n        time.Sleep(1 * time.Second)\n        ch1 &lt;- \"Channel 1\"\n    }()\n\n    go func() {\n        time.Sleep(2 * time.Second)\n        ch2 &lt;- \"Channel 2\"\n    }()\n\n    for i := 0; i &lt; 2; i++ {\n        select {\n        case msg1 := &lt;-ch1:\n            fmt.Println(\"Received:\", msg1)\n        case msg2 := &lt;-ch2:\n            fmt.Println(\"Received:\", msg2)\n        case &lt;-time.After(3 * time.Second):\n            fmt.Println(\"Timeout!\")\n        }\n    }\n}\n\n\nPractical Example: Worker Pool\nHere’s a more complex example using channels to implement a worker pool pattern:\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\nfunc main() {\n    jobs := make(chan int, 100)\n    results := make(chan int, 100)\n\n    // Start 3 workers\n    var wg sync.WaitGroup\n    for w := 1; w &lt;= 3; w++ {\n        wg.Add(1)\n        go worker(w, jobs, results, &wg)\n    }\n\n    // Send jobs\n    for j := 1; j &lt;= 9; j++ {\n        jobs &lt;- j\n    }\n    close(jobs)\n\n    // Wait for workers to finish\n    go func() {\n        wg.Wait()\n        close(results)\n    }()\n\n    // Collect results\n    for result := range results {\n        fmt.Printf(\"Result: %d\\n\", result)\n    }\n}\n\nfunc worker(id int, jobs &lt;-chan int, results chan&lt;- int, wg *sync.WaitGroup) {\n    defer wg.Done()\n    for job := range jobs {\n        fmt.Printf(\"Worker %d processing job %d\\n\", id, job)\n        time.Sleep(time.Second) // Simulate work\n        results &lt;- job * 2\n    }\n}"
  },
  {
    "objectID": "posts/GoConcurrency.html#mutexes-protecting-shared-state",
    "href": "posts/GoConcurrency.html#mutexes-protecting-shared-state",
    "title": "Go Concurrency",
    "section": "Mutexes: Protecting Shared State",
    "text": "Mutexes: Protecting Shared State\nWhile channels are preferred for communication, sometimes you need to protect shared state directly. Mutexes (mutual exclusions) provide synchronized access to shared resources.\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\ntype Counter struct {\n    mu    sync.Mutex\n    value int\n}\n\nfunc (c *Counter) Increment() {\n    c.mu.Lock()         // Acquire lock\n    defer c.mu.Unlock() // Release lock when function returns\n    c.value++\n}\n\nfunc (c *Counter) Value() int {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    return c.value\n}\n\nfunc main() {\n    counter := &Counter{}\n    var wg sync.WaitGroup\n\n    // Launch 100 goroutines, each incrementing the counter 100 times\n    for i := 0; i &lt; 100; i++ {\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            for j := 0; j &lt; 100; j++ {\n                counter.Increment()\n            }\n        }()\n    }\n\n    wg.Wait()\n    fmt.Printf(\"Final counter value: %d\\n\", counter.Value()) // Should be 10000\n}\n\nRWMutex: Read-Write Locks\nWhen you have many readers and few writers, sync.RWMutex can provide better performance:\npackage main\n\nimport (\n    \"fmt\"\n    \"sync\"\n    \"time\"\n)\n\ntype SafeMap struct {\n    mu   sync.RWMutex\n    data map[string]int\n}\n\nfunc NewSafeMap() *SafeMap {\n    return &SafeMap{\n        data: make(map[string]int),\n    }\n}\n\nfunc (sm *SafeMap) Set(key string, value int) {\n    sm.mu.Lock()         // Write lock\n    defer sm.mu.Unlock()\n    sm.data[key] = value\n}\n\nfunc (sm *SafeMap) Get(key string) (int, bool) {\n    sm.mu.RLock()         // Read lock\n    defer sm.mu.RUnlock()\n    val, ok := sm.data[key]\n    return val, ok\n}\n\nfunc main() {\n    sm := NewSafeMap()\n    var wg sync.WaitGroup\n\n    // Writers\n    for i := 0; i &lt; 5; i++ {\n        wg.Add(1)\n        go func(i int) {\n            defer wg.Done()\n            key := fmt.Sprintf(\"key%d\", i)\n            sm.Set(key, i*10)\n        }(i)\n    }\n\n    // Readers\n    for i := 0; i &lt; 20; i++ {\n        wg.Add(1)\n        go func(i int) {\n            defer wg.Done()\n            time.Sleep(10 * time.Millisecond) // Give writers time\n            key := fmt.Sprintf(\"key%d\", i%5)\n            if val, ok := sm.Get(key); ok {\n                fmt.Printf(\"Read %s: %d\\n\", key, val)\n            }\n        }(i)\n    }\n\n    wg.Wait()\n}"
  },
  {
    "objectID": "posts/GoConcurrency.html#best-practices-and-common-patterns",
    "href": "posts/GoConcurrency.html#best-practices-and-common-patterns",
    "title": "Go Concurrency",
    "section": "Best Practices and Common Patterns",
    "text": "Best Practices and Common Patterns\n\nChannel Direction\nYou can restrict channels to be send-only or receive-only:\nfunc sender(ch chan&lt;- int) {  // Send-only channel\n    ch &lt;- 42\n}\n\nfunc receiver(ch &lt;-chan int) { // Receive-only channel\n    value := &lt;-ch\n    fmt.Println(value)\n}\n\n\nBuffered Channels\nBuffered channels can hold a limited number of values without blocking:\nch := make(chan int, 3) // Buffer size of 3\nch &lt;- 1 // Doesn't block\nch &lt;- 2 // Doesn't block  \nch &lt;- 3 // Doesn't block\nch &lt;- 4 // Would block without a receiver\n\n\nContext for Cancellation\nUse context for graceful cancellation:\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\nfunc worker(ctx context.Context) {\n    for {\n        select {\n        case &lt;-ctx.Done():\n            fmt.Println(\"Worker cancelled\")\n            return\n        default:\n            fmt.Println(\"Working...\")\n            time.Sleep(500 * time.Millisecond)\n        }\n    }\n}\n\nfunc main() {\n    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\n    defer cancel()\n\n    go worker(ctx)\n    \n    time.Sleep(3 * time.Second) // Wait longer than context timeout\n}"
  },
  {
    "objectID": "posts/GoConcurrency.html#conclusion",
    "href": "posts/GoConcurrency.html#conclusion",
    "title": "Go Concurrency",
    "section": "Conclusion",
    "text": "Conclusion\nGo’s concurrency model provides powerful, yet simple tools for building concurrent applications:\n\nGoroutines for lightweight parallel execution\nChannels for safe communication between goroutines\nWaitGroups for synchronizing goroutine completion\nMutexes for protecting shared state when channels aren’t suitable\n\nThe key is choosing the right tool for each situation. Start with channels and goroutines - they solve most concurrency problems elegantly. Use mutexes sparingly, only when you need to protect shared state that doesn’t fit the channel communication model.\nRemember: “Don’t communicate by sharing memory; share memory by communicating.”"
  },
  {
    "objectID": "posts/Git.html",
    "href": "posts/Git.html",
    "title": "Local Git Commands",
    "section": "",
    "text": "Thanks Cottle for creating this educational tool. Ever thought of Git graphs as linked lists? Here is a great explanation.\nFirst things first,"
  },
  {
    "objectID": "posts/Git.html#git-commit",
    "href": "posts/Git.html#git-commit",
    "title": "Local Git Commands",
    "section": "git commit",
    "text": "git commit\nThis creates a new commit C1, which references where it was based off of — in this case, a C0 which is a initial commit becomes the parent.\ngit commit\n\n\n\n\n\n\n\nflowchart TB\n  main:::pointer -.-&gt; C0:::commit\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-branch",
    "href": "posts/Git.html#git-branch",
    "title": "Local Git Commands",
    "section": "git branch",
    "text": "git branch\nBranches is Git are simply pointers to a specific commit – nothing more. This is why many Git enthusiasts chant the mantra\n\nbranch early, and branch often.\n\nWhen we start mixing branches (pointers) and commits, we will see how these two features combine. For now, just remember that a branch essentially says\n\nI want to include the work of this commit and all parent commits.\n\ngit checkout main\ngit branch dev\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  dev:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\ngit checkout main\ngit checkout -b dev\ngit checkout main\ngit switch -c dev\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  main:::pointer -.-&gt; C1\n  dev:::pointer -.-&gt; C1\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-merge",
    "href": "posts/Git.html#git-merge",
    "title": "Local Git Commands",
    "section": "git merge",
    "text": "git merge\nmerge command eventually creates a special commit which has two unique parents. A commit with two parents essentially means\n\nI want to include all the work from both parents, and the set of all their parents.\n\ngit checkout main\n1git merge dev\ngit checkout dev\n2git merge main\n\n1\n\nThe command merges dev branch into the current main branch. This leaves the dev branch (pointer) behind.\n\n2\n\nWe don’t need to derive the work from both the main and dev parents again because the dev graph is a subgraph of the main graph. Instead, we can simply move the dev pointer to match the position of the main pointer. This is also called fast forward.\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C2:::commit\n  C4:::commit --&gt; C3:::commit\n\n  main:::pointer -.-&gt; C4\n  dev:::pointer -.-&gt; C4\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-rebase",
    "href": "posts/Git.html#git-rebase",
    "title": "Local Git Commands",
    "section": "git rebase",
    "text": "git rebase\nrebase copies the commits and stack them on somewhere else.\n\n\n\n\n\n\nNote\n\n\n\nThe position of the HEAD pointer is different when merging and rebasing.\ngit checkout main # &lt;- HEAD\ngit merge dev\ngit checkout dev # &lt;- HEAD\ngit rebase main\nWhen rebasing, we are willing to rebase with copied commits onto main.\n\n\ngit checkout dev\ngit rebase main\ngit checkout main\ngit merge dev\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; dev\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3':::commit --&gt; C2:::commit\n  C4':::commit --&gt; C3':::commit\n  main:::pointer -.-&gt; C4'\n  dev:::pointer -.-&gt; C4'\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-checkout",
    "href": "posts/Git.html#git-checkout",
    "title": "Local Git Commands",
    "section": "git checkout",
    "text": "git checkout\nDidn’t we use the git checkout command without any discomfort so far? What checkout does is point HEAD to the desired object such as a branch or a commit.\ngit checkout 1b7979e16daafabf7c052411b083ea9e2e8a13d5\n\nRelative reference (^ and ~)\ngit checkout C1\ngit checkout C2^\ngit checkout main^^\ngit checkout main~2\ngit checkout HEAD^; git checkout HEAD^\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C2:::commit\n  main:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C2:::commit\n  main:::pointer -.-&gt; C3\n  HEAD:::pointer -.-&gt; C1\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\nBranch forcing (git branch -f)\nThis is called branch forcing.\ngit branch -f feature dev^\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  C5:::commit --&gt; C4:::commit\n  C6:::commit --&gt; C5:::commit\n  C7:::commit --&gt; C4:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C6\n  feature:::pointer -.-&gt; C7\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000\n\n\n\n\n\n\n\n\n\n\n\nflowchart TB\n  C1:::commit --&gt; C0:::commit\n  C2:::commit --&gt; C1:::commit\n  C3:::commit --&gt; C1:::commit\n  C4:::commit --&gt; C3:::commit\n  C5:::commit --&gt; C4:::commit\n  C6:::commit --&gt; C5:::commit\n  main:::pointer -.-&gt; C2\n  dev:::pointer -.-&gt; C6\n  feature:::pointer -.-&gt; C5\n  HEAD:::pointer -.-&gt; main\n  classDef commit fill: #abc, color: #000\n  classDef pointer fill: #fff, color #000, font: #000"
  },
  {
    "objectID": "posts/Git.html#git-reset",
    "href": "posts/Git.html#git-reset",
    "title": "Local Git Commands",
    "section": "git reset",
    "text": "git reset\nThe reset command is often used to undo changes that have been staged or committed. This sets the HEAD to the desired commit object. The most common command will be as follows.\nUndoing the add command.\ngit reset\ngit reset HEAD\nThis command moves HEAD to the parent of the current commit (HEAD^), effectively undoing the most recent commit.\ngit reset HEAD^\nUndo a commit and make a topic branch out of it.\ngit branch topic/foo\ngit reset --hard HEAD~3\ngit checkout topic/foo"
  },
  {
    "objectID": "posts/Git.html#git-revert",
    "href": "posts/Git.html#git-revert",
    "title": "Local Git Commands",
    "section": "git revert",
    "text": "git revert\nrevert is a command that creates a new commit that undoes the changes made by a previous commit. This means that instead of deleting or altering past commits, git revert adds a new commit on top of the branch.\nWhen HEAD is on the merge commit,\ngit revert HEAD -m 1\ngit revert HEAD -m 2 \nThe number (index) of a parent can be checked with the command git cat-file -p HEAD."
  },
  {
    "objectID": "posts/GAN.html",
    "href": "posts/GAN.html",
    "title": "GAN",
    "section": "",
    "text": "There is an interactive playground available at GAN Lab. Feel free to explore it."
  },
  {
    "objectID": "posts/GAN.html#zero-sum-game-of-a-generator-and-a-discriminator",
    "href": "posts/GAN.html#zero-sum-game-of-a-generator-and-a-discriminator",
    "title": "GAN",
    "section": "Zero-sum game of a Generator and a Discriminator",
    "text": "Zero-sum game of a Generator and a Discriminator\nThe zero-sum property (if one gains, another loses) means that any result of a zero-sum situation is Pareto optimal which is also called a conflict game1.\nThe generator and discriminator engage in a zero-sum game, where the generator tries to produce data that fools the discriminator, while the discriminator aims to correctly identify real versus generated (fake) data. This interaction can be expressed with a payoff matrix.\n\n\n\n\n\n\n\n\\(D_\\text{good}\\)\n\n\n\\(D_\\text{poor}\\)\n\n\n\n\n\\(G_\\text{good}\\)\n\n\n\\(( 0, 0)\\)\n\n\n\\(( 1,-1)\\)\n\n\n\n\n\\(G_\\text{poor}\\)\n\n\n\\((-1, 1)\\)\n\n\n\\(( 0, 0)\\)\n\n\n\n\n\n\nThe payoff matrix shows all the combinations of what players can move. The gains for \\(G\\) and \\(D\\) in each state are denoted by tuples.\nThe best choice for \\(D\\), regardless of what \\(G\\) chooses, is \\(D_\\text{good}\\). When \\(G\\) chooses \\(G_\\text{good}\\), \\(D\\) can move from \\(-1\\) to \\(0\\), gaining \\(+1\\), and when \\(G\\) chooses \\(G_\\text{poor}\\), \\(D\\) can move from \\(0\\) to \\(1\\), also gaining \\(+1\\), making \\(D_\\text{good}\\) a dominant strategy. Same for \\(G\\), making \\(G_\\text{good}\\) a dominant strategy.\nThe solution for this game is choosing \\(D_\\text{good}\\) and \\(G_\\text{good}\\) which is considered as a Nash Equilibrium in GANs, maximizing both players’ gains.\nHowever, the question of whether a Nash Equilibrium exists in the GAN framework remains open. Read Farnia and Ozdaglar (2020) to find out more.\nSince the GAN framework can be modeled as a zero-sum game, we can also derive the same Nash Equilibrium using the second element of the tuples (\\(D\\)’s gain), which provides a more compact representation. This becomes the value function."
  },
  {
    "objectID": "posts/GAN.html#the-value-function",
    "href": "posts/GAN.html#the-value-function",
    "title": "GAN",
    "section": "The Value Function",
    "text": "The Value Function\n\\[\n\\min_G \\max_D V(D,G)\n= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D(G(\\mathbf z)))]\n\\]\nThe generator takes a latent variable \\(\\mathbf z\\) as input and outputs generated data \\(\\mathbf x\\). The discriminator takes data \\(\\mathbf x\\) as input and outputs a probability \\(y\\), representing whether the data is real (\\(1\\)) or fake (\\(0\\)).\n\\[\n\\begin{align*}\nG &:\\mathbf z \\to \\mathbf x \\\\\nD &:\\mathbf x \\to y\n\\end{align*}\n\\]\nThe value function (\\(V\\)) consists of two log-likelihood losses, each from a Bernoulli distribution: one representing the genuine data distribution and the other, the fake data distribution.\nLet \\(\\mathbf{x} \\sim p_\\text{data}(\\mathbf{x})\\) represent a sample drawn from the genuine data distribution. They are all labeled as \\(1\\). Same thing for the fake data \\(\\mathbf{z} \\sim p_\\mathbf{z}(\\mathbf{z})\\). They are all labeled as \\(0\\).\n\n\n\n\n\n\nNote\n\n\n\nDuring training, we will keep the number of samples for \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) the same.\n\n\n\nObjective of \\(D\\)\nThe Binary Cross-Entropy (BCE) is defined as follows.\n\\[\n\\mathcal L_\\text{BCE}(\\hat y, y)\n= - \\lbrace y \\log{\\hat y} + (1-y) \\log{(1-\\hat y)} \\rbrace\n\\]\nWe can try to optimize the model by decreasing the \\(\\mathcal L\\). Conversely, increasing \\(-\\mathcal L\\) resembles the same objective. When dealing with \\(D\\)’s loss, we use the latter approach.\nThe loss of \\(D\\) is calculated with negative BCE loss on both real and fake data distributions and are added together.\n\\[\n\\max_D V(D,G) =\n\\sum_{\\mathbf x}\n-\\mathcal L_\\text{BCE}(D(\\mathbf x), 1) +\n\\sum_{\\mathbf z}\n-\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 0)\n\\]\n\\(D\\) will try to maximize the value function \\(V(D,G)\\) thus, \\(\\theta_\\text{d}\\) is updated with gradient ascent.\n\n\nObjective of \\(G\\)\nIn \\(G\\)’s perspective, \\(G\\)’s objective is to fool \\(D\\) by creating more realistic data.\n\\[\n\\min_G V(G) =\n\\sum_{\\mathbf z}\n-\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 0) \\\\\n\\]\n\\(G\\) will try to minimize the value function \\(V(G)\\) thus, \\(\\theta_\\text{d}\\) is updated with gradient descent.\n\n\n\n\n\n\nUnifying the loss with respect to \\(V\\)\n\n\n\nNote that we are not using \\(\\mathcal L_\\text{BCE}(D(G(\\mathbf z)), 1)\\) as the value function for \\(G\\). In the context of zero-sum loss, we will try to unify the losses in terms of \\(V\\)."
  },
  {
    "objectID": "posts/GAN.html#theoretical-results",
    "href": "posts/GAN.html#theoretical-results",
    "title": "GAN",
    "section": "Theoretical Results",
    "text": "Theoretical Results\n\n\n\n\n\n\nFigure 1: Training GANs\n\n\n\nThe figure above is from Goodfellow et al. (2014). The genuine data distribution \\(p_\\text{data}\\) is represented by a black dotted line. and green denotes the generated distribution \\(p_\\text{g}\\), and lastly the color blue denotes the discriminated distribution,\nAs mentioned above, the output of \\(D\\) is the probability that the data is genuine, so the height of the blue bar represents the corresponding value between \\([0, 1]\\). The value of the blue distribution is one-half on \\(x\\), where \\(x\\) represents the intersection of the black and blue lines.\n\n\n\n\n\n\nNote\n\n\n\nThe results of this section are done in a non-parametric setting, e.g. we represent a model with infinite capacity by studying convergence in the space of probability density functions; \\(p_\\text{data}\\) and \\(p_\\text{g}\\).\n\n\nWe reach the global optimum of \\(V\\) when we approach optimal \\(D\\) and \\(G\\).\n\nOptimal \\(D\\)\nOptimal \\(D\\) can be obtained by maximizing \\(V\\). For any given \\(G\\), \\(D\\) will try it’s best to discriminate genuine data from the fake.\n\\[\\begin{align*}\nV(D,G)\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D(G(\\mathbf z)))] \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x) d \\mathbf x\n+ \\int_\\mathbf{z} p_\\mathbf{z}(\\mathbf z) \\log (1 - D(G(\\mathbf z))) d \\mathbf z \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x) d \\mathbf x\n+ \\int_\\mathbf{x} p_\\text{g}(\\mathbf x) \\log (1 - D(\\mathbf x)) d \\mathbf x \\\\\n\n&= \\int_\\mathbf{x} p_\\text{data}(\\mathbf x) \\log D(\\mathbf x)\n                + p_\\text{g}(\\mathbf x) \\log (1 - D(\\mathbf x)) d \\mathbf x\n\\end{align*}\\]\nBy setting \\(p_\\text{data}(\\mathbf x)\\) as \\(a\\) and \\(p_\\text{g}(\\mathbf x)\\) as \\(b\\), the inner part of the integral can be expressed as the following.\n\\[\\begin{align*}\nf(y) &= a \\log y + b \\log (1-y) \\\\\n\\dfrac{d}{dy} f(y) &= \\dfrac{a}{y} - \\dfrac{b}{1-y} = 0 \\\\\n\\therefore y &= \\frac{a}{a+b}\n\\end{align*}\\]\nThe optimal \\(D\\) can be defined as follows.\n\\[\nD^*(\\mathbf x) = \\dfrac{p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\]\n\n\nOptimal \\(G\\)\nWith same \\(V\\), optimal \\(G\\) can be obtained by minimizing \\(V\\). We will find optimal \\(G\\) with respective to the optimal discriminator \\(D^*\\).\n\\[\\begin{align*}\nV(D^*,G)\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)}[\\log D^*(\\mathbf x)]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)}[\\log (1-D^*(G(\\mathbf z)))] \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{p_\\text{g}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right] \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{2 \\ p_\\text{data}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{2 \\ p_\\text{g}(\\mathbf x)}{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}\n\\right]\n- \\log 4 \\\\\n\n&= \\mathbb E_{\\mathbf x \\sim p_\\text{data}(\\mathbf x)} \\left[\n  \\log \\dfrac{p_\\text{data}(\\mathbf x)}{\\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2}}\n\\right]\n+ \\mathbb E_{\\mathbf z \\sim p_\\mathbf{z}(\\mathbf z)} \\left[\n  \\log \\dfrac{p_\\text{g}(\\mathbf x)}{\\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2}}\n\\right]\n- \\log 4\n\\\\\n\n&= D_\\text{KL} \\left( p_\\text{data}(\\mathbf x) \\bigg\\| \\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2} \\right)\n+ D_\\text{KL} \\left( p_\\text{g}(\\mathbf x) \\bigg\\| \\dfrac{p_\\text{data}(\\mathbf x) + p_\\text{g}(\\mathbf x)}{2} \\right) - \\log 4 \\\\\n\n&= 2 \\ \\mathrm{JSD}(p_\\text{data}(\\mathbf x) \\| p_\\text{g}(\\mathbf x)) - \\log 4 \\\\\n\n&\\geq -\\log 4 \\\\[20pt]\n\n\\min_{G} V(D^*, G) &= -\\log 4 \\iff p_\\text{data} = p_\\text{g} \\\\\n\nV(D^*,G^*) &= -\\log4\n\\end{align*}\\]\n\n\n\n\n\n\nDoes the order of \\(\\min\\), \\(\\max\\) matter?\n\n\n\nYes, it does.\nMathematically, \\(\\min_G \\max_D V(D, G)\\) is the same as \\(\\min_G (\\max_D V(D, G))\\).\nWe try to solve from the inner parentheses.\nIn GAN’s perspective, as long as \\(D\\) tries its best to discriminate the two distributions, the best strategy for \\(G\\) is to mimic \\(p_\\text{data}\\) as closely as possible."
  },
  {
    "objectID": "posts/GAN.html#footnotes",
    "href": "posts/GAN.html#footnotes",
    "title": "GAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia - Zero-sum game↩︎"
  },
  {
    "objectID": "posts/DifferentialEntropy.html",
    "href": "posts/DifferentialEntropy.html",
    "title": "Differential Entropy",
    "section": "",
    "text": "What is entropy? Randomness? Compression lower bound?\nIn this post, let’s think of entropy as a scalar function of the random variable \\(X\\). It is also correct to say it is a scalar function of a distribution.\nLet’s first look at the definition of entropy.\n\\[\nH(X) = \\mathbb E \\left[ \\log \\dfrac{1}{p_X(X)} \\right] = \\sum_{x \\in \\mathcal X} p_X(x) \\log \\dfrac{1}{p_X(x)}\n\\]\nThere’s nothing particularly special here. We discussed this in the previous\npost. We should be able to apply this concept to all types of\ndistributions — not just probability mass functions (PMFs), but also probability density functions (PDFs).\nCheck this lecture note from Duke to find out more.\nDifferential entropy is defined as follows.\n\\[\nh(X) = \\mathbb E \\left[ \\log \\dfrac{1}{f_X(X)} \\right] = \\int_{\\mathcal X} f_X(x) \\log \\dfrac{1}{f_X(x)} dx\n\\]\n\\(h\\) of PDFs are initially derived from \\(H\\) by binning the continuous random variable \\(X\\) into \\(X^\\Delta\\). Let’s see how it works."
  },
  {
    "objectID": "posts/DifferentialEntropy.html#proof",
    "href": "posts/DifferentialEntropy.html#proof",
    "title": "Differential Entropy",
    "section": "Proof",
    "text": "Proof\nFirst, prepare an arbitrary PDF. In this example, we are using \\(\\mathcal N(x;0,1)\\).\n\n\n\n\n\n\n\n\n\nWe set \\(\\Delta\\) to bin the continuous RV \\(X\\).\n\n\n\n\n\n\n\n\n\nBy mean value theorem (MVT), for continuous \\(f\\):\n\\[\n\\exists x_i \\in [i\\Delta, (i+1)\\Delta] :\nf_X(x_i) \\Delta = \\int_{i\\Delta}^{(i+1)\\Delta} f_X(x) dx\n\\]\n\n\n\n\n\n\n\n\n\n\\[\np_{X^\\Delta}(x_i) \\triangleq f_X(x_i) \\Delta\n\\]\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nH(X^\\Delta)\n&= \\sum_i p_{X^\\Delta}(x_i) \\log \\dfrac{1}{p_{X^\\Delta}(x_i)} \\\\\n&= \\sum_i f_X(x_i) \\Delta \\log \\dfrac{1}{f_X(x_i) \\Delta} \\\\\n&= \\sum_i f_X(x_i) \\Delta \\log \\dfrac{1}{f_X(x_i)} + \\sum_i f_X(x_i) \\Delta \\log \\dfrac{1}{\\Delta} \\\\\n&\\approx \\int_{\\mathcal X} f_X(x) \\log \\dfrac{1}{f_X(x)} dx + \\log \\dfrac{1}{\\Delta} \\\\[20pt]\n\nh(X) &= \\lim_{\\Delta \\to 0} \\left( H(X^\\Delta) - \\log \\dfrac{1}{\\Delta} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/CertManager.html",
    "href": "posts/CertManager.html",
    "title": "Cert Manager",
    "section": "",
    "text": "After installing Nginx as the ingress controller, apply the following:\n\n\nclusterissuer.yaml\n\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: user@example.com\n    privateKeySecretRef:\n      name: letsencrypt-key\n    solvers:\n      - http01:\n          ingress:\n            ingressClassName: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kuard\n  annotations:\n1    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - www.example.com\n    secretName: letsencrypt-tls\n  rules:\n  - host: www.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kuard\n            port:\n              number: 80\n\n\n1\n\nIf using a ClusterIssuer, remember to update the Ingress annotation cert-manager.io/issuer to cert-manager.io/cluster-issuer."
  },
  {
    "objectID": "posts/BiasVariance.html",
    "href": "posts/BiasVariance.html",
    "title": "Bias and Variance",
    "section": "",
    "text": "Refer to the lecture note by Kilian Weinberger.\nLet \\(f(x)\\) be the unknown function and \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)\\) be the noise when observing the reality. Then, we can set \\(y = f(x) + \\epsilon\\).\n\n\\(f(\\cdot)\\): unknown funciton (constant)\n\\(\\hat{f}_D(\\cdot)\\): estimated funciton trained with \\(D\\)\n\\(\\bar{f}(\\cdot) = \\mathbb{E}[\\hat{f}_D(\\cdot)]\\): averaged estimated funciton (constant)\n\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)\\): noise (constant)\n\\(y = f(x) + \\epsilon\\): target (constant)\n\n\\[\n\\begin{align*}\n\\mathbb{E}[((y - \\hat{f}_D(x)))^2]\n&= \\mathbb{E}[(f(x) + \\epsilon - \\hat{f}_D(x))^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}_D(x))^2 + 2\\epsilon(f(x) - \\hat{f}_D(x)) + \\epsilon^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}_D(x))^2] + 2\\mathbb{E}[\\epsilon(f(x) - \\hat{f}_D(x))] + \\mathbb{E}[\\epsilon^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}_D(x))^2] + 2\\mathbb{E}[\\epsilon(f(x) - \\hat{f}_D(x))] + \\mathbb{E}[\\epsilon^2] \\\\\n&= \\mathbb{E}[(f(x) - \\hat{f}_D(x))^2] + \\sigma_\\epsilon^2 \\\\\n&= \\mathbb{E}[(f(x) - \\bar{f}(x) + \\bar{f}(x) - \\hat{f}_D(x))^2] + \\sigma_\\epsilon^2 \\\\\n&= \\mathbb{E}[(f(x) - \\bar{f}(x))^2 + 2(f(x) - \\bar{f}(x))(\\bar{f}(x) - \\hat{f}_D(x)) + (\\bar{f}(x) - \\hat{f}_D(x))^2] + \\sigma_\\epsilon^2 \\\\\n&= \\underbrace{\\mathbb{E}[(f(x) - \\bar{f}(x))^2]}_{\\text{bias}^2} + \\underbrace{\\mathbb{E}[(\\bar{f}(x) - \\hat{f}_D(x))^2]}_\\text{variance} + \\underbrace{\\sigma_\\epsilon^2}_{\\text{noise}} \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Made with ♥ by Shane Oh"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n.kube/config\n\n\nwith KubeContext\n\n2 min\n\n\nKubernetes\n\n\n\n\n\n\n\nShane Oh\n\n\nAug 23, 2023\n\n\n\n\n\n\n\n\n\n\n\nBias and Variance\n\n\n\n\n1 min\n\n\nMachine Learning\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\nBinary Heap\n\n\nFor better priority queuing\n\n12 min\n\n\nAlgorithms\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCert Manager\n\n\n\n\n1 min\n\n\nKubernetes\n\nIngress\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCloudflare 526 Error\n\n\nSSL/TLS Encryption Modes\n\n1 min\n\n\nCloudflare\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition of Vaccine Efficacy\n\n\nBehind the scenes of COVID-19\n\n2 min\n\n\nClinical Trials\n\nStatistics\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nDerivatives of Neural Net Layers\n\n\n\n\n4 min\n\n\nMachine Learning\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\nDifferential Entropy\n\n\nEntropy for PDFs\n\n3 min\n\n\nEntropy\n\nInformation Theory\n\n\n\n\n\n\n\nShane Oh\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nEntropy\n\n\nThe lower bound of \\(\\bar \\lambda\\)\n\n4 min\n\n\nInformation Theory\n\nData Compression\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nGAN\n\n\nWhich Comes First? Generator or Discriminator?\n\n7 min\n\n\nML\n\nGenerative Models\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\nGeometric Seuqnece\n\n\nAdding up terms in a sequence with a pattern\n\n1 min\n\n\nMath\n\n\n\n\n\n\n\nShane Oh\n\n\nAug 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nGit Prompt\n\n\nCheck the branch you are working on\n\n1 min\n\n\nGit\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo Concurrency\n\n\nGoroutines, Channels, WaitGroups, and Mutexes\n\n6 min\n\n\nGo\n\nConcurrency\n\nProgramming\n\n\n\nA comprehensive guide to Go’s concurrency primitives with practical examples\n\n\n\nShane Oh\n\n\nSep 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nHuffman Coding\n\n\nLossless data compression\n\n11 min\n\n\nInformation Theory\n\nData Compression\n\nAlgorithms\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nInternally Dividing Point\n\n\n\n\n1 min\n\n\nMath\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Keycloak\n\n\n\n\n3 min\n\n\nKeycloak\n\nKubernetes\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nJensen’s Inequality\n\n\n\n\n1 min\n\n\nMath\n\nProbability Theory\n\n\n\n\n\n\n\nShane Oh\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nKafka 101\n\n\nReal-time event streaming at scale\n\n9 min\n\n\nKafka\n\n\n\n\n\n\n\nShane Oh\n\n\nJun 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nKubernetes Cluster with Raspberry Pi\n\n\nFrom zero to hero!\n\n10 min\n\n\nKubernetes\n\n\n\nInspired by NetworkChuck\n\n\n\nShane Oh\n\n\nOct 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Git Commands\n\n\n\n\n7 min\n\n\nGit\n\n\n\n\n\n\n\nShane Oh\n\n\nApr 14, 2023\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Distributions\n\n\nGibbs’ inequality and more!\n\n4 min\n\n\nEntropy\n\nInformation Theory\n\n\n\nJust like Huffman coding’s optimality, we can discuss entropy’s optimality using Gibbs’ inequality\n\n\n\nShane Oh\n\n\nOct 4, 2024\n\n\n\n\n\n\n\n\n\n\n\nNginx Ingress\n\n\nLoadBalancer and Beyond\n\n4 min\n\n\nKubernetes\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nOpenID Connect\n\n\nDifference Between OAuth2.0 and More!\n\n5 min\n\n\nKeycloak\n\nKubernetes\n\n\n\nBuild your own OICD provider with Keycloak\n\n\n\nShane Oh\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nRook Ceph\n\n\nDistributed Storage Across Nodes\n\n13 min\n\n\nKubernetes\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTmux\n\n\nTerminal multiplexer\n\n5 min\n\n\nLinux\n\n\n\n\n\n\n\nShane Oh\n\n\nJun 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraefik dashboard with Helm\n\n\n\n\n1 min\n\n\nKubernetes\n\nHelm\n\n\n\n\n\n\n\nShane Oh\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\ncertSANs\n\n\nPost configuration for K8s\n\n2 min\n\n\nKubernetes\n\nNetworking\n\n\n\n\n\n\n\nShane Oh\n\n\nOct 27, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/BinaryHeap.html",
    "href": "posts/BinaryHeap.html",
    "title": "Binary Heap",
    "section": "",
    "text": "Check out a video by Abdul Bari."
  },
  {
    "objectID": "posts/BinaryHeap.html#represent-a-binary-tree-in-an-array",
    "href": "posts/BinaryHeap.html#represent-a-binary-tree-in-an-array",
    "title": "Binary Heap",
    "section": "Represent a binary tree in an array",
    "text": "Represent a binary tree in an array\n\n\n\n\n\nflowchart TB\n  A(1: A) --- B(2: B)\n  A --- C(3: C)\n  B --- D(4: D)\n  B --- E(5: E)\n  C --- F(6: F)\n  C --- G(7: G)\n\n\n\n\n\n\n\n  \n  A\n  B\n  C\n  D\n  E\n  F\n  G\n\n  idx\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo represent a binary tree in an array, you can follow these rules:\n\nIf the root of the tree is at index 1 &lt;- easier to remember\nFor a node at index \\(i\\):\n\nThe left child is at index \\(2i\\).\nThe right child is at index \\(2i+1\\).\nThe parent is at index \\(\\lfloor \\frac{i}{2} \\rfloor\\).\n\nIf the root of the tree is at index 0 &lt;- when implementing\nFor a node at index \\(i\\):\n\nThe left child is at index \\(2i+1\\).\nThe right child is at index \\(2i+2\\).\nThe parent is at index \\(\\lfloor \\frac{i - 1}{2} \\rfloor\\).\n\n\n\n\nThe definition of an (almost) complete binary tree becomes clearer when we represent tree structure as an array. We should not allow any null values between the elements.\nNow you may be thinking about the difference between binary trees and heaps. Actually, while all heaps are binary trees, not all binary trees are heaps. You must fulfill the heap property. So what is it?"
  },
  {
    "objectID": "posts/BinaryHeap.html#heap-property",
    "href": "posts/BinaryHeap.html#heap-property",
    "title": "Binary Heap",
    "section": "Heap property",
    "text": "Heap property\n\n  \n    Tree\n    \n      Binary tree\n      \n        Almost complete binary tree\n        \n          Heap\n          \n            Min-heap / Max-heap\n          \n        \n      \n    \n  \n\n\n\nThe heap property dictates the relationship between a parent node and its children in a binary tree. It can be defined in two ways, leading to two different types of heaps:\n\nMin-heap property \\[A[\\lfloor \\frac{i}{2} \\rfloor] \\leq A[i]\\]\nMax-heap property \\[A[\\lfloor \\frac{i}{2} \\rfloor] \\geq A[i]\\]\n\nI find defining the property with the relationship of a current node \\(i\\) and the parent node \\(\\lfloor \\dfrac{i}{2} \\rfloor\\) is more simple (since it can handle the root condition).\nIf you have an array \\(A\\) with no null values between elements, you are already satisfying the almost complete binary tree property and are halfway ready to be a heap structure. How cool is that!\nThe remaining half of the process is called build-heap. We build-heap by heapifying \\(n\\) times.\nFrom now on, we will take a close look at the max-heap since the min-heap and max-heap are basically the same."
  },
  {
    "objectID": "posts/BinaryHeap.html#heapify",
    "href": "posts/BinaryHeap.html#heapify",
    "title": "Binary Heap",
    "section": "Heapify",
    "text": "Heapify\nCheck out the video by Techdose helps!\n\n\n\n\n\nflowchart TB\n  A(  ) -.- 10\n  A(  ) -.- B(  )\n  10 --- 8\n  10 --- 12\n  8 -.- C(  )\n  8 -.- D(  )\n  12 -.- E(  )\n  12 -.- F(  )\n\n\n\n\n\n\nLet’s examine the tree above (with node 10 as root). Assert that the subtrees under node 10 are max-heaps. By recursively sifting down (similar to bubble sorting) on node 10, the entire tree will eventually become a max-heap.\nThe time complexity of heapifying is the same as the height of the heapifying index. Thankfully, the tree is balanced from the very start, which makes it \\(O(\\log n)\\). Obvious, right?"
  },
  {
    "objectID": "posts/BinaryHeap.html#build-heap",
    "href": "posts/BinaryHeap.html#build-heap",
    "title": "Binary Heap",
    "section": "Build heap",
    "text": "Build heap\nThanks again for the video!\nLet’s transform an arbitrary array \\(A\\) into a heap using the build-heap process.\n\\[\\begin{align*}\n  A &\\quad\n  \\begin{bmatrix}\n    2 & 12 & 5 & 15 & 16 & 2 & 6 & 9 & 1 & 4\n  \\end{bmatrix} \\\\\n\n  \\text{Build-Max-Heap}(A) &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\n\\end{align*}\\]\nAt first glance, it might seem intuitive that repeatedly heapifying the array \\(n\\) times, starting from the right (or bottom), will transform \\(A\\) into a heap.\nWould it be surprising to you if I told you that the time complexity of the build-heap algorithm is actually \\(O(n)\\) instead of \\(O(n \\log n)\\)?\n\n\nmaxheapify.go\n\n// MaxHeap constructs a max-heap from an unordered array\n// `i` starts from 0 in this code\n// We are basically bubble sorting from node `i` to the leaf node\n// while iterating `i` from n to 0.\nfunc BuildMaxHeap(arr []int, n int) {\n    // Start from the last non-leaf node and heapify each node\n1    for i := n/2 - 1; i &gt;= 0; i-- {\n        MaxHeapify(arr, n, i)\n    }\n}\n\n// MaxHeapify ensures the subtree rooted at index i is a max-heap\n//      i\n//    /   \\\n// left   right\nfunc MaxHeapify(arr []int, n, i int) {\n    largest := i       // Initialize largest as root\n    left := 2*i + 1    // left child index\n    right := 2*i + 2   // right child index\n\n    // If left child is larger than root\n    if left &lt; n && arr[left] &gt; arr[largest] {\n        largest = left\n    }\n\n    // If right child is larger than the largest so far\n    if right &lt; n && arr[right] &gt; arr[largest] {\n        largest = right\n    }\n\n    // If largest is not root\n    if largest != i {\n        arr[i], arr[largest] = arr[largest], arr[i]  // Swap\n\n        // Recursively heapify the affected subtree\n        MaxHeapify(arr, n, largest)\n    }\n}\n\n\n1\n\nThe for loop can be iterated from n to 0. However, since half of the elements (which are leaf nodes) are already part of a heap, we can start from the node that is not a leaf.\n\n\n\n\n\n\n\n\nLevels and heights of binary trees\n\n\n\nLevel 0                  1                  Height 3\n                        / \\                         \nLevel 1          2               3          Height 2\n                / \\             / \\                 \nLevel 2      4       5       6       7      Height 1\n            / \\     / \\     / \\     / \\             \nLevel 3    8   9  10   11 12   13 14   15   Height 0\nTry to imagine a complete binary tree with a large number of levels. Pick any level you desire in between and set it as \\(l\\).\n\nThe indices of the first elements at each level are \\(2^l\\) and the level of a certain index is \\(\\lfloor \\log_{2} i \\rfloor\\).\nThere are \\(2^{l}-1\\) nodes in the whole tree just before level \\(l\\).\nThere are \\(2^{l}\\) nodes at level \\(l\\).\nThere are \\(2^{l+1}\\) nodes at the next level, \\(l+1\\), which is twice as many.\nAt height \\(h\\), there are a maximum of \\(\\lceil \\frac{N}{2^{h+1}} \\rceil\\) nodes.\n\n\n\nThe nodes at height \\(h\\) needs to be heapified by sifting down \\(h\\) times and there are \\(\\lceil \\frac{N}{2^{h+1}} \\rceil\\) nodes max at each height \\(h\\) which makes,\n\\[\n\\begin{align*}\n\\sum_{h=0}^{\\lfloor \\log_2 N \\rfloor} \\lceil \\dfrac{N}{2^{h+1}} \\rceil O(h)\n&&lt; O \\left( \\sum_{h=0}^{\\infty} \\dfrac{N}{2^{h+1}} C h \\right) \\\\\n&= O \\left( \\dfrac{CN}{2} \\sum_{h=0}^{\\infty} \\dfrac{h}{2^{h}} \\right) \\\\\n&= O \\left( \\dfrac{CN}{2} 2 \\right) \\\\\n&= O(N)\n\\end{align*}\n\\]\nThe talor series is useful when explaining this. Check out the [post] if you are interested.\n\n\n\n\n\n\nWhat is the range of leaf nodes?\n\n\n\nThe parent of the last element can be considered the last node that is not a leaf node.\n\\[\nA[\\lfloor \\dfrac{n}{2} \\rfloor + 1:n]\n\\]\n\n\nCongratulations! You now have a beautiful max-heap ready. Let’s utilize this as a priority queue."
  },
  {
    "objectID": "posts/BinaryHeap.html#inserting-and-popping-elements-in-the-queue",
    "href": "posts/BinaryHeap.html#inserting-and-popping-elements-in-the-queue",
    "title": "Binary Heap",
    "section": "Inserting and popping elements in the queue",
    "text": "Inserting and popping elements in the queue\nInsert from the right, pop from the left.\nInserting from the right helps us to maintain the almost complete binary tree property, but the max-heap property is broken.\n\\[\\begin{align*}\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  A_\\text{broken-max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4 & 100\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 100 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 100 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    100 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\\end{align*}\\]\nThe solution is to check the integrity from the inserted leaf node to the root. Simply compare with the parent node and propagate upward to the top. In the array representation, it seems like hopping to the left for \\(\\log n\\) times.\nPopping is done by removing the root and replacing it with the last leaf node. To preserve the max-heap property, we propagate downward from the root to the bottom.\n\\[\\begin{align*}\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    100 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4 & 12\n  \\end{bmatrix}\\\\\n\n  A_\\text{broken-max-heap} &\\quad\n  \\begin{bmatrix}\n    12 & 16 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  &\\quad\n  \\begin{bmatrix}\n    16 & 12 & 6 & 9 & 15 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n  A_\\text{max-heap} &\\quad\n  \\begin{bmatrix}\n    16 & 15 & 6 & 9 & 12 & 2 & 5 & 2 & 1 & 4\n  \\end{bmatrix}\\\\\n\n\\end{align*}\\]\nNotice that the root node always holds the maximum value of the entire tree. This characteristic enables sorting; you simply keep popping the root until the heap is empty."
  },
  {
    "objectID": "posts/BinaryHeap.html#heap-sort",
    "href": "posts/BinaryHeap.html#heap-sort",
    "title": "Binary Heap",
    "section": "Heap sort",
    "text": "Heap sort\nWhy do we need to create a dedicated paragraph for sorting when it is so straightforward? There is a fun little idea behind heap sorting in an array that makes it more elegant.\n\n\nheapsort.go\n\nfunc HeapSort(arr []int, n int) {\n1    BuildMaxHeap(arr)\n2    for m := n - 1; m &gt;= 0; m-- {\n3        arr[0], arr[m] = arr[m], arr[0]\n        MaxHeapify(arr, m, 0)\n    }\n}\n\n\n1\n\nFirst, create a max-heap.\n\n2\n\nInstead of popping and stacking the elements into a new empty array, we utilize the original array. After popping, there is a spare index because the size of the heap is reduced.\n\n3\n\nBy marking the end of the max-heap with m, we can swap the root value with the leaf at the very end. The popped value will be stacked from the end of the array, and eventually, the array will become a sorted array in ascending order."
  },
  {
    "objectID": "posts/Cloudflare526.html",
    "href": "posts/Cloudflare526.html",
    "title": "Cloudflare 526 Error",
    "section": "",
    "text": "[Browser]--- A ---[Cloudflare]--- B ---[Origin server]\nWhen using Cloudflare as a reverse proxy:\n\n\nThis connection is between the end-user’s browser and Cloudflare’s edge servers. It is typically encrypted via free SSL certificate.\n\n\n\nThis is the connection between Cloudflare and your origin server. It can be either HTTP or HTTPS depending on your server configurations.\nCloudflare supports several encryption modes:\n\nOff (no encryption)\nFlexible\nFull\nFull (strict)\nStrict (SSL-Only Origin Pull)\n\nUse Full mode if using a self-signed certificate. Use Full (strict) mode only if your certificate is issued by a trusted CA.\n\n\n\nCloudflare dashboard"
  },
  {
    "objectID": "posts/Cloudflare526.html#introduction",
    "href": "posts/Cloudflare526.html#introduction",
    "title": "Cloudflare 526 Error",
    "section": "",
    "text": "[Browser]--- A ---[Cloudflare]--- B ---[Origin server]\nWhen using Cloudflare as a reverse proxy:\n\n\nThis connection is between the end-user’s browser and Cloudflare’s edge servers. It is typically encrypted via free SSL certificate.\n\n\n\nThis is the connection between Cloudflare and your origin server. It can be either HTTP or HTTPS depending on your server configurations.\nCloudflare supports several encryption modes:\n\nOff (no encryption)\nFlexible\nFull\nFull (strict)\nStrict (SSL-Only Origin Pull)\n\nUse Full mode if using a self-signed certificate. Use Full (strict) mode only if your certificate is issued by a trusted CA.\n\n\n\nCloudflare dashboard"
  },
  {
    "objectID": "posts/Entropy.html",
    "href": "posts/Entropy.html",
    "title": "Entropy",
    "section": "",
    "text": "I highly recommend taking a look at the Huffman Coding post first. Huffman coding provides an optimal compression solution for a given data distribution, whereas Shannon-Fano coding does not.\nIt may be easier for us to first learn Huffman coding (a bottom-up approach to building the tree) in the algorithms class and then move on to Shannon-Fano coding (a top-down approach).\nTake a look at the video."
  },
  {
    "objectID": "posts/Entropy.html#recap-of-huffman-coding",
    "href": "posts/Entropy.html#recap-of-huffman-coding",
    "title": "Entropy",
    "section": "Recap of Huffman coding",
    "text": "Recap of Huffman coding\nIn the last post, we derived the average codeword length of Huffman coding.\n\\[\n\\bar \\lambda = \\mathbb E [ \\lambda ]\n= \\sum_{i=1}^N p(x_i) \\lambda_i\n= \\sum_{i=1}^N p(x_i) \\left\\lceil \\log_2 \\dfrac{1}{p(x_i)} \\right\\rceil\n\\]\nOne thing we had a hard time deriving was \\(\\lambda\\). We needed to check the frequency (or probability) of each character and aggregate them into a binary tree structure. After that, we traced the path from the root node to each leaf, encoding every character into a binary codeword. Finally, we mapped all of the codewords to the function len to obtain our most desired value, the length of each codeword.\nThe value \\(\\lambda\\) can be interpreted as the number of bits (information) required to losslessly represent an arbitrary group (such as a character or color) within the given data. As you can feel from the word group, we were working on a discrete random variable.\nWe can guarantee the optimality of \\(\\lambda\\) only with the given data distribution. If different data is provided, the frequency (probability) changes, causing the entire Huffman tree to differ from before.\n\n\n\n\n\n\nWhat if we provide a data only with the most frequent character from the existing Huffman tree?\n\n\n\nThis will reduce \\(\\bar \\lambda\\) exceptionally but not with an optimal length. If all characters are the same, the optimal length for the given data will be 0 because the leaf node will also be the root."
  },
  {
    "objectID": "posts/Entropy.html#entropy-as-a-lower-bound-of-bar-lambda",
    "href": "posts/Entropy.html#entropy-as-a-lower-bound-of-bar-lambda",
    "title": "Entropy",
    "section": "Entropy as a lower bound of \\(\\bar \\lambda\\)",
    "text": "Entropy as a lower bound of \\(\\bar \\lambda\\)\nWhat if I say we don’t need all of the cumbersome processes mentioned above? Take a look at the formula below.\n\\[\nH(X) = \\mathbb E[I(X)] = \\sum_{i=1}^N p(x_i) \\log_2 \\dfrac{1}{p(x_i)}\n\\]\nIs entropy just another form of average codeword length?\nThe answer is no. This works as the theoretical lower bound for any data distribution (both discrete and continuous) when compressing. We can not get below this bound if we are performing a lossless compression.\nWe can see that the \\(\\lambda_i\\) has turned into a \\(\\log\\) form with a probability. Since \\(x_i\\) is the only parameter, we can define \\(H\\) as a scalar funciton for any random vaiable \\(X\\). As the probability of \\(x_i\\) increases we can compress the information into a smaller, more compact \\(I(x_i)\\).\nWe can set the theoretical lower bound for \\(\\bar \\lambda\\) as below.\n\\[\nH(X) \\leq \\bar \\lambda\n\\]\nLet’s prove this!"
  },
  {
    "objectID": "posts/Entropy.html#proofing-the-lower-bound-of-bar-lambda",
    "href": "posts/Entropy.html#proofing-the-lower-bound-of-bar-lambda",
    "title": "Entropy",
    "section": "Proofing the lower bound of \\(\\bar \\lambda\\)",
    "text": "Proofing the lower bound of \\(\\bar \\lambda\\)\nWe need to know some basics of Jensen’s inequality and Kraft-McMillan inequality to prove this. Check out the Jensen’s inequality, Kraft-McMillan inequality posts to see more.\n\\[\n\\begin{align*}\nH(X) - \\bar{\\lambda}\n&= \\sum_{i=1}^N \\left( p(x_i) \\dfrac{1}{\\log_2 p(x_i)} - p(x_i) \\lambda_i \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\left( \\dfrac{1}{\\log_2 p(x_i)} - \\lambda_i \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\left( \\dfrac{1}{\\log_2 p(x_i)} \\log_2 2^{-\\lambda_i} \\right) \\\\[10pt]\n&= \\sum_{i=1}^N p(x_i) \\log_2 \\left( \\dfrac{2^{-\\lambda_i}}{p(x_i)} \\right)\n   \\leq \\log_2 \\sum_{i=1}^N p(x_i) \\dfrac{2^{-\\lambda_i}}{p(x_i)} &\\cdots \\text{Jensen's inequality} \\\\[10pt]\n&= \\log_2 \\sum_{i=1}^N 2^{-\\lambda_i} \\leq \\log_2 1 &\\cdots \\text{Kraft-McMillan inequality} \\\\[10pt]\n&= 0\n\\end{align*}\n\\]\nWell, that’s it! For fun, let’s take a look at the graph below.\n\n\n\n\n\n\nFigure 1: Lower bound of lambda bar\n\n\n\nThink \\(\\varphi(x)\\) as \\(-\\log_2 p(x)\\). You can clearly see that entropy (the blue cross) is working as a lower bound.\nWe have not talked about the upper bound of \\(\\bar \\lambda\\).\n\\[\nH(X) \\leq \\bar \\lambda \\lt H(X)+1\n\\]\nThe red and blue crosses are the weighted averages of each function’s outputs. Try ripping the crosses apart from each other. Can you make it greater than 1? Probably not. Infinitely approaching all \\(p(x_i)\\) to values near powers of 2 from the left-hand side is the most probable way, but you won’t be able to get there.\nIf you do get there, it means you are allowing all \\(x_i\\) to have an extra bit, which is not optimal in the first place."
  },
  {
    "objectID": "posts/GeometricSequence.html",
    "href": "posts/GeometricSequence.html",
    "title": "Geometric Seuqnece",
    "section": "",
    "text": "When adding up terms in a sequence with a specific pattern, such as a geometric sequence, using a formula or recognizing a pattern can make the process much easier.\nLet’s see how it works.\n\\[\na, ar, ar^2, ar^3, \\cdots, ar^n\n\\]\nThe sequence above is an arbitrary geometric sequence, where each character denotes the following:\n\n\\(a\\): the first term\n\\(r\\): the common ratio\n\nThe sum from the first term to the n-th term can be calculated using the following formula.\n\\[\n\\begin{align*}\nS_n &= a + ar + ar^2 + \\cdots + ar^{n-1} \\\\\nrS_n &= ar + ar^2 + ar^3 \\cdots + ar^n \\\\\nrS_n - S_n &= ar^n - a \\\\\nS_n &= \\dfrac{a(r^n - 1)}{r-1}\n\\end{align*}\n\\]\nThis formula can be easily seen when counting nodes in a complete binary tree or calculating the moving average of a gradient using the momentum."
  },
  {
    "objectID": "posts/GitPrompt.html",
    "href": "posts/GitPrompt.html",
    "title": "Git Prompt",
    "section": "",
    "text": "Download the git-prompt.sh with the curl command.\ncurl https://raw.githubusercontent.com/git/git/master/contrib/completion/git-prompt.sh -o ~/.git-prompt.sh\nThen add the following lines to the rc files depending on your environment.\n\n\n.zshrc\n\nsource ~/.git-prompt.sh\nsetopt PROMPT_SUBST\nPS1='%n@%m %c%F{green}$(__git_ps1 \" (%s)\")%f \\$ '\n\n\n\n.bashrc\n\nsource ~/.git-prompt.sh\nexport GIT_PS1_SHOWDIRTYSTATE=1\nexport PS1='\\[\\e[0;32m\\]\\u@\\h\\[\\e[0m\\] \\[\\e[0;34m\\]\\W\\[\\e[0m\\]\\[\\e[0;33m\\]$(__git_ps1 \" (%s)\")\\[\\e[0m\\] \\$ '"
  },
  {
    "objectID": "posts/HuffmanCoding.html",
    "href": "posts/HuffmanCoding.html",
    "title": "Huffman Coding",
    "section": "",
    "text": "Thanks to Pizzey Technology for the wonderful video."
  },
  {
    "objectID": "posts/HuffmanCoding.html#concepts",
    "href": "posts/HuffmanCoding.html#concepts",
    "title": "Huffman Coding",
    "section": "Concepts",
    "text": "Concepts\nHuffman coding is a type of variable-length prefix coding that assigns shorter codes to more frequent symbols and longer codes to less frequent symbols. Formal definitions aren’t very useful when dealing with other concepts, in this case, entropy.\nWell, keep this in mind: lossless.\nHuffman coding is one of the lossless compression methods. In terms of compression, you cannot compress data smaller than the limit defined by entropy.\nHuffman is a bottom-up approach to compression which is optimal, while Shannon entropy defines the theoretical limit, a lower bound.\nExamples first."
  },
  {
    "objectID": "posts/HuffmanCoding.html#ascii",
    "href": "posts/HuffmanCoding.html#ascii",
    "title": "Huffman Coding",
    "section": "ASCII",
    "text": "ASCII\nASCII is a character encoding developed in the ’60s by ANSI. It uses 7 bits for character representation. However, in modern computing, characters are stored in bytes (8 bits) for compatibility reasons.\nLet’s take a look at the 8-bit encoded sentence.\n\n\nAN APPLE A DAY KEEPS THE DOCTOR AWAY\n\n01000001 01001110 00100000 01000001 01010000 01010000 01001100 01000101 \n00100000 01000001 00100000 01000100 01000001 01011001 00100000 01001011 \n01000101 01000101 01010000 01010011 00100000 01010100 01001000 01000101 \n00100000 01000100 01001111 01000011 01010100 01001111 01010010 00100000 \n01000001 01010111 01000001 01011001 \n\n288 bits / 36.0 bytes\ncompression rate: 0.0%\n\n\nYou can easily determine the total number of bits in this sentence by simply multiplying the number of characters (including spaces) by the fixed encoding size."
  },
  {
    "objectID": "posts/HuffmanCoding.html#compress-it",
    "href": "posts/HuffmanCoding.html#compress-it",
    "title": "Huffman Coding",
    "section": "Compress it!",
    "text": "Compress it!\nThe more frequently a character appears, the shorter its encoded length becomes.\nThe phrase AN APPLE A DAY KEEPS THE DOCTOR AWAY uses A to rhyme which makes the example more fun.\n\nHuffman tree\nWe build the tree from the bottom using the less frequent characters. Eventually, the less frequent ones are placed at deeper levels. The deeper a character is, the longer the traversal route from the root, which results in a longer encoding length.\nCheck the code below.\n\nfrom collections import Counter\n\nclass Node:\n    def __init__(self, char='', freq=0, left=None, right=None):\n        self.char  = char\n        self.freq  = freq\n        self.left  = left\n        self.right = right\n    \n    @property\n    def is_leaf(self):\n        return self.left is None and self.right is None\n    \n    def __str__(self):\n        if self.is_leaf:\n            return f\"'{self.char}': {self.freq}\"\n        return str(self.freq)\n    \n    def __lt__(self, other):\n        return self.freq &lt; other\n\n    def __gt__(self, other):\n        return self.freq &gt; other\n        \nclass HuffmanTree:\n    def __init__(self, freq_table):\n        self.freq_table = dict(freq_table)\n        self.encoded_table = {}\n        self.root = None\n\n        self.build()\n        self.encode()\n\n    @property\n    def l_bar(self):\n        total = 0\n        for char, freq in self.freq_table.items():\n            total += len(self.encoded_table[char]) * freq\n        return total / sum(self.freq_table.values())\n    \n    def build(self):\n        nodes = [Node(char, freq) for char, freq in self.freq_table.items()]\n        while len(nodes) &gt; 1:\n3            node1 = nodes.pop(nodes.index(min(nodes)))\n            node2 = nodes.pop(nodes.index(min(nodes)))\n            node  = Node(freq=node1.freq+node2.freq,\n                         left=node2,\n                         right=node1)\n            nodes.append(node)\n            self.root = node\n    \n    def encode(self):\n        def dfs(node, path=''):\n1            if node.is_leaf:\n                self.encoded_table[node.char] = path\n                return\n2            dfs(node.left,  path+'0')\n            dfs(node.right, path+'1')\n\n        dfs(self.root)\n\n\n1\n\nCharacters are only at the leaf nodes. This property ensures that each encoded value is not a substring of another and keeps the unique decodability. This is also called as the prefix-free coding.\n\n2\n\nUse 0 for the left and 1 for the right. The path from the root to the leaf node represents the encoded result.\n\n3\n\nAs the function min returns the index of the first occurring minimum value (node.freq) from the list nodes, the built tree is not unique but preserves optimality.\n\n\n\n\n\n\n\n\n\ngraph RL\n140637169108176[36]\n140637169108176 -- 0 --&gt; 140637169114576\n140637169108176 -- 1 --&gt; 140637169110800\n140637169114576[21]\n140637169114576 -- 0 --&gt; 140637169117328\n140637169114576 -- 1 --&gt; 140637169121104\n140637169110800[15]\n140637169110800 -- 0 --&gt; 140637169113232\n140637169110800 -- 1 --&gt; 140637169241808\n140637169117328[13]\n140637169117328 -- 0 --&gt; 140637169088528\n140637169117328 -- 1 --&gt; 140637169084944\n140637169121104[8]\n140637169121104 -- 0 --&gt; 140637169239760\n140637169121104 -- 1 --&gt; 140637169248272\n140637169113232[8]\n140637169113232 -- 0 --&gt; 140637169238864\n140637169113232 -- 1 --&gt; 140637169238096\n140637169241808[7]\n140637169241808 -- 0 --&gt; 140637168970000\n140637169241808 -- 1 --&gt; 140637168968080\n140637169088528[' ': 7]\n140637169084944['A': 6]\n140637169239760[4]\n140637169239760 -- 0 --&gt; 140637169239376\n140637169239760 -- 1 --&gt; 140637169238672\n140637169248272[4]\n140637169248272 -- 0 --&gt; 140637169253008\n140637169248272 -- 1 --&gt; 140637169239632\n140637169238864[4]\n140637169238864 -- 0 --&gt; 140637168960976\n140637169238864 -- 1 --&gt; 140637168966992\n140637169238096[4]\n140637169238096 -- 0 --&gt; 140637168960144\n140637169238096 -- 1 --&gt; 140637168965520\n140637168970000['E': 4]\n140637168968080['P': 3]\n140637169239376[2]\n140637169239376 -- 0 --&gt; 140637169239056\n140637169239376 -- 1 --&gt; 140637169241680\n140637169238672[2]\n140637169238672 -- 0 --&gt; 140637169241360\n140637169238672 -- 1 --&gt; 140637168971536\n140637169253008[2]\n140637169253008 -- 0 --&gt; 140637168967184\n140637169253008 -- 1 --&gt; 140637168972496\n140637169239632[2]\n140637169239632 -- 0 --&gt; 140637168962192\n140637169239632 -- 1 --&gt; 140637169088464\n140637168960976['O': 2]\n140637168966992['T': 2]\n140637168960144['Y': 2]\n140637168965520['D': 2]\n140637169239056['W': 1]\n140637169241680['R': 1]\n140637169241360['C': 1]\n140637168971536['H': 1]\n140637168967184['S': 1]\n140637168972496['K': 1]\n140637168962192['L': 1]\n140637169088464['N': 1]\n\n\n\n\n\n\n\n\nHuffman code\nFinally, the Huffman coded result is below.\n\n\nAN APPLE A DAY KEEPS THE DOCTOR AWAY\n\n001 01111 000 001 111 111 01110 110 000 001 000 1011 001 1010 000 01101 \n110 110 111 01100 000 1001 01011 110 000 1011 1000 01010 1001 1000 01001 \n000 001 01000 001 1010 \n\n132 bits / 16.5 bytes\ncompression rate: 54.2%\n\n\nYou might be familiar with the pangram THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG. It contains all the letters from A to Z, which makes it harder to compress.\n\n\nTHE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\n\n00011 00010 0101 001 11111 00001 11110 11101 11100 001 11011 00000 0100 \n11010 11001 001 11000 0100 10111 001 10110 00001 10101 10100 10011 001 0100 \n10010 0101 00000 001 00011 00010 0101 001 10001 10000 01111 01110 001 01101 \n0100 01100 \n\n192 bits / 24.0 bytes\ncompression rate: 44.2%\n\n\nJust for fun, let’s apply Huffman coding to images.\n\n\n\n\n\n\n\n\n\n\n\n(a) Mondriaan\n\n\n\n\n\n\n\n\n\n\n\n(b) Monet\n\n\n\n\n\n\n\nFigure 1: Image compression\n\n\n\nBoth Figure 1 (a) and Figure 1 (b) are resized to 100x100 greyscale images. Each pixel is represented by the intensity of light from 0 to 255 which corresponds to one byte of information. This results in 10000 bytes for each image.\nAfter applying Huffman coding to each image, each is compressed to 7903.6 bytes and 8354.9 bytes, respectively.\nSo, why do we think Figure 1 (a) is easier to draw? Why is it easier to remember pop music notes than those of bebop jazz?\nIt becomes clear when explained using data compression. Since it has a higher compression rate, it can be described verbally or communicated with a shorter and more compact explanation."
  },
  {
    "objectID": "posts/HuffmanCoding.html#average-codeword-length-per-character-barlambda",
    "href": "posts/HuffmanCoding.html#average-codeword-length-per-character-barlambda",
    "title": "Huffman Coding",
    "section": "Average codeword length per character (\\(\\bar{\\lambda}\\))",
    "text": "Average codeword length per character (\\(\\bar{\\lambda}\\))\nLet’s use some notations from now on.\n\\[\n\\bar \\lambda = \\mathbb E [\\lambda] = \\sum_{i=1}^N p(x_i) \\lambda_i\n\\]\n\n\\(\\lambda_i\\): codeword length for the character \\(x_i\\)\n\\(p(x_i)\\): probability of the character \\(x_i\\) occurring\n\nFor the phrase AN APPLE A DAY KEEPS THE DOCTOR AWAY, \\(\\bar \\lambda\\) is 3.667 with the unit in bits."
  },
  {
    "objectID": "posts/HuffmanCoding.html#sec-kraft-mcmillan-inequality",
    "href": "posts/HuffmanCoding.html#sec-kraft-mcmillan-inequality",
    "title": "Huffman Coding",
    "section": "Kraft-McMillan inequality",
    "text": "Kraft-McMillan inequality\nIn coding theory, the Kraft–McMillan inequality gives a necessary and sufficient condition for the existence of a prefix code1.\nWhen we discuss about the upper bound of \\(\\lambda_i\\), we are assuming the worst case scenario for each \\(x_i\\).\nAssert \\(p_i\\) is smallest among all probabilities. Let’s sort all \\(p\\) in the descending order, just before picking the least occuring two.\n                 1   &lt;- until the end\n                  \\\n                   .\n                    .\n                     \\\n         ... &gt;= p &gt;= p_i''   &lt;- and again\n                        \\\n       ... &gt;= p &gt;= p &gt;= p_i'   &lt;- and again\n                       /   \\\n... &gt;= p &gt;= p &gt;= p &gt;= p &gt;= p_i   &lt;- we are at the very end\n           / \\  / \\\n          .        p\n         .        / \\\nThe parent of each node accumulates the minimum possible value, which follows powers of \\(2\\). This accumulation process stops when we reach the root. We can set each \\(\\lambda_i\\) as the stopping condition, defined by reaching the root.\n\\[\n2^{\\lambda_i} p_i \\geq 1\n\\]\nWe cannot stop the iteration until the cumulative probability reaches \\(1\\). The upper inequality represents a partial form of the Kraft-McMillan inequality.\n\\[\\begin{align*}\n2^{\\lambda_i} p_i &\\geq 1 \\\\\np_i &\\geq 2^{-\\lambda_i} \\\\\n\\sum_{i=1}^{N} p_i &\\geq \\sum_{i=1}^{N} 2^{-\\lambda_i} \\\\\n\\sum_{i=1}^{N} 2^{-\\lambda_i} &\\leq 1 \\quad \\cdots \\text{Kraft-McMillan Inequality} \\\\\n\\end{align*}\\]\nWe can add a condition when reaching the root. Since we are assuming \\(\\lambda_i\\) to be the necessary and sufficient number of steps, we can say that if, after iterating \\(\\lambda_i - 1\\) times, we have not reached the root, the process should continue.\n\\[\n2^{\\lambda_i - 1} p_i &lt; 1\n\\]\nWe can represent this in a figure like below.\n\n\n\n\n\n\nFigure 2: Kraft-McMillan inequality\n\n\n\nIn Figure 2, all \\(\\lambda_i\\) are integers. We are assigning the total width of \\(1\\) by the power of 2 for each codewords.\nBy combining two inequalities of stopping conditions:\n\\[\n\\log_2 \\dfrac{1}{p_i} \\leq \\lambda_i &lt; \\log_2 \\dfrac{1}{p_i} + 1\n\\]\n\\(\\lambda_i\\) is considered as an positive integer. Thus,\n\\[\n\\lambda_i = \\left\\lceil \\log_2 \\dfrac{1}{p_i} \\right\\rceil\n\\]"
  },
  {
    "objectID": "posts/HuffmanCoding.html#footnotes",
    "href": "posts/HuffmanCoding.html#footnotes",
    "title": "Huffman Coding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKraft-McMillan inequality - Wikipedia↩︎"
  },
  {
    "objectID": "posts/JensensInequality.html",
    "href": "posts/JensensInequality.html",
    "title": "Jensen’s Inequality",
    "section": "",
    "text": "Checkout this post to see the background behind this.\nJensen’s inequality generalizes the statement that the secant line of a convex function lies above the graph of the function1.\n\\(f:\\mathbb R \\to \\mathbb R\\) is convex when,\n\\[\n\\forall t \\in [0,1], f(tx_1 + (1-t)x_2) \\leq tf(x_1)+(1-t)f(x_2)\n\\]"
  },
  {
    "objectID": "posts/JensensInequality.html#jensens-inequality-with-probability-theory",
    "href": "posts/JensensInequality.html#jensens-inequality-with-probability-theory",
    "title": "Jensen’s Inequality",
    "section": "Jensen’s inequality with probability theory",
    "text": "Jensen’s inequality with probability theory\n\n\n\n\n\n\nFigure 2: Jensen’s inequality with probability theory\n\n\n\nWhen \\(\\varphi: \\mathbb R \\to \\mathbb R\\) is a convex function,\n\\[\n\\begin{align*}\n\\varphi(\\mathbb E [X]) &\\leq \\mathbb E [\\varphi(X)] \\\\[10pt]\n\\varphi \\left( \\sum p(x_i) \\ x_i \\right) &\\leq \\sum p(x_i) \\varphi(x_i) \\\\\n\\end{align*}\n\\]\n\nFinite form\nWhen \\(\\varphi: \\mathbb R \\to \\mathbb R\\) is a convex function,\n\\[\n\\varphi \\left( \\dfrac{\\sum a_i x_i}{\\sum a_i} \\right) \\leq \\dfrac{\\sum a_i \\varphi(x_i)}{\\sum a_i}\n\\]\nJensen’s inequality also can be applied under weighted average conditions. Weighted average of \\(\\varphi(x_i)\\) can reside in the dashed quardrangle (including the dashed line).\n\n\nExamples\n\\[\n\\mathrm{Var}[X] = \\mathbb E[X^2] - \\mathbb E[X]^2\n\\]\nWhen \\(\\varphi: x \\mapsto x^2\\), \\(\\varphi\\) is convex, which makes \\(\\mathbb E[X]^2 \\leq \\mathbb E[X^2]\\). This is correct because the variance of random variables cannot be negative."
  },
  {
    "objectID": "posts/JensensInequality.html#footnotes",
    "href": "posts/JensensInequality.html#footnotes",
    "title": "Jensen’s Inequality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJensen’s inequality - Wikipedia↩︎"
  },
  {
    "objectID": "posts/Kafka.html",
    "href": "posts/Kafka.html",
    "title": "Kafka 101",
    "section": "",
    "text": "There is a introductory youtube video on Kafka from ByteByteGo. You may also want to check out the tutorial videos from Confluent.\nKafka is a distributed event streaming platform that is used for building real-time data pipelines and streaming applications. It is designed to handle high throughput and low latency, making it suitable for processing large volumes of data in real-time.\nIt was first developed by LinkedIn and later open-sourced in 2011. Kafka is written in Scala and Java, and it is now maintained by the Apache Software Foundation.\nThe founder of the company is Jay Kreps, who was one of the original developers of Kafka at LinkedIn, co-founded Confluent in 2014 to provide a commercial offering around Kafka and to help organizations leverage its capabilities for real-time data processing.\nConfluent offers cloud services for Kafka, including managed Kafka clusters, connectors, and tools for building and managing streaming applications."
  },
  {
    "objectID": "posts/Kafka.html#concepts",
    "href": "posts/Kafka.html#concepts",
    "title": "Kafka 101",
    "section": "Concepts",
    "text": "Concepts\n\nThe big picture of Kafka\n[producers] -&gt; [topics] -&gt; [consumers]\n                  |\n             [partitions] for parallelization\n                  |\n          [leader/followers] for fault tolerance\n                  |\n          [logs with offsets] stored on disk\n\n\nEvents\nAn event records the fact that “something happened” in the world or in your business. It is also called record or message. The key components of a message are as follows.\n\n\nmessage\n\n{\n  key: \"Alice\",\n  value: \"{\\\"event\\\": \\\"login\\\"}\",\n  timestamp: \"2025-06-20T14:00:00Z\"\n  headers: {\n    event-type: \"user-action\",\n    source: \"web\"\n  },\n}\n\n\nkey (optional): Later used for distributing messages into partitions, hash(\"Alice\") % #partitions.\nvalue: The actual message from the event source.\ntimestamp (optional; auto-generated if not provided): The time when the event was produced.\nheaders (optional): Contains metadata such as routing information, etc.\n\n\n\nProducers and Consumers\nProducers are those client applications that publish (write) events to Kafka, and consumers are those that subscribe to (read and process) these events by polling.\nIn modern architectures, it is common to have multiple producers and consumers interacting — producers are likely to be pumping data at a high rate, while consumers may handle data at their own pace. This makes it crucial to decouple the data producers from the consumers.\n\n\n\n\n\n\nKafka vs message queues\n\n\n\nThe key difference is that Kafka is a log-based system, whereas traditional message queues are queue-based, where consumed events are typically ephemeral.\nKafka takes a more durable and fault-tolerant approach compared to other message queues by using a file-based log system. Logs in Kafka are append-only, ordered, and durable, which makes them replayable.\nSince logs do accumulate data over time, we can configure a retention period to control how long the data is kept.\n\n\n\n\nTopics and Partitions\nA Kafka cluster can have multiple brokers and controllers. Controller is elected among quorum using Kraft which was previously Zookeeper.\n┌─Kafka cluster───────────────────────────────┐\n│┌─node.id=1 process.roles=broker,controller─┐│ ╮\n││            topic=A partition=1            ││ │ \n││            topic=A partition=2            ││ │\n││            topic=B partition=1            ││ │\n│└───────────────────────────────────────────┘│ │ quorum\n│┌─node.id=2 process.roles=broker,controller─┐│ │\n││            topic=B partition=2            ││ │\n││            topic=B partition=3            ││ │\n││            topic=C partition=1            ││ │\n│└───────────────────────────────────────────┘│ ╯\n└─────────────────────────────────────────────┘\n\nEach topic has one or more partition for parallelism.\nEach partition can be replicated.\nAmong replicated partitions, a leader is elected from the in-sync replicas (ISR).\n\n                         #offset             ╭─ High water mark\n             partition=X [0][2][3][4][5][6][7]       In sync follower ╮\nProd &lt;-ack-- partition=X [0][2][3][4][5][6][7][8][9] Leader           │ In sync replicas (ISR)\n             partition=X [0][2][3][4][5][6][7][8]    In sync follower ╯\n             partition=X [0][2][3][4][5]             Out of sync follower\n                                       ╰────Lag────╯\nWhen a producer sends a message to Kafka, it receives an ack only after the message is successfully written to the leader.\n\n\n\n\n\n\n\n\nacks\nDescription\nDurability\n\n\n\n\nacks=0\nProducer does not wait for any broker ack\nLow (high risk of message loss)\n\n\nacks=1\nOnly the leader broker acknowledges receipt to the producer\nMedium\n\n\nacks=all or -1\nLeader and all in-sync replicas acknowledge receipt\nHigh"
  },
  {
    "objectID": "posts/Kafka.html#hands-on",
    "href": "posts/Kafka.html#hands-on",
    "title": "Kafka 101",
    "section": "Hands on",
    "text": "Hands on\n\nSingle-node\nA tutorial from the blog is good place to start.\n$ docker run -d --name kafka apache/kafka:4.0.0\n$ docker exec -it kafka /bin/sh\nIn the docker container\n\n/opt/kafka/config: config files for Kafka\n\nserver.properties\n\n/opt/kafka/bin: bins files directory\n\nkafka-server-start.sh server.properties\nkafka-server-stop.sh\nkafka-topics.sh --list --bootstrap-server localhost:9092\nkafka-topics.sh --create --topic X --bootstrap-server localhost:9092\nkafka-topics.sh --delte --topic X --bootstrap-server localhost:9092\n\n/tmp/kraft-combined-logs: log files from Kafka\n\n\n\nMulti-node (with Kraft)\nCheck the docs from docker hub page for more.\n  ┌─────Kafka cluster─────┐\n  │      ┌controller-1~3┐ │\n  │  9093:9093          │ │\n  │      └──────────────┘ │\n  │      ┌───broker-1───┐ │\n  │ 19092:19092         │ │\n ───29092:9092          │ │\n  │      └──────────────┘ │\n  │      ┌───broker-2───┐ │\n  │ 19092:19092         │ │\n ───39092:9092          │ │\n  │      └──────────────┘ │\n  │      ┌───broker-3───┐ │\n  │ 19092:19092         │ │\n ───49092:9092          │ │\n  │      └──────────────┘ │\n  └───────────────────────┘\n\n\ndocker-compose.yaml\n\nservices:\n  controller-1:\n    image: apache/kafka:latest\n    container_name: controller-1\n    environment:\n      KAFKA_NODE_ID: 1\n      KAFKA_PROCESS_ROLES: controller\n      KAFKA_LISTENERS: CONTROLLER://:9093\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n\n  controller-2:\n    image: apache/kafka:latest\n    container_name: controller-2\n    environment:\n      KAFKA_NODE_ID: 2\n      KAFKA_PROCESS_ROLES: controller\n      KAFKA_LISTENERS: CONTROLLER://:9093\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n\n  controller-3:\n    image: apache/kafka:latest\n    container_name: controller-3\n    environment:\n      KAFKA_NODE_ID: 3\n      KAFKA_PROCESS_ROLES: controller\n      KAFKA_LISTENERS: CONTROLLER://:9093\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n\n  broker-1:\n    image: apache/kafka:latest\n    container_name: broker-1\n    ports:\n      - 29092:9092\n    environment:\n      KAFKA_NODE_ID: 4\n      KAFKA_PROCESS_ROLES: broker\n      KAFKA_LISTENERS: 'PLAINTEXT://:19092,PLAINTEXT_HOST://:9092'\n      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-1:19092,PLAINTEXT_HOST://localhost:29092'\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n    depends_on:\n      - controller-1\n      - controller-2\n      - controller-3\n\n  broker-2:\n    image: apache/kafka:latest\n    container_name: broker-2\n    ports:\n      - 39092:9092\n    environment:\n      KAFKA_NODE_ID: 5\n      KAFKA_PROCESS_ROLES: broker\n      KAFKA_LISTENERS: 'PLAINTEXT://:19092,PLAINTEXT_HOST://:9092'\n      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-2:19092,PLAINTEXT_HOST://localhost:39092'\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n    depends_on:\n      - controller-1\n      - controller-2\n      - controller-3\n\n  broker-3:\n    image: apache/kafka:latest\n    container_name: broker-3\n    ports:\n      - 49092:9092\n    environment:\n      KAFKA_NODE_ID: 6\n      KAFKA_PROCESS_ROLES: broker\n      KAFKA_LISTENERS: 'PLAINTEXT://:19092,PLAINTEXT_HOST://:9092'\n      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker-3:19092,PLAINTEXT_HOST://localhost:49092'\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@controller-1:9093,2@controller-2:9093,3@controller-3:9093\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n    depends_on:\n      - controller-1\n      - controller-2\n      - controller-3"
  },
  {
    "objectID": "posts/Kubeconfig.html",
    "href": "posts/Kubeconfig.html",
    "title": ".kube/config",
    "section": "",
    "text": "I have set up Minikube on my MacBook for an easy development environment, and at home, I have a Raspberry Pi cluster set up for my homelab. These configurations allow me to access and manage my clusters from anywhere. Specifically, I have properly configured the .kube/config file to enable remote access to the cluster at home.\nBelow is an example of the configured .kube/config file:\n\n\n.kube/config\n\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0...LS0tCg==\n    server: https://kubernetes.docker.internal:6443\n  name: docker-desktop\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://121.135.111.111:6443\n  name: raspberrypi\n- cluster:\n    certificate-authority: /Users/shane/.minikube/ca.crt\n    extensions:\n    - extension:\n        last-update: Sun, 20 Aug 2023 17:04:00 KST\n        version: v1.30.1\n        provider: minikube.sigs.k8s.io\n      name: cluster_info\n    server: https://127.0.0.1:60544\n  name: minikube\n\nThe above configuration file defines three clusters:\n\ndocker-desktop\nraspberrypi (the Raspberry Pi cluster at home, with the server address https://121.135.111.111:6443)\nminikube\n\nWith this configuration file, you can easily access various clusters using the kubectl command. For example, to access the Raspberry Pi cluster, you can use the following command:\nkubectl config use-context raspberrypi\nTo bypass the process of verifying the SSL certificate as a public certificate, use the following command:\n- cluster:\n    insecure-skip-tls-verify: true"
  },
  {
    "objectID": "posts/NNDerivatives.html",
    "href": "posts/NNDerivatives.html",
    "title": "Derivatives of Neural Net Layers",
    "section": "",
    "text": "Note\n\n\n\nNumerator layout is used for the equations in this post."
  },
  {
    "objectID": "posts/NNDerivatives.html#preliminaries",
    "href": "posts/NNDerivatives.html#preliminaries",
    "title": "Derivatives of Neural Net Layers",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nChain rule\n\\[\n\\begin{align*}\n\\dfrac{d}{dx} h(g(f(x)))\n&= \\underbrace{h'(\\overbrace{g(f(x))}^{\\text{forward}})}_{\\text{backward}}\n\\quad \\underbrace{g'(\\overbrace{f(x)}^{\\text{forward}})}_{\\text{backward}}\n\\quad \\underbrace{f'(x)}_{\\text{backward}} \\\\\n\\dfrac{\\partial}{\\partial \\mathbf x} h(\\mathbf g(\\mathbf f(\\mathbf x)))\n&= \\dfrac{\\partial h}{\\partial \\mathbf g} \\dfrac{\\partial \\mathbf g}{\\partial \\mathbf f} \\dfrac{\\partial \\mathbf f}{\\partial \\mathbf x}\n= \\nabla h(\\mathbf g( \\mathbf f(\\mathbf x))) \\ \\mathbf J_\\mathbf g(\\mathbf f(\\mathbf x)) \\ \\mathbf J_\\mathbf f(\\mathbf x)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/NNDerivatives.html#derivatives",
    "href": "posts/NNDerivatives.html#derivatives",
    "title": "Derivatives of Neural Net Layers",
    "section": "Derivatives",
    "text": "Derivatives\n\n\\(\\mathcal{L}_\\mathrm{BCE}\\)\n\\[\n\\begin{align*}\nH(y, \\hat{y})\n&= y \\log\\dfrac{1}{\\hat{y}} + (1-y) \\log\\dfrac{1}{1-\\hat{y}} \\\\\n&= -\\left\\{ y \\log{\\hat{y}} + (1-y) \\log(1-\\hat{y}) \\right\\} \\\\\n\\dfrac{d H(y, \\hat{y})}{d\\hat{y}}\n&= -\\dfrac{y}{\\hat y} + \\dfrac{1-y}{1-\\hat y} = \\dfrac{\\hat y - y}{\\hat y (1-\\hat y)}\n\\end{align*}\n\\]\n\n\n\\(\\mathcal{L}_\\mathrm{CE}\\)\n\\[\n\\begin{align*}\nH(\\mathbf y, \\hat{\\mathbf y})\n&= \\sum_i y_i \\log \\dfrac{1}{\\hat{y}_i} = -\\sum_i y_i \\log \\hat{y}_i \\\\\n\\dfrac{\\partial H(\\mathbf y, \\hat{\\mathbf y})}{\\partial \\hat{\\mathbf y}}\n&=\n- \\begin{pmatrix}\n\\dfrac{y_1}{\\hat{y}_1}\n& \\cdots &\n\\dfrac{y_n}{\\hat{y}_n}\n\\end{pmatrix}\n\\end{align*}\n\\]\n\n\nSigmoid\nLet \\(\\sigma : \\mathbb R \\to \\mathbb R\\).\n\\[\n\\begin{align*}\n\\dfrac{d}{dx} \\sigma(x)\n&= \\dfrac{d}{dx} (1 + e^{-x})^{-1} \\\\\n&= e^{-x}(1 + e^{-x})^{-2}\n= \\dfrac{1}{1 + e^{-x}} \\dfrac{e^{-x}}{1 + e^{-x}} \\\\\n&= \\sigma(x) (1-\\sigma(x))\n\\end{align*}\n\\]\n\n\nSoftmax\nLet \\(\\mathbf{s} : \\mathbb R^n \\to \\mathbb R^n\\).\n\n\\(\\mathbf x = (x_1 \\cdots x_n)^\\top\\)\n\\(\\mathbf{s} = (s_1 \\cdots s_n)^\\top\\)\n\\(s_i(\\mathbf x) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\quad (s_i:\\mathbb R^n \\to \\mathbb R)\\)\n\n\\[\n\\begin{align*}\n\\mathbf J_{\\mathbf{s}}\n&=\n\\begin{bmatrix}\n\\dfrac{\\partial s_1(\\mathbf x)}{\\partial x_1} & \\cdots & \\dfrac{\\partial s_1(\\mathbf x)}{\\partial x_n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{\\partial s_n(\\mathbf x)}{\\partial x_1} & \\cdots & \\dfrac{\\partial s_n(\\mathbf x)}{\\partial x_n}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\dfrac{e^{x_1} \\sum_j e^{x_j} - e^{x_1} e^{x_1}}{(\\sum_j e^{x_j})^2}\n& \\cdots &\n\\dfrac{0 \\sum_j e^{x_j} - e^{x_1} e^{x_n}}{(\\sum_j e^{x_j})^2} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\dfrac{0 \\sum_j e^{x_j} - e^{x_n} e^{x_1}}{(\\sum_j e^{x_j})^2}\n& \\cdots &\n\\dfrac{e^{x_n} \\sum_j e^{x_j} - e^{x_n} e^{x_n}}{(\\sum_j e^{x_j})^2}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\dfrac{e^{x_1}}{\\sum_j e^{x_j}}\n\\dfrac{\\sum_j e^{x_j} - e^{x_1}}{\\sum_j e^{x_j}}\n& \\cdots &\n- \\dfrac{e^{x_1}}{\\sum_j e^{x_j}} \\dfrac{e^{x_n}}{\\sum_j e^{x_j}} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n- \\dfrac{e^{x_n}}{\\sum_j e^{x_j}} \\dfrac{e^{x_i}}{\\sum_j e^{x_j}}\n& \\cdots &\n\\dfrac{e^{x_n}}{\\sum_j e^{x_j}}\n\\dfrac{\\sum_j e^{x_j} - e^{x_n}}{\\sum_j e^{x_j}}\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\ns_1(\\mathbf x)(1-s_1(\\mathbf x)) & \\cdots & -s_1(\\mathbf x) s_n(\\mathbf x) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n-s_n(\\mathbf x) s_1(\\mathbf x) & \\cdots & s_n(\\mathbf x)(1-s_n(\\mathbf x)) \\\\\n\\end{bmatrix} \\\\\n&=\n\\mathrm{diag}(\\mathbf{s}(\\mathbf x)) -\\mathbf{s}(\\mathbf x) \\mathbf{s}(\\mathbf x)^\\top\n\\end{align*}\n\\]\n\n\nSigmoid -&gt; \\(\\mathcal L_\\mathrm{BCE}\\)\n\\[\n\\begin{align*}\n\\dfrac{d \\mathcal L_\\mathrm{BCE}}{d x}\n&=\n\\dfrac{d \\mathcal L_\\mathrm{BCE}}{d \\hat{y}}\n\\dfrac{d \\hat{y}}{dx} \\\\\n&=\n\\dfrac{\\hat y - y}{\\hat y (1-\\hat y)} {\\hat y (1-\\hat y)} \\\\\n&=\n\\hat y - y\n\\end{align*}\n\\]\n\n\nSoftmax -&gt; \\(\\mathcal L_\\mathrm{CE}\\)\n\\[\n\\begin{align*}\n\\dfrac{\\partial \\mathcal L_\\mathrm{CE}}{\\partial \\mathbf x}\n&=\n\\dfrac{\\partial \\mathcal L_\\mathrm{CE}}{\\partial \\hat{\\mathbf y}}\n\\dfrac{\\partial \\hat{\\mathbf y}}{\\partial \\mathbf x} \\\\\n&=\n- \\left( \\dfrac{\\mathbf y}{\\hat{\\mathbf y}} \\right)^\\top\n\\left( \\mathrm{diag}(\\hat{\\mathbf y}) - \\hat{\\mathbf y} \\hat{\\mathbf y}^\\top \\right) \\\\\n&=\n\\begin{pmatrix}\n-y_1 + \\sum_i y_i \\hat{y}_1\n& \\cdots &\n-y_n + \\sum_i y_i \\hat{y}_n\n\\end{pmatrix}\n\\end{align*}\n\\]\n\n\nLinear\n\\[\n\\begin{bmatrix}\n\\\\\n&A& \\vert \\mathbf b \\\\\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\\\\n\\mathbf x \\\\\n\\\\\n\\hline\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\\\\n\\mathbf z \\\\\n\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{align*}\n\\dfrac{\\partial z_i}{\\partial [A]_i^\\top} &= \\dfrac{\\partial [A]_i\\mathbf x + b_i}{\\partial [A]_i^\\top} = \\mathbf x^\\top \\\\\n\\dfrac{\\partial z_i}{\\partial b_j} &= \\dfrac{\\partial [A]_i\\mathbf x + b_i}{\\partial b_j} = \\delta_{ij} \\\\\n\\dfrac{\\partial \\mathbf z}{\\partial \\mathbf b} &= I\n\\end{align*}\n\\]\n\n\nReLU\n\nimport numpy as np\n\nclass ReLU:\n    def __init__(self):\n        self.grad = None\n\n    def derivative(self, x):\n        \"\"\"∂ReLU(x)/∂x\"\"\"\n1        return np.where(x&gt;0, 1.0, 0.0)\n      \n    def __call__(self, x):\n        self.grad = self.derivative(x)\n        return np.maximum(0, x)\n  \nrelu = ReLU()\nx = np.random.randn(3,3)\n\nprint(x)\nprint(relu(x))\nprint(relu.grad)\n\n\n1\n\nMathmatically, np.diag(np.ravel(np.where(x&gt;0,1,0))) is correct.\n\n\n\n\n[[ 2.84240767  0.23374714 -0.19969603]\n [-0.6255382   0.43207221  1.46811099]\n [-2.34788582 -0.19502473  0.53880593]]\n[[2.84240767 0.23374714 0.        ]\n [0.         0.43207221 1.46811099]\n [0.         0.         0.53880593]]\n[[1. 1. 0.]\n [0. 1. 1.]\n [0. 0. 1.]]\n\n\n\n\nLinear -&gt; ReLU\n\\[\n\\begin{bmatrix}\n\\\\\n&A& \\vert \\mathbf b \\\\\n\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\\\\n\\mathbf x \\\\\n\\\\\n\\hline\n1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\\\\n\\mathbf y \\\\\n\\\\\n\\end{bmatrix},\n\\quad \\mathrm{ReLU}(\\mathbf y) = \\mathbf z\n\\]\n\\[\n\\dfrac{\\partial \\mathcal L}{\\partial \\begin{bmatrix}&[A]_i&|b_i\\end{bmatrix}^\\top}\n=\n\\begin{bmatrix}\n&\\mathbf x^\\top& \\vert 1 \\\\\n\\end{bmatrix}\n\\mathbb{1}_{y_i &gt; 0}(y_i)\n\\frac{\\partial \\mathcal L}{\\partial z_i}\n\\]\n\n\nConvolution\n\n\\(\\mathbf x_{4 \\times 4}\\): input\n\\(\\kappa_{3 \\times 3}\\): kernel\n\\(\\mathbf z_{2 \\times 2}\\): output\n\n\\[\n\\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} \\\\\nx_{41} & x_{42} & x_{43} & x_{44}\n\\end{bmatrix} \\star\n\\begin{bmatrix}\n\\kappa_{11} & \\kappa_{12} & \\kappa_{13} \\\\\n\\kappa_{21} & \\kappa_{22} & \\kappa_{23} \\\\\n\\kappa_{31} & \\kappa_{32} & \\kappa_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\nz_{11} & z_{12} \\\\\nz_{21} & z_{22}\n\\end{bmatrix}\n\\]\n\\[\n\\dfrac{\\partial \\mathbf z}{\\partial \\kappa}\n\\]\n\\[\n\\dfrac{\\partial \\mathbf z}{\\partial \\kappa} =\n\\begin{matrix}\n       & \\kappa_{11} & \\kappa_{12} & \\kappa_{13} & \\kappa_{21} & \\kappa_{22} & \\kappa_{23} & \\kappa_{31} & \\kappa_{32} & \\kappa_{33} \\\\\nz_{11} & x_{11} & x_{12} & x_{13} & x_{21} & x_{22} & x_{23} & x_{31} & x_{32} & x_{33} \\\\\nz_{12} & x_{12} & x_{13} & x_{14} & x_{22} & x_{23} & x_{24} & x_{32} & x_{33} & x_{34} \\\\\nz_{21} & x_{21} & x_{22} & x_{23} & x_{31} & x_{32} & x_{33} & x_{41} & x_{42} & x_{43} \\\\\nz_{22} & x_{22} & x_{23} & x_{24} & x_{32} & x_{33} & x_{34} & x_{42} & x_{43} & x_{44}\n\\end{matrix}\n\\]\n\\[\n\\dfrac{\\partial \\mathcal L}{\\partial \\kappa} =\n\\dfrac{\\partial \\mathcal L}{\\partial \\mathbf z}\n\\dfrac{\\partial \\mathbf z}{\\partial \\kappa}\n\\]"
  },
  {
    "objectID": "posts/OIDC.html",
    "href": "posts/OIDC.html",
    "title": "OpenID Connect",
    "section": "",
    "text": "This document is based on the OpenID Connect Core specification. Additionally, the video by Okta elucidates the core idea of the whole concept with a simple diagram. Great explaination Nate!\nBefore we begin, let’s talk about auth first."
  },
  {
    "objectID": "posts/OIDC.html#authentication-and-authorization",
    "href": "posts/OIDC.html#authentication-and-authorization",
    "title": "OpenID Connect",
    "section": "Authentication and Authorization",
    "text": "Authentication and Authorization\nAuthentication and Authorization are two fundamental concepts in the world of security.\n\nAuthentication (AuthN)\nAuthentication is the process of verifying the identity of a user or system. It answers the question: Who are you?\nCommon methods of authentication include:\n\nUsername and password\nMulti-factor authentication (MFA)\n\n\n\nAuthorization (AuthZ)\nAuthorization, on the other hand, determines what an authenticated user or system is allowed to do. It answers the question: **What are you allowed to do?\nAuthorization typically involves:\n\nRole-based access control (RBAC)\nAttribute-based access control (ABAC)\n\nUnderstanding the difference between AuthN and AuthZ is crucial for implementing secure systems. While authentication ensures that the user is who they claim to be, authorization ensures that the user has permission to access the requested resources."
  },
  {
    "objectID": "posts/OIDC.html#overview",
    "href": "posts/OIDC.html#overview",
    "title": "OpenID Connect",
    "section": "Overview",
    "text": "Overview\n\nOpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It enables Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.\n\nSection 1.3. explains the steps that the OpenID Connect (OIDC) protocol follows from a bird’s-eye view.\n\n\n\nGeneral steps of OpenID Connect\n\n\nThere are three actors in this diagram.\n\nEnd-User: The person who clicks, types, user credentials to get logged in.\nRP (Relying Party): The application that wants to verify the identity of the End-User and obtain information about the End-User.\nOP (OpenID Provider): The server that authenticates the End-User and provides the identity information to the RP."
  },
  {
    "objectID": "posts/OIDC.html#prerequisites",
    "href": "posts/OIDC.html#prerequisites",
    "title": "OpenID Connect",
    "section": "Prerequisites",
    "text": "Prerequisites\nLet’s take an example with Keycloak to understand the steps involved in the OpenID Connect flow. In this example, Keycloak is acting as an OP.\nIn Keycloak, a realm is a space where you manage objects such as users, applications, and roles. Each realm is isolated from other realms and can have its own configuration and settings.\nGo to the dropdown menu in the upper left menu and select the realm you want to configure. Then, follow these steps:\n\nCreate realm: This requires a name which will be used as a REALM_NAME later on.\nManage &gt; Clients &gt; Clients list &gt; Create client: This creates an RP with a distinctive name CLIENT_ID.\n\n\n\n\nKeycloak Clients"
  },
  {
    "objectID": "posts/OIDC.html#flow",
    "href": "posts/OIDC.html#flow",
    "title": "OpenID Connect",
    "section": "Flow",
    "text": "Flow\n\nAuthentication Request\nThe first request is made by the client application when the Login with Keycloak button is clicked. This redirects the the browser’s page to the Keycloak’s auth page.\n&lt;button\n  onClick={() =&gt; (\n    window.location.href = `https://${HOST}/auth/realms/${REALM_NAME}/protocol/openid-connect/auth`\n    + `?client_id=${CLIENT_ID}`\n1    + `&redirect_uri=${REDIRECT_URI}`\n2    + `&response_type=${RESPONSE_TYPE}`\n3    + `&scope=${SCOPE}`\n  )}\n&gt;\n  Login with Keycloak\n&lt;/button&gt;\n\n1\n\nThe RP’s callback url. https://www.shaneoh.org/callback in this case.\n\n2\n\nEither code or token.\n\n3\n\nUsually openid, openid email, openid email profile will do.\n\n\nNotice that there is no fetch request in this button. Instead, it simply redirects the page.\n\n\n\n\n\n\nNote\n\n\n\nThe RP does not make a direct API request to the OP. Instead, it redirects the user’s browser to the OP’s authentication page. Since this is a browser-driven interaction, the only way to pass information is through URL parameters.\n\n\nAfter the end-user’s username:password input is finished and the login button (of Keycloak’s) is pressed, this is when the first API request is invoked.\nThe click of a login button will invoke the authentication request, and Keycloak will validate the user’s credentials. If successful, Keycloak will redirect the user back to the specified redirect_uri with a query parameter containing an authorization code or an authorization token. This can be chosen with the RESPONSE_TYPE parameter by setting code or token.\nFor security reasons, the Authorization Code Flow (RESPONSE_TYPE=code) is preferred, as it minimizes the risk of exposing sensitive information through query parameters.\nSometimes there are some cases when only client service (React etc.) exists. In theses cases the impliclit flow is used by retrieving access token right away.\n\n\nToken Request\nOnce the client application receives the authorization code, it needs to exchange this code for an access token. This is done by making a POST request to the Keycloak token endpoint.\n\ngrant_type: This should be set to authorization_code.\ncode: The authorization code received from the previous step.\nredirect_uri: The same redirect URI used in the authentication request.\nclient_id: The client ID of the application.\nclient_secret: The client secret, if client authentication is required.\n\nThe request should be made with the Content-Type header set to application/x-www-form-urlencoded.\nHere is an example of how to make this request using JavaScript:\nconst tokenRequest = async (authCode) =&gt; {\n  const response = await fetch(`https://${HOST}/auth/realms/${REALM_NAME}/protocol/openid-connect/token`, {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/x-www-form-urlencoded'\n    },\n    body: new URLSearchParams({\n      grant_type: 'authorization_code',\n      code: authCode,\n      redirect_uri: REDIRECT_URI,\n      client_id: CLIENT_ID,\n      client_secret: CLIENT_SECRET\n    })\n  });\n\n  if (!response.ok) {\n    throw new Error('Token request failed');\n  }\n\n  const tokenData = await response.json();\n  return tokenData;\n};\n\n\nUserInfo Request\nAfter obtaining the access token, the client application can use it to request user information from the Keycloak UserInfo endpoint. This endpoint returns claims about the authenticated user.\nHere is an example of how to make this request using JavaScript:\nconst userInfoRequest = async (accessToken) =&gt; {\n  const response = await fetch(`https://${HOST}/auth/realms/${REALM_NAME}/protocol/openid-connect/userinfo`, {\n    method: 'GET',\n    headers: {\n      'Authorization': `Bearer ${accessToken}`\n    }\n  });\n\n  if (!response.ok) {\n    throw new Error('UserInfo request failed');\n  }\n\n  const userInfo = await response.json();\n  return userInfo;\n};"
  },
  {
    "objectID": "posts/RaspiK8s.html",
    "href": "posts/RaspiK8s.html",
    "title": "Kubernetes Cluster with Raspberry Pi",
    "section": "",
    "text": "Start building your own Kubernetes (K8s) cluster with Raspberry Pi!\nCheck out this video from NetworkChuck for guidance and inspiration.\nI’ve chosen to create a fully-fledged K8s cluster on Raspberry Pi, as a hands-on way to learn and troubleshoot Kubernetes. If you’d prefer a simpler setup, consider using lightweight distributions like K3s or K0s, which are optimized for ease of use on resource-constrained devices."
  },
  {
    "objectID": "posts/RaspiK8s.html#materials",
    "href": "posts/RaspiK8s.html#materials",
    "title": "Kubernetes Cluster with Raspberry Pi",
    "section": "Materials",
    "text": "Materials\nHere are the materials I started with to set up my Raspberry Pi Kubernetes cluster:\n\n4 \\(\\times\\) Raspberry Pi 4 Model B (4GB)\n4 \\(\\times\\) Micro SD cards (128GB)\n4 \\(\\times\\) RJ45 Ethernet cables\n4 \\(\\times\\) USB Type-C cables\n4 \\(\\times\\) USB chargers\n1 \\(\\times\\) Network switch\n1 \\(\\times\\) Acrylic tower case\n\nOnce assembled, the end product will look like this:\n\n\n\nRaspberry Pi Kubernetes Cluster\n\n\nThat wraps up the hardware setup! Now it’s time for the fun part."
  },
  {
    "objectID": "posts/RaspiK8s.html#installing",
    "href": "posts/RaspiK8s.html#installing",
    "title": "Kubernetes Cluster with Raspberry Pi",
    "section": "Installing",
    "text": "Installing\n\nOperating system\nLet’s start by installing the OS. For this project, we’ll be using ubuntu-20.04.5-preinstalled-server-arm64+raspi.img.\nLet’s bake some images onto the microSD cards. This part can feel tedious, but fortunately, there is a dedicated Raspberry Pi Imager from the official project that simplifies the process. It lets you select an OS from a dropdown list and burn images to the SD cards.\ncloud-init gets handy when installing with configurations. it streamlines the process of setting up and configuring instances, whether you’re dealing with a cloud VM or a local server. By tweaking settings in user-data and network-config, you can:\n\nSet up user accounts, SSH keys.\nInstall packages or run scripts on the first boot.\nConfigure network settings like static IP addresses or DNS.\n\nFor my case, the network architecture is configured as:\nISP ─── modem\n          │\n        router ─── L2 switch\n     192.168.0.1       │\n          │            ├── knode-01  192.168.0.10\n        macbook        ├── knode-02  192.168.0.11\n    192.168.0.100      ├── knode-03  192.168.0.12\n                       └── knode-04  192.168.0.13\nThe knode-01 will be configured as a control plane and rest as worker nodes. Check the example network-config file below.\n\n\nnetwork-config\n\nversion: 2\nethernets:\n  eth0:\n1    dhcp4: no\n    addresses:\n      - 192.168.0.10/24\n    gateway4: 192.168.0.1\n    nameservers:\n      addresses:\n        - 8.8.8.8\n        - 8.8.4.4\n\n\n1\n\nChange the IPv4 address of the physical Raspberry Pi node by disabling DHCP and setting a static IP.\n\n\nThe Raspberry Pi will boot up within a couple of minutes. The ol’ping works great for checking the state (otherwise you can try using AngryIPScanner).\nAfter booting up the servers, I recommend using tmux simply helps you run the same command across multiple SSH sessions simultaneously. Turn the sync on/off with Ctrl-bCtrl-b :: setw synchronize-panes on/off.\n\n\nKubernetes\nThe official documentation is here.\nAlso there are plenty of tutorials out there. A blog post by Daniel and a youtube by NetworkChuck helped me a lot.\nMainly, there are three components to install. Those are:\n\nkubelet: Runs on each node in a Kubernetes cluster, managing containerized applications and ensuring their health.\nkubeadm: Assists in setting up and configuring Kubernetes clusters by initializing the control plane and joining nodes.\nkubectl: Command-line tool for interacting with the Kubernetes API, enabling deployment, management, and troubleshooting of applications within the cluster.\n\nFollow the steps below.\n\nTurn off swap memory perpetually\n1sudo swapoff -a\n2sudo sed -i '/ swap / s/^/#/' /etc/fstab\nfree -h\n\n1\n\nTurns off swap memory temporarily.\n\n2\n\nDisables swap on boot to keep it turned off after rebooting.\n\n\n\n\nInstall CRI runtime\nkubelet requires a CRI runtime to work. In this tutorial, we will be installing containerd.\nThis tutorial follows the official installation guide.\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\nfor pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt install -y containerd.io\n\n\nConfigure CRI runtime\nCheck the official guide\nsudo sed -i '$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/' /boot/firmware/cmdline.txt\n\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1 \nnet.bridge.bridge-nf-call-iptables = 1 \nnet.ipv4.ip_forward = 1\nEOF\nsudo sysctl --system\n\nlsmod | grep br_netfilter\nmodprobe br_netfilter\necho 'br_netfilter' &gt; /etc/modules-load.d/br_netfilter.conf\n\ncat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\nsystemctl restart kubelet containerd\n\n\n/etc/containerd/config.toml\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n1    SystemdCgroup = true\n\n\n1\n\nThe SystemdCgroup parameter is a configuration setting in container runtimes, such as containerd, to manage cgroups (control groups) using systemd rather than relying on the runtime’s native cgroup management. Setting SystemdCgroup=true enables systemd to control the cgroup hierarchy for containers.\n\n\nsudo systemctl restart containerd\n\n\nkubelet, kubeadm and kubectl\nsudo systemctl restart containerd\n\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n\nsudo systemctl enable --now kubelet\n\n\nInitialize control-plane\nWe are planning to use Flannel for container networking.\nFirstly, initialize the control plane with kubeadm init command and later join the worker nodes.\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\nJoin worker nodes\n1kubeadm join 192.168.0.10:6443 \\\n  --token 1b120a.0349bfe2a349b331 \\ \n  --discovery-token-ca-cert-hash \\\n    sha256:09321a32...39058c12\n\n1\n\nThe IP address for the control plane’s API server with the port defaulting to 6443.\n\n\n\n\nApply CNI plugin\nFlannel is a popular CNI (Conatainer Networking Interface) plugin in Kubernetes that provides an unified network across the entire cluster.\nFlannel enables this by:\n\nOverlay Network: Flannel creates an overlay network, allowing pods on different nodes to communicate as though they were on the same local network. This abstraction flattens the network by providing a single, consistent IP address space for all pods within the cluster.\nIP Address Management: Each pod receives a unique IP address within this flat network, enabling packets to travel between nodes without requiring complex routing or Network Address Translation (NAT). This makes inter-node communication more straightforward.\nFlexible Backends: Flannel offers different backends to implement the overlay network. For example:\n\nVXLAN: This backend encapsulates network traffic in UDP packets, enabling scalable and simple flat networking.\nHost-GW: This backend enables direct routing between nodes on the same subnet, which can be useful in specific networking environments.\n\n\nSSH into the control-plane and create pods for CNI networking.\nwget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\nkubectl apply -f kube-flannel.yml\n\n\nApply LoadBalancer\nWe have created a baremetal Kubernetes cluster. To properly load balance services, consider using MetalLB.\nwget https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml\nkubectl apply -f metallb-native.yaml\n\n\nmetallb-config.yaml\n\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: metallb-pool-01\n  namespace: metallb-system\nspec:\n  addresses:\n1  - 192.168.0.20-192.168.0.30\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: metallb-adv-01\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - metallb-pool-01\n\n\n1\n\nThis IP range is used by MetalLB to assign external IPs to LoadBalancer services. Ensure that the range falls within your network’s subnet (e.g., 192.168.0.0/24)\n\n\nkubectl apply -f metallb-config.yaml\n\n\nEnabling NVIDIA driver in Kubernetes\nI found a well documented tutorial post by Deagwon Bu. Highly regared.\n\n\n\n\n\n\nNote\n\n\n\nThe version of the NVIDIA driver on the host sets the upper limit for the NVIDIA runtime version that can be utilized in the container.\n\n\nFirstly, install the nvidia-container-toolkit to enable GPU-accelerated containers. There is an official install guide for this.\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\nWhen using containerd, config.toml is where NVIDIA drivers are defined.\n\n\n/etc/containerd/containerd.toml\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n  ...\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n    ...\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n      ...\n      SystemdCgroup = true\n\n  [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia]\n    privileged_without_host_devices = false\n    runtime_engine = \"\"\n    runtime_root = \"\"\n    runtime_type = \"io.containerd.runc.v1\"\n\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia.options]\n      BinaryName = \"/usr/bin/nvidia-container-runtime\"\n      SystemdCgroup = true\n\nsudo systemctl restart containerd\nWhen using K3s by Rancher, also amend the following file:\n\n\n/var/lib/rancher/k3s/agent/etc/containerd/config.toml\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\"]\n  runtime_type = \"io.containerd.runc.v2\"\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\".options]\n  BinaryName = \"/usr/bin/nvidia-container-runtime\"\n  SystemdCgroup = true\n\nsudo systemctl restart k3s\nnodeSelector can be useful when applying NVIDIA plugin to only GPU enabled nodes.\nkubectl label nodes &lt;node&gt; gpu=nvidia\n\nwget https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.16.2/deployments/static/nvidia-device-plugin.yml\n\n\nnvidia-device-plugin.yml\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidia-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: nvidia-device-plugin-ds\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: nvidia-device-plugin-ds\n    spec:\n1      nodeSelector:\n        gpu: nvidia\n        ...\n\n\n1\n\nAdd the following lines.\n\n\nkubectl apply -f nvidia-device-plugin.yml\nCreate the file below and apply it.\n\n\nnvidia-runtime-class.yml\n\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: nvidia\nhandler: nvidia\n\nkubectl apply -f nvidia-runtime-class.yml\nIt’s all set! Run pods with GPU resource. Check the template below.\n\n\ngpu-test.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n1  runtimeClassName: nvidia\n2  nodeSelector:\n    gpu: nvidia\n  containers:\n  - name: gpu-container\n    image: nvidia/cuda:12.1.0-runtime-ubuntu20.04\n    resources:\n      limits:\n        cpu: \"500m\"\n        memory: \"1Gi\"\n3        nvidia.com/gpu: 2\n    env:\n    - name: CUDA_VISIBLE_DEVICES\n      value: \"0,1\"\n    - name: TF_FORCE_GPU_ALLOW_GROWTH\n      value: \"true\"\n    - name: CUDA_DEVICE_ORDER\n      value: \"PCI_BUS_ID\"\n\n\n1\n\nSet the runtime defined in config.toml.\n\n2\n\nAllow pod creation in GPU enabled nodes.\n\n3\n\nSet the number of GPUs."
  },
  {
    "objectID": "posts/RaspiK8s.html#accessing",
    "href": "posts/RaspiK8s.html#accessing",
    "title": "Kubernetes Cluster with Raspberry Pi",
    "section": "Accessing",
    "text": "Accessing\nThere are typical authentication methods to access the cluster: Bearer tokens and CA certificates. However, kubectl can support other methods such as client certificates and external authentication providers.\nThe ~/.kube/config file contains the CA certificate issued when initializing the node.\nTo use the same configuration on another system, you can transfer the config file via sftp to the remote computer.\n\n\n~/.kube/context\n\napiVersion: v1\nkind: Config\n\nclusters:\n- name: raspberrypi\n  cluster:\n    certificate-authority-data: &lt;super-secret&gt;\n    server: https://&lt;public-ip&gt;:6443\n\nusers:\n- name: raspberrypi-admin\n  user:\n    client-certificate-data: &lt;super-secret&gt;\n    client-key-data: &lt;super-secret&gt;\n\ncontexts:\n- name: raspberrypi-admin@raspberrypi\n  context:\n    cluster: raspberrypi\n    user: raspberrypi-admin\n\ncurrent-context: raspberrypi-admin@raspberrypi\n\npreferences: {}\n\nWith your router configured to forward requests from the public IP to the private IP, this sould work seamlessly."
  },
  {
    "objectID": "posts/Tmux.html",
    "href": "posts/Tmux.html",
    "title": "Tmux",
    "section": "",
    "text": "Start off with installing tmux with homebrew on Mac."
  },
  {
    "objectID": "posts/Tmux.html#intro",
    "href": "posts/Tmux.html#intro",
    "title": "Tmux",
    "section": "Intro",
    "text": "Intro\nThere are three main concepts in tmux: session, window, and pane. Start by entering tmux in the terminal.\nThe screen you see right after entering the command is a pane in a window."
  },
  {
    "objectID": "posts/Tmux.html#pane",
    "href": "posts/Tmux.html#pane",
    "title": "Tmux",
    "section": "Pane",
    "text": "Pane\nSplit the pane using Ctrl-bCtrl-b %% and Ctrl-bCtrl-b \"\".\nThe Ctrl-bCtrl-b works as a prefix to send a command — later below.\nNavigate through the panes using Ctrl-bCtrl-b ↑↑, Ctrl-bCtrl-b →→, Ctrl-bCtrl-b ↓↓, Ctrl-bCtrl-b ←←."
  },
  {
    "objectID": "posts/Tmux.html#window",
    "href": "posts/Tmux.html#window",
    "title": "Tmux",
    "section": "Window",
    "text": "Window\nOpen a new window with Ctrl-bCtrl-b cc.\nSee the windows you opened on the bottom green bar? This gives us some information about the windows in the session. The current window you are seeing is marked with an *.\nNavigate through the windows using Ctrl-bCtrl-b nn — n is for next. This will cycle through all the windows in the current session. Reverse navigate with Ctrl-bCtrl-b pp — p is for previous. You can also navigate directly to a window using the index. The bindings will be like Ctrl-bCtrl-b 11."
  },
  {
    "objectID": "posts/Tmux.html#session",
    "href": "posts/Tmux.html#session",
    "title": "Tmux",
    "section": "Session",
    "text": "Session\nFrom the very first, right after the tmux command, you are attached to a session with an auto generated index. Detach the session with the Ctrl-bCtrl-b dd command — and d is for detach. It’s almost the same as starting the bash session and detaching with the exit command. Tmux can also be detached using the exit command, but this can be tedious because each split pane needs to be closed individually with the exit command."
  },
  {
    "objectID": "posts/Tmux.html#configurations",
    "href": "posts/Tmux.html#configurations",
    "title": "Tmux",
    "section": "Configurations",
    "text": "Configurations\nFeeling comfortable with the keybindings? I hope not. The default keybindings can put significant stress on your left pinky. This brings us to the .tmux.conf file for some configuration.\n\n\n~/.tmux.conf\n\nunbind-key C-b\nset -g prefix C-a\n1bind-key C-a send-prefix\n\n2set -g mouse on\n3set -g base-index 1\nset -g renumber-windows on\nset -g default-terminal \"tmux-256color\"\n\n4bind r source-file ~/.tmux.conf \\; display-message \".tmux.conf reloaded!\"\n\n5bind '\\' split-window -h -c \"#{pane_current_path}\"\n6bind - split-window -v -c \"#{pane_current_path}\"\n\n7bind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n8bind x kill-pane\n9bind X kill-window\n\n\n1\n\nReplace the prefix key from Ctrl-bCtrl-b to Ctrl-aCtrl-a. When you are using a keyboard like HHKB, this will come in pretty handy. Let’s talk more about HHKB in some other posts.\n\n2\n\nPretty straight forward. Helps you navigate split panes with a mouse.\n\n3\n\nThe default is 0. For me, 0 key is a bit far for everyday use.\n\n4\n\nReload the .tmux.conf file and display a message when done — similar to something like source .bashrc.\n\n5\n\nSplit the window horizontally using the current pane’s path.\n\n6\n\nSplit the window vertically using the current pane’s path.\n\n7\n\nMove between panes using Vim-style keybindings.\n\n8\n\nKill the current pane.\n\n9\n\nKill the current window.\n\n\nPersonal preference on the following one.\n\n\n.zshrc\n\ntmux() {\n    if [ \"$#\" -eq 0 ]; then\n1        command tmux new-session -A -s default\n    else\n        command tmux \"$@\"\n    fi\n}\n\n\n1\n\nAttach a session named default when tmux is typed."
  }
]