---
title: Entropy
subtitle: The lower bound of $\bar \lambda$
description: ""
author: Shane Oh
date: 2023-09-14
image: false
categories:
  - Information Theory
  - Data Compression
---

I highly recommend taking a look at the [Huffman Coding](HuffmanCoding.qmd) post first.
Huffman coding provides an optimal compression solution for a given data distribution,
whereas Shannon-Fano coding does not.

It may be easier for us to first learn Huffman coding (a bottom-up approach to building the tree)
in the algorithms class and then move on to Shannon-Fano coding (a top-down approach).

Take a look at the [video](https://youtu.be/B3y0RsVCyrw?si=ikWennqSHzgY9pfn).

::: {.callout-note}
# Historical Context: Shannon's Entropy and Huffman Coding
After Claude Shannon introduced entropy in 1948, which defines the theoretical limit for
optimal data compression, David A. Huffman built on this in 1952 by creating Huffman coding
during his Ph.D.
:::

## Recap of Huffman coding

In the last [post](HuffmanCoding.qmd), we derived the average codeword length
of Huffman coding.

$$
\bar \lambda = \mathbb E [ \lambda ]
= \sum_{i=1}^N p(x_i) \lambda_i
= \sum_{i=1}^N p(x_i) \lceil \log_2 \dfrac{1}{p(x_i)} \rceil
$$

One thing we had a hard time deriving was $\lambda$.
We needed to check the frequency (or probability) of each character and
aggregate them into a binary tree structure.
After that, we traced the path from the root node to each leaf,
encoding every character into a binary codeword.
Finally, we mapped all of the codewords to the function `len` to obtain our most desired value,
the length of each codeword.

The value $\lambda$ can be interpreted as the number of bits (information) required to
losslessly represent an arbitrary group (such as a character or color) within
the given data. As you can feel from the word group, we were working on a discrete random variable.

We can guarantee the optimality of $\lambda$ only with the given data distribution.
If different data is provided, the frequency (probability) changes,
causing the entire Huffman tree to differ from before.

::: {.callout-note}
# What if we provide a data only with the most frequent character from the existing Huffman tree?
This will reduce $\bar \lambda$ exceptionally but not with an optimal length.
If all characters are the same, the optimal length for the given data
will be 0 because the leaf node will also be the root.
:::

## Entropy as a lower bound of $\bar \lambda$

What if I say we don't need all of the cumbersome processes mentioned above?
Take a look at the formula below.

$$
H(X) = \mathbb E[I(X)] = \sum_{i=1}^N p(x_i) \log_2 \dfrac{1}{p(x_i)}
$$

Is entropy just another form of average codeword length?

The answer is no. This works as the theoretical **lower bound** for any data distribution
(both **discrete** and **continuous**) when compressing.
We can not get below this bound if we are performing a lossless compression.

We can see that the $\lambda_i$ has turned into a $\log$ form with a probability.
Since $x_i$ is the only parameter, we can define **$H$ as a scalar funciton for any random vaiable $X$**.
As the probability of $x_i$ increases
we can compress the information into a smaller, more compact $I(x_i)$.

We can set the theoretical lower bound for $\bar \lambda$ as below.

$$
H(X) \leq \bar \lambda
$$

Let's prove this!

## Proofing the lower bound of $\bar \lambda$

We need to know some basics of Jensen's inequality and Kraft-McMillan inequality to prove this.
Check out the [Jensen's inequality](JensensInequality.qmd),
[Kraft-McMillan inequality](HuffmanCoding.qmd#sec-kraft-mcmillan-inequality) posts to see more.

$$
\begin{align*}
H(X) - \bar{\lambda}
&= \sum_{i=1}^N \left( p(x_i) \dfrac{1}{\log_2 p(x_i)} - p(x_i) \lambda_i \right) \\[10pt]
&= \sum_{i=1}^N p(x_i) \left( \dfrac{1}{\log_2 p(x_i)} - \lambda_i \right) \\[10pt]
&= \sum_{i=1}^N p(x_i) \left( \dfrac{1}{\log_2 p(x_i)} \log_2 2^{-\lambda_i} \right) \\[10pt]
&= \sum_{i=1}^N p(x_i) \log_2 \left( \dfrac{2^{-\lambda_i}}{p(x_i)} \right)
   \leq \log_2 \sum_{i=1}^N p(x_i) \dfrac{2^{-\lambda_i}}{p(x_i)} &\cdots \text{Jensen's inequality} \\[10pt]
&= \log_2 \sum_{i=1}^N 2^{-\lambda_i} \leq \log_2 1 &\cdots \text{Kraft-McMillan inequality} \\[10pt]
&= 0
\end{align*}
$$

Well, that's it! For fun, let's take a look at the graph below.

::: {#fig-lower-bound}
![](Entropy/entropy.svg){width=80%}

Lower bound of lambda bar
:::

Think $\varphi(x)$ as $-\log_2 p(x)$.
You can clearly see that entropy (the blue cross) is working as a lower bound.

We have not talked about the upper bound of $\bar \lambda$.

$$
H(X) \leq \bar \lambda \lt H(X)+1
$$

The red and blue crosses are the weighted averages of each function's outputs. 
Try ripping the crosses apart from each other. Can you make it greater than 1? 
Probably not. Infinitely approaching all $p(x_i)$ to values near powers of 2 from 
the left-hand side is the most probable way, but you won't be able to get there. 

If you do get there, it means you are allowing all $x_i$ to have an extra bit, 
which is not optimal in the first place.

## Examples

```{python}
# | echo: false
import matplotlib.pyplot as plt
from scipy.stats import binom, uniform
import numpy as np

plt.rc('text', usetex=True)
plt.rc('font', family='serif')

def binomcoord(n, p):
    X = list(range(n+1))
    Y = [binom.pmf(r, n, p) for r in X]
    return X, Y

n, p = 10, 0.5
X, Y = binomcoord(n, p)
fig, ax = plt.subplots(figsize=(2,2))
ax.set_title(f"Bin({n},{p})")
ax.plot(X, Y, 'ko', ms=3)
ax.vlines(X, 0, Y, 'k', lw=1)
ax.set_ylim((0, 1.1))
ax.set_xticks(X)
ax.margins(0.1)
fig.show()

n, p = 10, 0.99
X, Y = binomcoord(n, p)
fig, ax = plt.subplots(figsize=(2,2))
ax.set_title(f"Bin({n},{p})")
ax.plot(X, Y, 'ko', ms=3)
ax.vlines(X, 0, Y, 'k', lw=1)
ax.set_ylim((0, 1.1))
ax.set_xticks(X)
ax.margins(0.1)
fig.show()


# # Sample plot mimicking TikZ style
# x = np.linspace(0, 2 * np.pi, 100)
# y = np.sin(x)

# # Create a clean plot
# fig, ax = plt.subplots()
# ax.plot(x, y, label=r'$\sin(x)$', color='black', linewidth=1)

# # Minimalistic style similar to TikZ
# ax.spines['top'].set_visible(False)
# ax.spines['right'].set_visible(False)
# ax.spines['left'].set_linewidth(0.5)
# ax.spines['bottom'].set_linewidth(0.5)

# ax.xaxis.set_ticks_position('bottom')
# ax.yaxis.set_ticks_position('left')

# # Set labels with LaTeX font
# ax.set_xlabel(r'$x$', fontsize=12)
# ax.set_ylabel(r'$\sin(x)$', fontsize=12)

# # Add a legend
# ax.legend(loc='upper right', fontsize=12)

# # Reduce tick size and make lines thin
# ax.tick_params(axis='both', which='major', labelsize=10, width=0.5)

# plt.show()
```