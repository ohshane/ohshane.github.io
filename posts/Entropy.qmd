---
title: Entropy
subtitle: The amount of uncertainty
description: ""
author: Shane Oh
date: 2023-04-14
image: false
categories:
  - Information Theory
  - Data Compression
---

I highly recommend taking a look at the [Huffman Coding](HuffmanCoding.qmd) post first.
Huffman coding provides an optimal compression solution for a given data distribution,
whereas Shannon-Fano coding does not.

It may be easier for us to first learn Huffman coding (a bottom-up approach to building the tree)
in the algorithms class and then move on to Shannon-Fano coding (a top-down approach).

Take a look at the [video](https://youtu.be/B3y0RsVCyrw?si=ikWennqSHzgY9pfn).

::: {.callout-note}
# Historical Context: Shannon's Entropy and Huffman Coding
After Claude Shannon introduced entropy in 1948, which defines the theoretical limit for
optimal data compression, David A. Huffman built on this in 1952 by creating Huffman coding
during his Ph.D.
:::

## Optimal lossless compresison