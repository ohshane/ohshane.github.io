---
title: Measuring Distributions
subtitle: Gibbs' inequality and more!
description: Just like Huffman coding's optimality, we can discuss entropy's optimality using Gibbs' inequality
author: Shane Oh
date: 2024-10-04
image: false
categories:
  - Entropy
  - Information Theory
bibliography: MeasureingDistributions/references.bib
---

Entropy plays an important role in measuring probability distributions (or RVs).
Besides coding theories such as Huffman and Shannon-Fano, etc., it helps 
quantify the amount of information using only probability.

$$
H(X) =
H(p) =
H(p,p) =
\sum_{\mathbf x \in \mathcal X} p_X(\mathbf x) \log \dfrac{1}{p_X(\mathbf x)}
$$

Thus, we can define entropy as a **scalar function** of an arbitrary probability mass 
function (PMF). We often write $H(X)$ as $H(p)$, same for the differential entropy: $h(X)$ as $h(f)$[^differential-entropy].

[^differential-entropy]: [ECE 587 / STA 563: Lecture 7 - Diï¬€erential Entropy](http://reeves.ee.duke.edu/information_theory/lecture7-Differential_Entropy.pdf)

One thing to become accustomed to is using $\mathbb E$ and 
$\sum_x p(x)$ (or $\int_\mathcal{X} f(x)$, depending on the RVs) interchangeably.

In the previous posts of [Huffman coding](HuffmanCoding.qmd) and [entropy](Entropy.qmd),
we have derived that entropy is the the lower bound of average codeword length ($\bar \lambda$),
simultaniously meaning the optimality of codeword length.

For the dedicated data distribution,
the best encoding strategy to achieve the shortest codeword length is
using the codewords with repective to the frequency of its own data distribution.
Shorter codewords length for more frequent random variable.

The optimality in Huffman coding is expressed as follows.
Huffman coding achieves optimality in lossless coding using a greedy algorithm.

$$
\sum_{i=1}^N p_i \lambda_i \leq
\sum_{i=1}^N p_i \lambda_i^{'}
$$

Similarly, the optimality for entropy can be expressed with Gibbs' inequality.

$$
\sum_{i=1}^{N} p_i \log \dfrac{1}{p_i} \leq \sum_{i=1}^{N} p_i \log \dfrac{1}{q_i}
$$

What this tells you is that for the given distribution $P$, $-\log P$ is global optimal.

## Proving Gibbs' inequality

\begin{align*}
&\sum_{i=1}^{N} p_i \log \dfrac{1}{q_i} - \sum_{i=1}^{N} p_i \log \dfrac{1}{p_i} \\
=& \sum_{i=1}^{N} p_i \log \dfrac{p_i}{q_i} \\
=& \sum_{i=1}^{N} p_i \left( -\log \dfrac{q_i}{p_i} \right) \\
\geq& -\log \sum_{i=1}^{N} p_i \dfrac{q_i}{p_i} \quad \cdots \text{Jensen's inequality} \\
=& -\log \sum_{i=1}^{N} q_i \\
=& \, 0
\end{align*}

## Entropy, cross entropy and KL divergence

We can reform the upper inequality as below.
$H(p,q)$ is called the **cross entropy**.

$$
D_\text{KL}(p \| q) \triangleq H(p,q) - H(p,p) \geq 0
$$

Entropy of distribution is **less than or equal to its cross entropy** with any other distribution.
The difference between the two quantities is the Kullback-Leibler divergence (relative entropy).

@murphy2013machine states that
the cross entropy is the average number of bits
needed to encode data coming from a source with distribution $p$ when we use model $q$ to
define our codebook. The Kullback Leibler (KL) divergence is the average number of extra bits needed to
encode the data, due to the fact that we used distribution $q$ to encode the data instead of the
true distribution $p$.

### Jensen-Shannon divergence

As you can see from the definition, it is generally not the case that $D_\text{KL}(p \| q) = D_\text{KL}(q \| p)$. In some cases, it is useful to define a non-negative scalar between the two distributions that can act as a symmetric distance metric.

$$
\mathrm{JSD}(p \| q) =
\dfrac{1}{2} D_\text{KL}\left(p \, \bigg\| \, \dfrac{p+q}{2} \right) +
\dfrac{1}{2} D_\text{KL}\left(q \, \bigg\| \, \dfrac{p+q}{2} \right)
$$

This will satisfy  $\mathrm{JSD}(p \| q) = \mathrm{JSD}(q \| p)$.

Since $D_\text{KL}$ is non-negative, $\mathrm{JSD}$ also inherits this property.
This will become useful when deriving the optimality of GANs.