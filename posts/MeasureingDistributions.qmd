---
title: Measuring Distributions
subtitle: Gibbs' inequality and more!
description: Just like Huffman coding's optimality, we can discuss entropy's optimality using Gibbs' inequality
author: Shane Oh
date: 2024-10-04
image: false
categories:
  - Entropy
  - Information Theory
bibliography: MeasureingDistributions/references.bib
---

Entropy plays an important role in measuring probability distributions (or RVs).
Besides coding theories such as Huffman and Shannon-Fano, etc., it helps 
quantify the amount of information using only probability.

$$
H(X) =
H(p) =
H(p,p) =
\sum_{\mathbf x \in \mathcal X} p_X(\mathbf x) \log \dfrac{1}{p_X(\mathbf x)}
$$

Thus, we can define entropy as a **scalar function** of an arbitrary probability mass 
function (PMF). We often write $H(X)$ as $H(p)$, same for the differential entropy: $h(X)$ as $h(f)$[^differential-entropy].

[^differential-entropy]: [ECE 587 / STA 563: Lecture 7 - Diï¬€erential Entropy](http://reeves.ee.duke.edu/information_theory/lecture7-Differential_Entropy.pdf)

One thing to become accustomed to is using $\mathbb E$ and 
$\sum_x p(x)$ (or $\int_\mathcal{X} f(x)$, depending on the RVs) interchangeably.

In the previous posts of [Huffman coding](HuffmanCoding.qmd) and [entropy](Entropy.qmd),
we have derived that entropy is the the lower bound of average codeword length ($\bar \lambda$),
simultaniously meaning the optimality of codeword length.

For the dedicated data distribution,
the best encoding strategy to achieve the shortest codeword length is
using the codewords with repective to the frequency of its own data distribution.
Shorter codewords length for more frequent random variable.

The optimality in Huffman coding is expressed as follows.
Huffman coding achieves optimality in lossless coding using a greedy algorithm.

$$
\sum_{i=1}^N p_i \lambda_i \leq
\sum_{i=1}^N p_i \lambda_i^{'}
$$

Similarly, the optimality for entropy can be expressed with Gibbs' inequality.

$$
\sum_{i=1}^{N} p_i \log \dfrac{1}{p_i} \leq \sum_{i=1}^{N} p_i \log \dfrac{1}{q_i}
$$

What this tells you is that for the given distribution $P$, $-\log P$ is global optimal.

## Proving Gibbs' inequality

\begin{align*}
&\sum_{i=1}^{N} p_i \log \dfrac{1}{q_i} - \sum_{i=1}^{N} p_i \log \dfrac{1}{p_i} \\
=& \sum_{i=1}^{N} p_i \log \dfrac{p_i}{q_i} \\
=& \sum_{i=1}^{N} p_i \left( -\log \dfrac{q_i}{p_i} \right) \\
\geq& -\log \sum_{i=1}^{N} p_i \dfrac{q_i}{p_i} \quad \cdots \text{Jensen's inequality} \\
=& -\log \sum_{i=1}^{N} q_i \\
=& \, 0
\end{align*}

## Kullback-Leibler divergence

We can reform the upper inequality as below.
$H(p,q)$ is called the **cross entropy**.

$$
D_\text{KL}(p \| q) \triangleq H(p,q) - H(p,p)
$$

$$
H(p,q) - H(p,p) = \sum_{i=1}^{N} p_i \log \dfrac{p_i}{q_i} \geq 0
$$

$$
D_\text{KL}(p \| q) = 0 \iff p = q
$$

Entropy of distribution is **less than or equal to its cross entropy** with any other distribution.
The difference between the two quantities is the Kullback-Leibler divergence (relative entropy).

@murphy2013machine states that
the cross entropy is the average number of bits
needed to encode data coming from a source with distribution $p$ when we use model $q$ to
define our codebook. The Kullback Leibler (KL) divergence is the average number of extra bits needed to
encode the data, due to the fact that we used distribution $q$ to encode the data instead of the
true distribution $p$.

### Jensen-Shannon divergence

As you can see from the definition, it is generally not the case that $D_\text{KL}(p \| q) = D_\text{KL}(q \| p)$. In some cases, it is useful to define a non-negative scalar between the two distributions that can act as a symmetric distance metric.

$$
\mathrm{JSD}(p \| q) =
\dfrac{1}{2} D_\text{KL}\left(p \, \bigg\| \, \dfrac{p+q}{2} \right) +
\dfrac{1}{2} D_\text{KL}\left(q \, \bigg\| \, \dfrac{p+q}{2} \right)
$$

This will satisfy  $\mathrm{JSD}(p \| q) = \mathrm{JSD}(q \| p)$.

Since $D_\text{KL}$ is non-negative, $\mathrm{JSD}$ also inherits this property.
This will become useful when deriving the optimality of GANs.

## Mutual information

Consider two random variables, $X$ and $Y$. Suppose we want to know how much 
knowing one variable tells us about the other. The more they are correlated, 
the more different $p(X,Y)$ and $p(X)p(Y)$ will get. Defining the $D_\text{KL}$ 
between $p(X,Y)$ and $p(X)p(Y)$ can represent how different $p(X,Y)$ is from independence.

$$
I(X;Y) \triangleq D_\text{KL}(p(X,Y) \| p(X)p(Y))
= \sum_{x} \sum_{y} p(x,y) \log \dfrac{p(x,y)}{p(x)p(y)}
$$

$$
I(X;Y) = 0 \iff p(X,Y) = p(X)p(Y) \iff X \perp Y
$$

```{python}
# | echo: false
# | layout-ncol: 2
# | label: fig-mutual-information
# | fig-cap: "Mutual Information"
# | fig-subcap: 
# |   - "p(X,Y)"
# |   - "p(X)p(Y)"
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

mean = [0.0, 0.0]
cov = [[1.0, 0.5],
       [0.5, 1.0]]

np.random.seed(42)
x, y = np.random.multivariate_normal(mean, cov, 200).T
heatmap, xedges, yedges = np.histogram2d(x, y, bins=20, range=[[-4,4],[-4,4]])
heatmap /= heatmap.sum()

X = heatmap.sum(0, keepdims=True)
Y = heatmap.sum(1, keepdims=True)

fig = plt.figure(figsize=(3,3))
gs = GridSpec(2, 2, width_ratios=[0.2, 4], height_ratios=[4, 0.2], wspace=0.04, hspace=0.04)

ax_main = fig.add_subplot(gs[0,1])
ax_main.pcolormesh(xedges, yedges, heatmap)
ax_main.set_axis_off()

ax = fig.add_subplot(gs[0,0], sharey=ax_main)
ax.pcolormesh([0,1], yedges, Y)
ax.set_xticks([])
ax.set_yticks(np.arange(-4,5,1))
ax.set_ylabel('Y')

ax = fig.add_subplot(gs[1,1], sharex=ax_main)
ax.pcolormesh(xedges, [0,1], X)
ax.set_xticks(np.arange(-4,5,1))
ax.set_yticks([])
ax.set_xlabel('X')
plt.show()

fig = plt.figure(figsize=(3,3))
gs = GridSpec(2, 2, width_ratios=[0.2, 4], height_ratios=[4, 0.2], wspace=0.04, hspace=0.04)

ax_main = fig.add_subplot(gs[0,1])
ax_main.pcolormesh(xedges, yedges, np.matmul(Y,X))
ax_main.set_axis_off()

ax = fig.add_subplot(gs[0,0], sharey=ax_main)
ax.pcolormesh([0,1], yedges, Y)
ax.set_xticks([])
ax.set_yticks(np.arange(-4,5,1))
ax.set_ylabel('Y')

ax = fig.add_subplot(gs[1,1], sharex=ax_main)
ax.pcolormesh(xedges, [0,1], X)
ax.set_xticks(np.arange(-4,5,1))
ax.set_yticks([])
ax.set_xlabel('X')
plt.show()
```
