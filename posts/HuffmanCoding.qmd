---
title: Huffman Coding
subtitle: Lossless data compression
description: ""
author: Shane Oh
date: 2023-04-14
image: false
categories:
  - Information Theory
  - Data Compression
  - Algorithms
---

```{python}
#| echo: false
s_apple = "AN APPLE A DAY KEEPS THE DOCTOR AWAY" # higher compression
s_quick = "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG" # pangram
```

Thanks to Pizzey Technology for the wonderful [video](https://youtu.be/iEm1NRyEe5c?si=SvWMcYR6AZraymSd).

## Concepts

Huffman coding is a type of **variable-length prefix coding** that assigns
shorter codes to more frequent symbols and longer codes to less frequent
symbols. Formal definitions aren't very useful when dealing with other
concepts, in this case, entropy.

Well, keep this in mind: **_lossless_**.

Huffman coding is one of the lossless
compression methods. In terms of compression, you cannot compress
data smaller than the limit defined by entropy.

Huffman is a bottom-up approach to compression which is optimal, while Shannon 
entropy defines the theoretical limit, a lower bound.

Examples first.

## ASCII
ASCII is a character encoding developed in the '60s by ANSI.
It uses 7 bits for character representation.
However, in modern computing, characters are stored in bytes (8 bits) for compatibility reasons.

Let's take a look at the 8-bit encoded sentence.

```{python}
#| echo: false
def c2b(c):
    return format(ord(c), '08b')

def show_b(s):
    print(s, end='\n\n')

    l = 0
    bits = 0
    for c in s:
        b = c2b(c)
        print(b, end=' ')
        l += len(b) + 1
        bits += len(b)
        if l > 71:
            l = 0
            print()

    print(f"\n\n{bits} bits / {bits/8:.1f} bytes")
    print(f"compression rate: {(1 - (bits/8) / len(s))*100:.1f}%")

show_b(s_apple)
```
You can easily determine the total number of bits in this sentence
by simply multiplying the number of characters (including spaces) by the **fixed** encoding size.

## Compress it!
The more **frequently** a character appears, the **shorter** its encoded length becomes.

```{python}
# | echo: false
# | output: asis
print(f"""The phrase `{s_apple}`
uses `A` to rhyme which makes the example more fun.""")
```

### Huffman tree {#sec-huffman-tree}
### Huffman tree {#sec-huffman-tree}
We **build the tree from the bottom** using the **less frequent characters**.
Eventually, the less frequent ones are placed at deeper levels.
The deeper a character is, the longer the traversal route from the root,
which results in a longer encoding length.

Check the code below.

```{python}
from collections import Counter

class Node:
    def __init__(self, char='', freq=0, left=None, right=None):
        self.char  = char
        self.freq  = freq
        self.left  = left
        self.right = right
    
    @property
    def is_leaf(self):
        return self.left is None and self.right is None
    
    def __str__(self):
        if self.is_leaf:
            return f"'{self.char}': {self.freq}"
        return str(self.freq)
    
    def __lt__(self, other):
        return self.freq < other

    def __gt__(self, other):
        return self.freq > other
        
class HuffmanTree:
    def __init__(self, freq_table):
        self.freq_table = dict(freq_table)
        self.encoded_table = {}
        self.root = None

        self.build()
        self.encode()

    @property
    def L(self):
        total = 0
        for char, freq in self.freq_table.items():
            total += len(self.encoded_table[char]) * freq
        return total / sum(self.freq_table.values())
    
    def build(self):
        nodes = [Node(char, freq) for char, freq in self.freq_table.items()]
        while len(nodes) > 1:
            node1 = nodes.pop(nodes.index(min(nodes))) # <3>
            node2 = nodes.pop(nodes.index(min(nodes))) # <3>
            node1 = nodes.pop(nodes.index(min(nodes))) # <3>
            node2 = nodes.pop(nodes.index(min(nodes))) # <3>
            node  = Node(freq=node1.freq+node2.freq,
                         left=node2,
                         right=node1)
            nodes.append(node)
            self.root = node
    
    def encode(self):
        def dfs(node, path=''):
            if node.is_leaf: # <1>
                self.encoded_table[node.char] = path # <1>
                return
            dfs(node.left,  path+'0') # <2>
            dfs(node.right, path+'1') # <2>

        dfs(self.root)
```
1. Characters are only at the leaf nodes.
This property ensures that each encoded value is not a substring of another and keeps the unique decodability. This is also called as the **prefix-free coding**.
2. Use `0` for the left and `1` for the right. The path from the root to the 
leaf node represents the encoded result.
3. As the function `min` returns the index of the first occurring minimum value (`node.freq`)
from the list `nodes`, the built tree is not unique but preserves optimality.

```{python}
#| echo: false 
tree_apple = HuffmanTree(Counter(s_apple))
tree_quick = HuffmanTree(Counter(s_quick))
```

```{python}
#| echo: false
#| output: asis 

def show_ht(tree):
    print("```{mermaid}") 
    print("graph RL")

    queue = [tree.root]
    visited = []
    while queue:
        node = queue.pop(0)
        if node not in visited:
            print(f"{id(node)}[{str(node)}]")
            if not node.is_leaf:
                queue.append(node.left)
                queue.append(node.right)
                print(f"{id(node)} -- 0 --> {id(node.left)}")
                print(f"{id(node)} -- 1 --> {id(node.right)}")
            visited.append(node)

    print("```") 

show_ht(tree_apple)
```

### Huffman code

Finally, the Huffman coded result is below.

```{python}
#| echo: false
def c2hc(c, tree):
    return tree.encoded_table[c]

def show_hc(s, tree):
    print(s, end='\n\n')

    l = 0
    bits = 0
    for c in s:
        b = c2hc(c, tree)
        print(b, end=' ')
        l += len(b) + 1
        bits += len(b)
        if l > 71:
            l = 0
            print()

    print(f"\n\n{bits} bits / {bits/8:.1f} bytes")
    print(f"compression rate: {(1 - (bits/8) / len(s))*100:.1f}%")
```

```{python}
#| echo: false
show_hc(s_apple, tree_apple)
```

```{python}
#| echo: false
#| output: asis 
print(f"""You might be familiar with the pangram `{s_quick}`.
It contains all the letters from `A` to `Z`, which makes it harder to compress.""") 
```

```{python}
#| echo: false
show_hc(s_quick, tree_quick)
```

Just for fun, let's apply Huffman coding for images too.

```{python}
#| echo: false
#| layout-ncol: 2
#| label: fig-image-compression
#| fig-cap: Image compression
#| fig-subcap:
#|   - Mondriaan
#|   - Monet
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

mondriaan = Image.open('HuffmanCoding/images/mondriaan.jpg').convert("L").resize((100,100))
monet     = Image.open('HuffmanCoding/images/monet.jpg').convert("L").resize((100,100))

s = np.array(mondriaan).flatten().tolist()
tree = HuffmanTree(Counter(s))
bits = 0
for k, v in tree.encoded_table.items():
    bits += len(v) * tree.freq_table[k]

mondriaan_bytes = bits/8

plt.imshow(mondriaan, cmap='binary_r')
plt.axis('off')
plt.show()

s = np.array(monet).flatten().tolist()
tree = HuffmanTree(Counter(s))
bits = 0
for k, v in tree.encoded_table.items():
    bits += len(v) * tree.freq_table[k]

monet_bytes = bits/8

plt.imshow(monet, cmap='binary_r')
plt.axis('off')
plt.show()
```
Both @fig-image-compression-1 and @fig-image-compression-2 are resized to 100x100 greyscale images.
Each pixel is represented by the intensity of light from `0` to `255` which corresponds to one byte of information.
This results in `10000` bytes for each image.

```{python}
#| echo: false 
#| output: asis
print(f"""After applying Huffman coding to each image,
each is compressed to `{mondriaan_bytes:.1f}` bytes
and `{monet_bytes:.1f}` bytes, respectively.""")
```

So, why do we think @fig-image-compression-1 is easier to draw?
Why is it easier to remember pop music notes than those of bebop jazz?

It becomes clear when explained using data compression.
Since it has a higher compression rate,
it can be described verbally or communicated with a shorter and more compact explanation.


## Average length per character ($L$)

Let's use some notations from now on.

$$
L \triangleq \mathbb E [l(x)] = \sum_{x \in \mathcal X} p(x) l(x)
$$

```{python}
#| echo: false
#| output: asis 
print(f"""
For the phrase `{s_apple}`, $L$ is {tree_apple.L:.2f} with the unit in bits.
""") 
```

The objective of making good prefix codes is to minimize $\mathbb E [l(x)]$ which leads to the property below.

$$
p(x_1) \gt p(x_2) \implies l(x_1) \leq l(x_2)
$$
