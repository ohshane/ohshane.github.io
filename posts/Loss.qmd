---
title: "Cross Entropy Loss"
subtitle: ""
description: ""
author: Shane Oh
date: 2023-09-07
image: false
categories:
  - Machine Learning
---

## Derivatives

### Binary cross entropy

$$
\begin{align*}
H(y, \hat{y})
&= y \log\dfrac{1}{\hat{y}} + (1-y) \log\dfrac{1}{1-\hat{y}} \\
&= -\left\{ y \log{\hat{y}} + (1-y) \log(1-\hat{y}) \right\} \\
\dfrac{d H(y, \hat{y})}{d\hat{y}}
&= -\dfrac{y}{\hat y} + \dfrac{1-y}{1-\hat y} = \dfrac{\hat y - y}{\hat y (1-\hat y)}
\end{align*}
$$

### Cross entropy

$$
\begin{align*}
H(\mathbf y, \hat{\mathbf y})
&= \sum_i y_i \log \dfrac{1}{\hat{y}_i} = -\sum_i y_i \log \hat{y}_i \\
\dfrac{\partial H(\mathbf y, \hat{\mathbf y})}{\partial \hat{\mathbf y}}
&= 
- \begin{pmatrix}
\dfrac{y_1}{\hat{y}_1}
& \cdots &
\dfrac{y_n}{\hat{y}_n}
\end{pmatrix}
\end{align*}
$$

### Sigmoid

Let $\sigma : \mathbb R \to \mathbb R$.

$$
\begin{align*}
\dfrac{d}{dx} \sigma(x)
&= \dfrac{d}{dx} (1 + e^{-x})^{-1} \\
&= e^{-x}(1 + e^{-x})^{-2}
 = \dfrac{1}{1 + e^{-x}} \dfrac{e^{-x}}{1 + e^{-x}} \\
&= \sigma(x) (1-\sigma(x))
\end{align*}
$$

Let $\sigma : \mathbb R^n \to \mathbb R^n$ (softmax).

- $\mathbf x = (x_1 \cdots x_n)^\top$
- $\sigma = (\sigma_1 \cdots \sigma_n)^\top$
- $\sigma_i(\mathbf x) = \frac{e^{x_i}}{\sum_j e^{x_j}} \quad (\sigma_i:\mathbb R^n \to \mathbb R)$

### Softmax

$$
\begin{align*}
\mathbf J_{\sigma}
&=
\begin{pmatrix}
\dfrac{\partial \sigma_1(\mathbf x)}{\partial x_1} & \cdots & \dfrac{\partial \sigma_1(\mathbf x)}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\dfrac{\partial \sigma_n(\mathbf x)}{\partial x_1} & \cdots & \dfrac{\partial \sigma_n(\mathbf x)}{\partial x_n}
\end{pmatrix} \\
&= 
\begin{pmatrix}
\dfrac{e^{x_1} \sum_j e^{x_j} - e^{x_1} e^{x_1}}{(\sum_j e^{x_j})^2}
& \cdots &
\dfrac{0 \sum_j e^{x_j} - e^{x_1} e^{x_n}}{(\sum_j e^{x_j})^2} \\
\vdots & \ddots & \vdots \\
\dfrac{0 \sum_j e^{x_j} - e^{x_n} e^{x_1}}{(\sum_j e^{x_j})^2}
& \cdots &
\dfrac{e^{x_n} \sum_j e^{x_j} - e^{x_n} e^{x_n}}{(\sum_j e^{x_j})^2}
\end{pmatrix} \\
&= 
\begin{pmatrix}
\dfrac{e^{x_1}}{\sum_j e^{x_j}}
\dfrac{\sum_j e^{x_j} - e^{x_1}}{\sum_j e^{x_j}}
& \cdots &
- \dfrac{e^{x_1}}{\sum_j e^{x_j}} \dfrac{e^{x_n}}{\sum_j e^{x_j}} \\
\vdots & \ddots & \vdots \\
- \dfrac{e^{x_n}}{\sum_j e^{x_j}} \dfrac{e^{x_i}}{\sum_j e^{x_j}}
& \cdots &
\dfrac{e^{x_n}}{\sum_j e^{x_j}}
\dfrac{\sum_j e^{x_j} - e^{x_n}}{\sum_j e^{x_j}}
\end{pmatrix} \\
&= 
\begin{pmatrix}
\sigma_1(\mathbf x)(1-\sigma_1(\mathbf x)) & \cdots & -\sigma_1(\mathbf x) \sigma_n(\mathbf x) \\
\vdots & \ddots & \vdots \\
-\sigma_n(\mathbf x) \sigma_1(\mathbf x) & \cdots & \sigma_n(\mathbf x)(1-\sigma_n(\mathbf x)) \\
\end{pmatrix} \\
&= 
\mathrm{diag}(\sigma(\mathbf x)) -\sigma(\mathbf x) \sigma(\mathbf x)^\top
\end{align*}
$$

### $\dfrac{d \mathcal L_\mathrm{BCE}}{d \mathbf x}$

$$
\begin{align*}
\dfrac{d \mathcal L_\mathrm{BCE}}{d x}
&= 
\dfrac{d \mathcal L_\mathrm{BCE}}{d \hat{y}}
\dfrac{d \hat{y}}{dx} \\
&=
\dfrac{\hat y - y}{\hat y (1-\hat y)} {\hat y (1-\hat y)} \\
&=
\hat y - y
\end{align*}
$$

### $\dfrac{\partial \mathcal L_\mathrm{CE}}{\partial \mathbf x}$

$$
\begin{align*}
\dfrac{\partial \mathcal L_\mathrm{CE}}{\partial \mathbf x}
&= 
\dfrac{\partial \mathcal L_\mathrm{CE}}{\partial \hat{\mathbf y}}
\dfrac{\partial \hat{\mathbf y}}{\partial \mathbf x} \\
&= 
- \begin{pmatrix}
\dfrac{y_1}{\hat{y}_1}
& \cdots &
\dfrac{y_n}{\hat{y}_n}
\end{pmatrix}
\left( \mathrm{diag}(\hat{\mathbf y}) - \hat{\mathbf y} \hat{\mathbf y}^\top \right) \\
&= 
\begin{pmatrix}
-y_1 + \sum_i y_i \hat{y}_1
& \cdots &
-y_n + \sum_i y_i \hat{y}_n
\end{pmatrix}
\end{align*}
$$