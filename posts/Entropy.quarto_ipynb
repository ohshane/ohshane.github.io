{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Entropy\n",
        "subtitle: The amount of uncertainty\n",
        "description: \"\"\n",
        "author: Shane Oh\n",
        "date: 2023-04-14\n",
        "image: false\n",
        "categories:\n",
        "  - Information Theory\n",
        "  - Data Compression\n",
        "---\n",
        "\n",
        "\n",
        "I highly recommend taking a look at the [Huffman Coding](HuffmanCoding.qmd) post first.\n",
        "Huffman coding provides an optimal compression solution for a given data distribution,\n",
        "whereas Shannon-Fano coding does not.\n",
        "\n",
        "It may be easier for us to first learn Huffman coding (a bottom-up approach to building the tree)\n",
        "in the algorithms class and then move on to Shannon-Fano coding (a top-down approach).\n",
        "\n",
        "Take a look at the [video](https://youtu.be/B3y0RsVCyrw?si=ikWennqSHzgY9pfn).\n",
        "\n",
        "::: {.callout-note}\n",
        "# Historical Context: Shannon's Entropy and Huffman Coding\n",
        "After Claude Shannon introduced entropy in 1948, which defines the theoretical limit for\n",
        "optimal data compression, David A. Huffman built on this in 1952 by creating Huffman coding\n",
        "during his Ph.D.\n",
        ":::\n",
        "\n",
        "$$\n",
        "H(X) \\leq L \\leq H(X) + 1\n",
        "$$\n",
        "\n",
        "\n",
        "### Lower and upper bounds\n",
        "We can set a theoretical lower and upper bound for $L$ as below.\n",
        "\n",
        "$$\n",
        "H(X) \\leq L \\lt H(X)+1\n",
        "$$\n",
        "\n",
        "Let's see if this really works. $H(X)$ is defined as $-\\sum_{x \\in \\mathcal X} p(x) \\log_2 p(x)$.\n"
      ],
      "id": "48534169"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "# | output: asis\n",
        "def entropy(p):\n",
        "    return -(p * np.log2(p)).sum()\n",
        "\n",
        "p = np.array(list(tree_apple.freq_table.values()))\n",
        "p = p / p.sum()\n",
        "\n",
        "print(f\"\"\"\n",
        "$$\n",
        "{entropy(p):.3f} \\leq {tree_apple.L:.3f} \\lt {entropy(p)+1:.3f}\n",
        "$$\n",
        "\"\"\")"
      ],
      "id": "e8a23a57",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/shane/.pyenv/versions/anaconda3-2023.07-0/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}